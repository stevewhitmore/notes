{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>My notes on tech stuff. </p> <p>Navigate with the menu above or just search for something.</p>"},{"location":"code/","title":"Code Overview","text":""},{"location":"code/java/","title":"Java","text":""},{"location":"code/java/#jave-ee-the-big-picture","title":"Jave EE: The Big Picture","text":""},{"location":"code/java/#four-platforms","title":"Four platforms","text":""},{"location":"code/java/#se-standard","title":"SE (Standard)","text":"<ul> <li>Core platform</li> <li>Core libraries and APIs</li> <li>Basic types and bojects to high-level classes</li> <li>JVM (JDK)</li> <li>Dev tools (JDK)</li> <li>Deployment and monitorying (JDK)</li> </ul>"},{"location":"code/java/#me-micro","title":"ME (Micro)","text":"<ul> <li>Subset of SE</li> <li>For mobile devices</li> <li>Small footprint JVM</li> <li>Small devices like sensors, printers, etc</li> <li>IoT</li> </ul>"},{"location":"code/java/#fx","title":"FX","text":"<ul> <li>Rich internet apps</li> <li>UI API</li> <li>Hardware-accelerated graphics</li> <li>High-performance clients</li> <li>Modern look and feel</li> <li>Connect to remote services</li> </ul>"},{"location":"code/java/#ee-enterprise","title":"EE (Enterprise)","text":"<ul> <li>Extends SE</li> <li>Large scale</li> <li>Distributed system</li> <li>The other 3 are often clients of this platform</li> </ul>"},{"location":"code/java/#enterprise-applications","title":"Enterprise Applications","text":"<p>Designed for:</p> <ul> <li>Multi-tiered</li> <li>Scalable</li> <li>Releiable</li> <li>Secure</li> <li>Solve the problems of large enterprises</li> </ul> <p>\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"</p>"},{"location":"code/java/#pojo-vs-java-bean","title":"POJO vs Java Bean","text":"<p>A JavaBean is a POJO that is serializable, has a no-argument constructor, and allows access to properties using getter and setter methods</p>"},{"location":"code/java/#pojo","title":"POJO","text":"<ul> <li>It doesn\u2019t have special restrictions other than those forced by Java language</li> <li>It doesn\u2019t provide much control on members.</li> <li>It can implement Serializable interface.</li> <li>Fields can be accessed by their names.</li> <li>Fields can have any visiblity.</li> <li>There can be a no-arg constructor.</li> <li>It is used when you don\u2019t want to give restriction on your members and give user complete access of your entity</li> </ul>"},{"location":"code/java/#java-bean","title":"Java Bean","text":"<ul> <li>It is a special POJO which have some restrictions.</li> <li>It provides complete control on members.</li> <li>It should implement serializable interface.</li> <li>Fields are accessed only by getters and setters.</li> <li>Fields have only private visiblity.</li> <li>It must have a no-arg constructor.</li> <li>It is used when you want to provide user your entity but only some part of your entity.</li> </ul>"},{"location":"code/java/#hibernate-vs-java-ee","title":"Hibernate vs Java EE","text":"<p>The relationship between Hibernate and Java EE is that Hibernate is a certified JPA provider, and Java EE is a platform that, among many other specifications, it contains the JPA specification. Hibernate is an implementation of the JPA specification.</p> <p>\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"</p>"},{"location":"code/java/#java-ee-architecture","title":"Java EE Architecture","text":"<ul> <li>Container</li> <li>Components</li> <li>Services</li> <li>APIs</li> <li>Packaging</li> <li>Deployment</li> <li>Protocols</li> <li>Java SE APIs</li> </ul>"},{"location":"code/java/#container","title":"Container","text":"<ul> <li>Runtime environment</li> <li>hide technical complexity</li> <li>enhance portability</li> <li>host apps</li> <li>handle complex low-level details</li> <li>admin applications</li> </ul>"},{"location":"code/java/#components","title":"Components","text":"<ul> <li>Static or dynamic web pages</li> <li>server-side classes</li> <li>handle business code</li> <li>process data</li> <li>access legacy systems</li> </ul>"},{"location":"code/java/#services","title":"Services","text":"<p>Provides services to containers such as:</p> <ul> <li>security</li> <li>transaction management</li> <li>naming</li> <li>remote connectivity and more</li> </ul> <p>Services are configurable Configuration is isolated</p> <ul> <li>Uses metadata</li> </ul>"},{"location":"code/java/#containers","title":"Containers","text":"<p>Open Source exmaples: JBoss, Glassfish, Tommy Propriatery: IBM Websphere, Oracle Weblogic Server You can deploy several applications within a single container. It provides isolation (each app has its own resources, components, and class-loader) and provides administration for each app.</p>"},{"location":"code/java/#protocols","title":"Protocols","text":"<p>For interacting with the apps.</p> <ul> <li>HTTP/S is the big one that's mostly used</li> <li>RMI/IIOP (Remote Method Invocation/Internet Inter-ORB Protocol)</li> <li>Allows user to integrate with CORBA objects which can be written in any language (Ada, C, C++, Java...)</li> </ul>"},{"location":"code/java/#deployment","title":"Deployment","text":"<p>Distributed applications are several instances of an application deployed to several containers. Allows for:</p> <ul> <li>Load balancing</li> <li>Fail over</li> <li>Scalability</li> <li>Availability</li> </ul>"},{"location":"code/java/#java-ee-services","title":"Java EE Services","text":"<p>Tiers:</p> <ul> <li>Business</li> <li>Web</li> <li>Interoperability</li> <li>Services common to most tiers</li> </ul>"},{"location":"code/java/#business-tier","title":"Business Tier","text":"<p>handles business logic and handles/processes data</p> <ul> <li>Transaction management services</li> <li> <p>very powerful. deals with relational dbs and messaging services</p> </li> <li> <p>Persistence services</p> </li> <li>maps between objects and relational databases</li> <li> <p>provides high-level query language</p> </li> <li> <p>Validation services</p> </li> <li> <p>Batch services</p> </li> <li>Allows user to handle bulk operations running on long-running jobs</li> </ul>"},{"location":"code/java/#web-tier","title":"Web Tier","text":"<p>Handles interaction between client and business tier</p> <ul> <li>Web pages</li> <li> <p>HTML5 static content</p> </li> <li> <p>Servlets</p> </li> <li> <p>handles http/s communication</p> </li> <li> <p>Expression language</p> </li> <li>Handles binding between graphical components and backend components</li> <li> <p>In other words, JSF, which is the bane of every developer's existence</p> </li> <li> <p>Web sockets</p> </li> <li>allow bi-direction conversation between web clients and servers</li> </ul>"},{"location":"code/java/#interoperability-tier","title":"Interoperability Tier","text":"<p>Allows for interaction with other applications. Is considered one of the biggest advantages of Java EE.</p> <ul> <li>SOAP Services*</li> <li>REST Services</li> <li>JSON</li> <li>Messaging</li> <li>Emails</li> <li>Java mail API</li> <li>Connectors</li> <li>Lets you connect to any db type</li> </ul>"},{"location":"code/java/#common-services-for-most-tiers","title":"Common Services for most tiers","text":"<ul> <li>*DI (Dependency Injection)</li> <li>Any resource/component can be injected into any other one</li> <li> <p>Most commonly used one</p> </li> <li> <p>Interception</p> </li> <li>Security</li> <li>Concurrency</li> <li>Important to handle loads easily</li> </ul>"},{"location":"code/java/#java-se-services","title":"Java SE Services","text":"<p>Java EE is a superset of SE. Some essential SE services include</p> <ul> <li>DB Access</li> <li>Naming and directory interface used for injection</li> <li>XML</li> <li>Remote (managing low level communication between components)</li> </ul>"},{"location":"code/java/#business-concerns","title":"Business Concerns","text":"<ul> <li>Standard</li> <li>Not good for startups that need to be bleeding edge. It's very concerned with backwards compatibility</li> <li>Open(ish) see SO answer: \"The problem is that in order to call something \"Java\" you need to get it certified as compliant to the Java spec.</li> </ul> <p>One of the pre-requisites of getting this certification is running you JVM through a test suite - Java Technology Compatibility Kit (TCK). This test suite is NOT open sourced.</p> <p>So you can build a JVM that behaves in a very Java like way and be completely open source, but, if you want to call it a \"Java JVM\" you need to buy the certification suite under a non open source license. To many open source advocates this is a complete non starter.\"</p> <ul> <li>Has paid support</li> <li>Easy to hire for. It's got millions of developers using it</li> </ul>"},{"location":"code/java/#technical-concerns","title":"Technical Concerns","text":"<ul> <li>Portable (write once, run everywhere)</li> <li>Can integrate with basically anything</li> <li>Not great for everything (like IoT or client-side dev)</li> <li>Scalable</li> <li>Security</li> <li>Not great for big data. It was designed to work with relation dbs and doesn't have any solid APIs for NoSQL database types</li> <li>Extensible and modular so is very adaptable for many uses</li> </ul>"},{"location":"code/python/","title":"Python","text":""},{"location":"code/python/#virtual-environments","title":"Virtual Environments","text":"<pre><code>$ python -m venv .venv\n$ . .venv/bin/activate\n</code></pre>"},{"location":"code/python/#static-http-file-server","title":"Static HTTP File Server","text":"<p>Running <code>python -m http.server 8000</code> will create a static file server pointed to whatever directory you ran the command in.</p>"},{"location":"code/python/#install-python-from-source","title":"Install Python from source","text":"<ol> <li>Go to https://www.python.org/downloads/ and find the desired version</li> <li>Click on Download link. Scroll to the bottom and find \"Gzipped source tarball\"</li> <li>Run <code>wget &lt;tarball url&gt;</code> followed by <code>tar -xzf &lt;tarball name&gt;</code></li> <li>cd into untarred directory</li> <li>Run <code>./configure --enable-optimizations</code> followed by <code>suco make altinstall</code></li> </ol> <p>That's it! Do a <code>which</code> on the version major and minor to to confirm it's present in the <code>/usr/local/bin</code> folder (e.g. <code>which python3.9</code> for version 3.9.10)</p>"},{"location":"code/python/#colors","title":"Colors","text":"<p>You can color the text output of your python scripts by wrapping strings in the following variables:</p> <pre><code>FORMAT_RESET = \"\\x1b[0m\"\nFORMAT_BOLD = \"\\x1b[1m\"\nFORMAT_UNBOLD = \"\\x1b[22m\"\nFORMAT_DIM = \"\\x1b[2m\"\nFORMAT_ITALIC = \"\\x1b[3m\"\nFORMAT_UNDERLINE = \"\\x1b[4m\"\nFORMAT_RED = \"\\x1b[31m\"\nFORMAT_GREEN = \"\\x1b[32m\"\nFORMAT_YELLOW = \"\\x1b[33m\"\nFORMAT_BLUE = \"\\x1b[34m\"\nFORMAT_MAGENTA = \"\\x1b[35m\"\nFORMAT_CYAN = \"\\x1b[36m\"\nFORMAT_WHITE = \"\\x1b[37m\"\n\nprint(\"Some text\")\nprint(f\"{FORMAT_YELLOW}Writing changes to {FORMAT_BOLD}{realm}{FORMAT_UNBOLD} path {some_path}{FORMAT_RESET}\")\nprint(\"Some more text here\")\n</code></pre> <p>Output:</p> <p></p>"},{"location":"code/typescript/rxjs/","title":"RxJS","text":"","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#rxjs-must-knows","title":"RxJS Must Knows","text":"<p>There are hundreds of RxJS functions and operators. These are 20 of the most commonly used ones and thus the most beneficial to master.</p>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#creational-operators","title":"Creational Operators","text":"","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#empty","title":"EMPTY","text":"<p>A simple Observable that emits no items to the Observer and immediately emits a complete notification.</p> <pre><code>EMPTY.subscribe({\n  next: () =&gt; console.log('Next'),\n  complete: () =&gt; console.log('Complete!')\n});\n\n// Outputs\n// Complete!\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#never","title":"NEVER","text":"<p>An Observable that emits no items to the Observer and never completes.</p> <pre><code>const info = () =&gt; console.log('Will not be called');\n\nconst result = NEVER.pipe();\nresult.subscribe({\n  next: info,\n  error: info,\n  complete: info\n});\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#from","title":"from","text":"<p>Creates an Observable from an Array, an array-like object, a Promise, an iterable object, or an Observable-like object. This differs from 'of' because it'll emit each item one by one</p> <pre><code>import { from } from 'rxjs';\n\nconst array = [10, 20, 30];\nconst result = from(array);\n\nresult.subscribe(x =&gt; console.log(x));\n\n// Outputs:\n// 10\n// 20\n// 30\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#of","title":"of","text":"<p>Similar to 'from' but it'll emit an iterable all at once</p> <pre><code>import { of } from 'rxjs';\n\nof(10, 20, 30)\n    .subscribe({\n        next: value =&gt; console.log('next:', value),\n        error: err =&gt; console.log('error:', err),\n        complete: () =&gt; console.log('the end'),\n    });\n\n// Outputs:\n// next: 10\n// next: 20\n// next: 30\n// the end\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#pipeable-operators","title":"Pipeable Operators","text":"","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#combinelatest","title":"combineLatest","text":"<p>Combines the values from all supplied observables. Use this when you need to support one observable being dependent upon another.</p> <pre><code>const firstTimer = timer(0, 1000); // emit 0, 1, 2... after every second, starting from now\nconst secondTimer = timer(500, 1000); // emit 0, 1, 2... after every second, starting 0,5s from now\nconst combinedTimers = combineLatest([firstTimer, secondTimer]);\ncombinedTimers.subscribe(value =&gt; console.log(value));\n\n// Outputs:\n// [0, 0] after 0.5s\n// [1, 0] after 1s\n// [1, 1] after 1.5s\n// [2, 1] after 2s\n// ...and so on\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#concatwith","title":"concatWith","text":"<p>Emit values from provided observable after the first observable completes. Use this when you need to handle multiple sets of data in the same way in order</p> <pre><code>const clicks$ = fromEvent(document, 'click');\nconst moves$ = fromEvent(document, 'mousemove');\n\nclicks$.pipe(\n  map(() =&gt; 'click'),\n  take(1),\n  concatWith(\n    moves$.pipe(\n      map(() =&gt; 'move')\n    )\n  )\n)\n.subscribe(x =&gt; console.log(x));\n\n// Outputs:\n// 'click'\n// 'move'\n// 'move'\n// 'move'\n// ...\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#concatmap","title":"concatMap","text":"<p>Allows for emitting values from an operation that creates a separate observable, it will emit values from the created observable. Original values will be emitted in order after each of their created observables have completed. Use this when you need to flatten an observable of observables when you want to handle each parent emit after the child observable completes.</p> <pre><code>const clicks = fromEvent(document, 'click');\nconst result = clicks.pipe(\n  concatMap(ev =&gt; interval(1000).pipe(take(4)))\n);\nresult.subscribe(x =&gt; console.log(x));\n\n// Results in the following:\n// (results are not concurrent)\n// For every click on the \"document\" it will emit values 0 to 3 spaced\n// on a 1000ms interval\n// one click = 1000ms-&gt; 0 -1000ms-&gt; 1 -1000ms-&gt; 2 -1000ms-&gt; 3\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#count","title":"count","text":"<p>Tells how many values were emitted, when the stream completes. Use this when you need to find out how many items were in an observable stream. This example shows an operation via count that doesn't effect the values.</p> <pre><code>const numbers = range(1, 7);\nconst result = numbers.pipe(count());\nresult.subscribe(x =&gt; console.log(x));\n\n// Output\n// 7\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#debouncetime","title":"debounceTime","text":"<p>Adds a time buffer to only emit when no other values have been emitted in the timeframe specified. Use this to limit longer running processes that can be requested multiple times when you only care about the most recent value. i.e. making HTTP calls for autocomplete  </p> <pre><code>const clicks = fromEvent(document, 'click');\nconst result = clicks.pipe(debounceTime(1000));\nresult.subscribe({\n  next: () =&gt; console.log('foo'),\n})\n\n// Outputs \n// 'foo' after 1 second:\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#distinct","title":"distinct","text":"<p>A given value is not emitted more than once. Use this when you do not want to reprocess the same information more than once.</p> <pre><code>of(1, 1, 2, 2, 2, 3, 4, 5, 5)\n    .pipe(\n        distinct(),\n    )\n    .subscribe(console.log);\n\n// Outputs:\n// 1\n// 2\n// 3\n// 4\n// 5\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#distinctuntilchanged","title":"distinctUntilChanged","text":"<p>Emits only when the current value is different than the previous value. Use this when you are only interested in doing something when there is a new value. i.e. don\u2019t validate a textbox value if the user pasted the same value over itself.  </p> <pre><code>of(1, 1, 1, 2, 2, 2, 1, 1, 3, 3)\n  .pipe(distinctUntilChanged())\n  .subscribe(console.log);\n\n// Outputs:\n// 1\n// 2\n// 1\n// 3\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#endwith","title":"endWith","text":"<p>Allows you to specify the last value to be emitted before completion.</p> <pre><code>const ticker$ = interval(5000).pipe(\n  map(() =&gt; 'tick')\n);\n\nconst documentClicks$ = fromEvent(document, 'click');\n\nticker$.pipe(\n  startWith('interval started'),\n  takeUntil(documentClicks$),\n  endWith('interval ended by click')\n)\n.subscribe(x =&gt; console.log(x));\n\n// Result (assuming a user clicks after 15 seconds)\n// 'interval started'\n// 'tick'\n// 'tick'\n// 'tick'\n// 'interval ended by click'\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#filter","title":"filter","text":"<p>Allows you to prevent values from being emitted based upon a supplied function. Use this when you want to control what values are emitted for further processing.</p> <pre><code>of(1, 2, 3, 4, 5, 6, 7, 8, 9)\n    .pipe(\n        filter(x =&gt; x % 2 === 0),\n    )\n    .subscribe(console.log);\n\n// Outputs:\n// 2\n// 4\n// 6\n// 8\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#first","title":"first","text":"<p>Only emit the first value, after the first value the observable will complete. Use this if you only care about the first value.</p> <pre><code>of(1, 2, 3, 4, 5, 6, 7, 8, 9)\n    .pipe(\n        first(),\n    )\n    .subscribe(console.log);\n\n// Outputs:\n// 1\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#last","title":"last","text":"<p>Only emit the last value prior to completion. Use this if you only care about the last value.</p> <pre><code>of(1, 2, 3, 4, 5, 6, 7, 8, 9)\n    .pipe(\n        last(),\n    )\n    .subscribe(console.log);\n\n// Outputs:\n// 9\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#map","title":"map","text":"<p>Allows the values to be modified to a new value. Use this when you want to change values being emitted.</p> <pre><code>of(1,2,3,4)\n  .pipe(map(x =&gt; x *= 2))\n  .subscribe(console.log);\n\n// Outputs:\n// 2\n// 4\n// 6\n// 8\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#mergemap","title":"mergeMap","text":"<p>Returns an Observable that emits items based on applying a function that you supply to each item emitted by the source Observable, where that function returns an Observable, and then merging those resulting Observables and emitting the results of this merger.</p> <pre><code>of('a', 'b', 'c')\n  .pipe(\n    mergeMap(x =&gt; interval(1000).pipe(map(i =&gt; x + i)))\n  )\n  .subscribe(console.log);\n\n// Results in the following:\n// a0\n// b0\n// c0\n// a1\n// b1\n// c1\n// continues to list a, b, c every second with respective ascending integers\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#startswith","title":"startsWith","text":"<p>Provides the ability to specify a value which will be the first value emitted by the observable. Use this to seed your observable with a specific value.</p> <pre><code>of(1, 2, 3, 4, 5)\n    .pipe(\n        startWith(1000),\n    )\n    .subscribe(console.log);\n\n// Outputs:\n// 1000\n// 1\n// 2\n// 3\n// 4\n// 5\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#switchmap","title":"switchmap","text":"<p>Allows for emitting values from an operation that creates a separate observable, it will emit values from the created observable. Values from the created observable will be emitted only from the most recent emitted source observable. Use this when you only care about the most recent parent emit value\u2019s child values, because a new parent emit will cancel the previous child observable</p> <pre><code>const switched = of(1, 2, 3).pipe(switchMap(x =&gt; of(x, x ** 2, x ** 3)));\nswitched.subscribe(x =&gt; console.log(x));\n\n// Outputs:\n// 1\n// 1\n// 1\n// 2\n// 4\n// 8\n// 3\n// 9\n// 27\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#take","title":"take","text":"<p>Allows specification of the number of emits that occur before the observable completes. Use this when you want to limit the number of values.</p> <pre><code>const intervalCount = interval(1000);\nconst takeFive = intervalCount.pipe(take(5));\ntakeFive.subscribe(x =&gt; console.log(x));\n\n// Outputs:\n// 0\n// 1\n// 2\n// 3\n// 4\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#takeuntil","title":"takeUntil","text":"<p>Allows specifying when an observable will complete based upon the emission of separate observable. Use this when you want one event to signal the completion of another.  </p> <pre><code>const source = interval(1000);\nconst clicks = fromEvent(document, 'click');\nconst result = source.pipe(takeUntil(clicks));\nresult.subscribe(x =&gt; console.log(x));\n\n// Outputs:\n// 0\n// 1\n// 2\n// ...etc until a click\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#tap","title":"tap","text":"<p>Allows for side-effects based upon the source observable, but does not have an effect on the values being emitted. Use this to use the emit of an observable to trigger something outside the scope of the observable. A common use case is to place debugging statements such as logging.</p> <pre><code>of(3).pipe(\n  tap(console.log),\n).subscribe();\n\n// Outputs:\n// 3\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#withlatestfrom","title":"withLatestFrom","text":"<p>Allows \u201cpulling\u201d latest value from another observable when the source observable emits. The source value is combined with the other observable in an array. Use this when you need information from another observable, but may not care when that observable emits.</p> <pre><code>const clicks = fromEvent(document, 'click');\nconst timer = interval(1000);\nconst result = clicks.pipe(withLatestFrom(timer));\nresult.subscribe(x =&gt; console.log(x));\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#rxjs-gotchas","title":"RxJS Gotchas","text":"","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#first-vs-take1","title":"first vs take(1)","text":"<p><code>first()</code> will throw an error if no value is provided while <code>take(1)</code> will not</p>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#forkjoin-vs-merge-vs-zip","title":"forkJoin vs merge vs zip","text":"","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#forkjoin","title":"forkJoin","text":"<p><code>forkJoin</code> is an operator that takes any number of input observables which can be passed either as an array or a dictionary of input observables. If no input observables are provided (e.g. an empty array is passed), then the resulting stream will complete immediately.</p> <p><code>forkJoin</code> will wait for all passed observables to emit and complete and then it will emit an array or an object with last values from corresponding observables.</p> <pre><code>const observable = forkJoin({\n  foo: of(1, 2, 3, 4),\n  bar: Promise.resolve(8),\n  baz: timer(4000)\n});\nobservable.subscribe({\n next: value =&gt; console.log(value),\n complete: () =&gt; console.log('This is how it ends!'),\n});\n\n// Outputs:\n// { foo: 4, bar: 8, baz: 0 } after 4 seconds\n// 'This is how it ends!' immediately after\n</code></pre> <p>also</p> <pre><code>const observable = forkJoin([\n  of(1, 2, 3, 4),\n  Promise.resolve(8),\n  timer(4000)\n]);\nobservable.subscribe({\n next: value =&gt; console.log(value),\n complete: () =&gt; console.log('This is how it ends!'),\n});\n\n// Outputs:\n// [4, 8, 0] after 4 seconds\n// 'This is how it ends!' immediately after\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#merge","title":"merge","text":"<p><code>merge</code> subscribes to each given input Observable (as arguments), and simply forwards (without doing any transformation) all the values from all the input Observables to the output Observable. The output Observable only completes once all input Observables have completed. Any error delivered by an input Observable will be immediately emitted on the output Observable.</p> <pre><code>const clicks = fromEvent(document, 'click');\nconst timer = interval(1000);\nconst clicksOrTimer = merge(clicks, timer);\nclicksOrTimer.subscribe(x =&gt; console.log(x));\n\n// Results in the following:\n// timer will emit ascending values, one every second(1000ms) to console\n// clicks logs MouseEvents to console every time the \"document\" is clicked\n// Since the two streams are merged you see these happening\n// as they occur.\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#zip","title":"zip","text":"<p>If the last parameter is a function, this function is used to compute the created value from the input values. Otherwise, an array of the input values is returned.</p> <pre><code>const age$ = of(27, 25, 29);\nconst name$ = of('Foo', 'Bar', 'Beer');\nconst isDev$ = of(true, true, false);\n\nzip(age$, name$, isDev$).pipe(\n  map(([age, name, isDev]) =&gt; ({ age, name, isDev }))\n)\n.subscribe(x =&gt; console.log(x));\n\n// Outputs:\n// { age: 27, name: 'Foo', isDev: true }\n// { age: 25, name: 'Bar', isDev: true }\n// { age: 29, name: 'Beer', isDev: false }\n</code></pre>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#flatmap-vs-mergemap","title":"flatMap vs mergeMap","text":"<p>They're the same thing. flatMap was renamed to mergeMap and is being removed in v8.</p>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/rxjs/#switchmap-vs-mergemap","title":"switchMap vs mergeMap","text":"<p><code>switchMap</code> cancels previous HTTP requests that are still in progress, while <code>mergeMap</code> lets all of them finish.</p>","tags":["TypeScript","Angular","RxJS"]},{"location":"code/typescript/types-vs-interfaces/","title":"Types vs Interfaces","text":"<p>Both are functionally similar, so when to use each?</p>","tags":["TypeScript"]},{"location":"code/typescript/types-vs-interfaces/#interfaces","title":"Interfaces","text":"<p>Are generally used to define the shape of an object. You define the properties you expect to see in the object assigned to the interface.</p>","tags":["TypeScript"]},{"location":"code/typescript/types-vs-interfaces/#types","title":"Types","text":"<p>Are good for when you have a group of interfaces to choose from:</p> <pre><code>export type ResponseModels =\n    m.EntityAddressModel\n    | m.EntityIndDemographicModel\n    | m.EntityOrgDemographicsModel;\n</code></pre> <p>or for having a group of primitives to choose from:</p> <pre><code>export type EntityType = 'Beneficiary'\n    | 'Defendant'\n    | 'Legal Team'\n    | 'Plaintiff';\n</code></pre>","tags":["TypeScript"]},{"location":"code/typescript/angular/angular-basics/","title":"Angular Basics (needs revising and cleanup)","text":"<p>Firebase - Google developed database system - handles setting up authentication system setup - JSON format</p> <p>Webpack to handle individual components of the app - bundles them into one file to deploy</p> <p>Protect sensitive areas using Routing</p> <p>Angular CLI: allows you to generate components, services, and routes</p> <p>karma.conf.js is a testing library to use with the project package.json installs libraries, used by npm</p> <p>\u2018let\u2019 block scopes our variables, doesn\u2019t let values leak outside enclosing blocks \u2018const\u2019 has same rules as \u2018let\u2019 but can\u2019t just reassign value, takes more to change value</p> <p>TypeScript - Can assign type 'any' if dont care or unsure what type     - if array can be 'any[]'</p> <p>Decorators A function that adds metadata to a class, its members, or its methods arguments - used a lot, preceded by \u2018@\u2018 symbol - immediately precede the classes they apply to - technically a function getting an object passed into it  - double curly braces indicate data binding </p> <ul> <li>you add the export statement to the class declaration to make it available to be imported to other components in the app</li> <li>\u2018export\u2019 breaks up code into modules to import into other files.</li> </ul> <p>app.module.ts - the root module - launches app.component.ts (the meat of the app)</p> <p>main.ts - runs the bootstrap method against app.module.ts import { platformBrowserDynamic } from '@angular/platform-browser-dynamic'; - converts code to be displayed in browser at runtime</p> <p>package.json requires \u2018name\u2019 and \u2018version\u2019 properties, the rest is for the users benefit - \u2018scripts\u2019 - \u2018postinstall\u2019 will run immediately after \u2018npm install\u2019 so it will install everything you need</p> <p>SystemJS was used but Webpack seems to be more popular now for bundling modules</p> <p>** Gulp tasks - If you do not return a stream, then the asynchronous result of each task will not be awaited by its caller, nor any dependent tasks.</p> <p>\u2018event-stream\u2019 - uses streaming (nodes philosophy) instead of temp directory to keep src and compiled files separate  ** task managers invoked where? webpack? that + other?</p> <p>Componenet Checklist     Class -&gt; Code     - Clear name (PascalCasing)     - Append \"Component\" to the name     export keyword     Data in properties     - Approprate data type and dfault value - camelCase with first letter lowercase     Logic in methods     - also use camelCase</p> <pre><code>Decorator -&gt; Metadata\nComponent decorator\n- Prefix with @; suffix with ()\nselector: component name in HTML\n- prefix for clarity\ntemplate: view's HTML\n- correct HTML syntax\n\nImport what we need (from modules or angular itself)\nDefines where to find the members that this component needs\nimport keyword\nMember name\n- correct spelling/casing\nModule path\n- enclose in quotes\n- correct spelling/casing - don't need to specify file extension\n\nDebug checklist:\n- View console errors\n- HTML\n    - close tags\n    - angular directives are case sensitive\n- TypeScript\n    - close braces\n    - case sensitive\n</code></pre>"},{"location":"code/typescript/angular/angular-basics/#component-template-classproperties-methods-metadata","title":"Component = Template + Class[Properties + Methods] + Metadata","text":"<p>Data Binding  Coordinates communication between the components class and its templalte and often involves passing data. Template &lt;- Class - Values for display Template -&gt; Class - Raises events or user input to give back to the class</p>"},{"location":"code/typescript/angular/angular-basics/#interpolation","title":"Interpolation{{pageTitle}}","text":"<p>DOM &lt;-- Component</p> <p>one way binding - from the class property to the template - Can do more than display a property {{'Title: ' + pageTitle}} {{2*20+1}} {{'Title: ' + getTitle()}}</p> <ul> <li>Text inside of double curlies are called 'Template Expressions' We use Interpolation if we want to display READ ONLY data </li> </ul>"},{"location":"code/typescript/angular/angular-basics/#property-binding","title":"Property Binding","text":"<p>DOM &lt;-- Component  [] - binding target '' - binding source This is generally the prefferred method of one way data binding, but in some cases we need to use interpolation: </p>"},{"location":"code/typescript/angular/angular-basics/#event-binding","title":"Event Binding","text":"<p>DOM --&gt; Component  () - target event '' - template statement - usually a component class method enclosed in quotes https://dveloper.mozila.org/en-US/docs/Web/Events for a list of standard events and documentation"},{"location":"code/typescript/angular/angular-basics/#two-way-binding","title":"Two-way Binding","text":"<p>DOM &lt;--&gt; Component Template &lt;-&gt; Class  We need to use ngModel directive for two-way binding. [] - property binding from the class property to the input element () - to indicate event binding to send notification of the user entered data back to the class property we assign this directive to a template expression think of a banana in a box! [()] ** for ngModel make sure to import FormsModule from forms and include in imports array in app.module.ts</p> <p>Directives Custom HTML element or attribute used to power up and extend our HTML</p> <p>Built in directives     - Structural Directives         * marks the directive as structural         ngIf: if logic         ngFor: for loops         *ngFor='let product of products' - the 'product' is a template input variable that iterates</p> <pre><code>    Why 'product of products' and not 'product in products'?\n        for...of iterates over iterable objects, such as an array\n            let things = ['foo', 'bar', 'baz'];\n            for (let thing of things) {\n                console.log(nickname)\n            }\n            result: foo, bar, baz\n\n        for...in iterates over the properties of an object\n            let things = ['foo', 'bar', 'baz'];\n            for (let thing of things) {\n                console.log(nickname)\n            }\n            result: 0, 1, 2\n\n- if *ngIf evaluates to false, it and its children are removed from the DOM\nBrowserModule enables our Components to utilize these structural directives\n</code></pre> <p>Template Checklist Inline Template - For short templates     - specify template property, use es2015 back ticks for multiple lines, watch syntax Linked Templates - For longer tempates     speciry the temlpateUrl perperty, define the path to the HTML file</p> <p>Component as a Directive Checklist 1. Use the directive as an element in the template for any other component. We use the directive component selector as the directive name. 2. We then declare the component so it's available to any template associated to this module.  3. Pass the component into the declarations array in th NgModule of the app.module.ts file</p> <p>Transforming Data with Pipes - Transform bound properties before display - Built-in pipes     * Date     * Number, decimal, percent, currency     * json, slice     * etc - Custom pipes</p> <p>Examples: </p> <p>{{ product.price | currency | lowercase }}</p> <p>Some pipes allow for arguments {{ product.price | currency:'USD':true:'1.2-2' }}</p> <p>Defining an Interface A specification identifying a related set of properties and methods A class commits to supporting the specification by implemeting th interface Use the interface as a data type es2015 does not support interfaces, but typescript does, so its only used during development</p> <p>Interface is a specification -</p> <p>export interface IProduct {     productID: number;     productName: string;     releaseDate: Date;     calculateDiscount(percent:number): number; }</p> <p>Using an interface as a data type -</p> <p>import { IProduct } from './product';</p> <p>export class ProductListComponent {     pageTitle: string = 'Product List';     ...     products: IProduct = [...];</p> <pre><code>toggleImage(): void {\n    this.showImage = !this.showImage;\n}\n</code></pre> <p>}</p> <p>Defines custom types Creating interfaces: - interface keyword - export it Implementing interfaces - implements keyword and interface name - write code for each property and method</p> <p>Encapsulating Component Styles Rather than have an external stylesheet linked to index.html we use 'styles' and 'styleUrls' in the component decorator @Component({     selector: 'pm-products',     templateUrl: './product-list.component.html',     styleUrls: ['./product-list.component.file1.css',                  './product-list.component.file2.css'] })</p> <p>Using Lifecycle Hooks Component lifecycle Create &gt; Render &gt; Create and render children &gt; process changes &gt; destroys it before removing its template from the DOM.</p> <p>OnInit: Perform component initialization, retrieve data OnChanges: Perform action after change to input properties OnDestroy: perform cleanup</p> <p>to use lifecycle hook, you need to implement its interface</p> <p>Bulding a Custom Pipe import { PipeTransform } from '@angular/core'; @Pipe({     name: 'convertToSpaces'; }) export class ConvertToSpacesPipe implements PipeTransform {     transform(value: string, character: string): string {</p> <pre><code>}\n</code></pre> <p>}</p> <p>Using a Custom Pipe Template -</p> {{ product.productCode | convertToSpaces:'-' }} <p>Pipe - transform(value: string, character: string): string { }</p> <p>product.productCode == 1st argument passed into transform() convertToSpaces == pipe name '-' == pipe parameter, the 2nd argument passed into transform()</p> <p>Nesting Components There are 2 ways to use a component and display its template.  1. As a directive       in the index.html file 2. as a Routing target - appears to user that they've traveled to another view</p> <p>A component is nest-able if: - Its template only manages a portion of a larger view - It has a selector - It communicates with its container (optional)</p> <p>If a nested component wants to receive input from its container it must expose a property to its container using @Input -</p>"},{"location":"code/typescript/angular/angular-basics/#starcomponentts","title":"star.component.ts","text":"<p>export class ChildComponent {     @Input() rating: number;     starWidth: number; }</p>"},{"location":"code/typescript/angular/angular-basics/#product-listcomponenthtml","title":"product-list.component.html","text":"<p>Container component uses property binding to pass value into nested component. The only time we can specify a nested components property as a property binding target on the left side of an equals sign is when that properity is decorated with the input decorator. The example above the @Input is combined with 'rating'.</p> <p>Riasing an Event (@Output) - Nested component sends info back to container with @Output     - The only way this can happen is if the property type is an event. The event to pass becomes the event payload. In Ang, an event is defined with an event emmitter object. </p> <p>The only time we can specify a nested components property as an event binding target on the left side of an equals sign is when that properity is decorated with the output decorator.</p>"},{"location":"code/typescript/angular/angular-basics/#starcomponentts_1","title":"star.component.ts","text":"<p>export class StarComponent {   @Input() rating: number;   starWidth: number;   @Output() notify: EventEmitter = new EventEmitter();   // TS allows for generics (see Java notes from cs200)   // the 'string' is identified as the type of the event payload <p>onClick() {     this.notify.emit('clicked');   }   // uses notify event property and uses the emit method to raise the event to the container. If we want to pass data in the event payload, we do so through the emit method.</p>"},{"location":"code/typescript/angular/angular-basics/#starcomponenthtml","title":"star.component.html","text":"... stars ...  <p>Input decorator - used whenever info needs to be passed from container to nested - Attachd to a property of any type - Prefix with @; Suffix with () since it's a function Output decorator - used whenever info needs to be passed from nested to container - Attached to a property declared as an Event Emitter - Use the generic argument to define the event payload type - Use the new keyword to create an instance of Event Emitter - Prefix with @; Suffix with () since it's also a function</p> <p>Container Component Use nested component as a directive - Directive name -&gt; nested component's name Use property binding to pass data to the nested component Use evnt binding to respond to events from the nested component - Use $event to access the event payload passed from the nested component</p> <p>Services and Dependency Injection</p>"},{"location":"code/typescript/angular/angular-basics/#service","title":"Service","text":"<p>A cass with a focused purpose. Used for features that: - Are independent from any particular component - Provide shared data or logic across components - Encapsulate external interactions</p> <p>Service -&gt; Injector -&gt; Component</p> <p>Retrieving Data using HTTP</p>"},{"location":"code/typescript/angular/angular-basics/#setup","title":"Setup","text":"<p>Add HttpClientModule to the imports array of one of the application's Angular Modules.</p>"},{"location":"code/typescript/angular/angular-basics/#service_1","title":"Service","text":"<ul> <li>Import what we need</li> <li>Define a dependency for the http client <ul> <li>use constructor parameter</li> </ul> </li> <li>Ceate a method for each http request</li> <li>Call the desired http method, such as get<ul> <li>pass in the url</li> </ul> </li> <li>use generics to specify the returned type -- this will transform the raw http response to the specified type</li> <li>do error handling as desired</li> </ul>"},{"location":"code/typescript/angular/angular-basics/#subscribing","title":"Subscribing","text":"<ul> <li>Call the subscribe method of the returned observable</li> <li>Provide a function to handle the emitted item<ul> <li>Normally assgns a property to the returned JSON object -- if property is bound to a template the retrieved data appears in the view</li> </ul> </li> <li>Provide an error function to handle any returned errors</li> </ul> <p>Navigation and Routing basics</p>"},{"location":"code/typescript/angular/angular-basics/#creating-a-component-via-angular-cli","title":"Creating a component via Angular CLI","text":"<p>ng g c path/name/product-detail.component --flat ng // angular cli g // generate c // component then name of component (starts with path leading to if applicable) --flat // stops auto generated folder from generated component</p> <p>How to prevent undefined or NULL property errors due to asyncronous loading -- {{pageTitle + ': ' + product?.productName}} ? // safe navigation operator if property is not loaded, '?' returns undefined or NULL and prevents error</p> <p>This is good, but doesn't work with 2 way binding -- [(ngModel)]='product?.productName' // this dont work it's also tedious af if displaying many properties</p> <p>better to use *ngIf='product' on parent div *</p>"},{"location":"code/typescript/angular/angular-basics/#how-routing-works","title":"How Routing Works","text":"<p>Many views take turns appearing on the same page -- we control this with routing. - Configure a route for each component - Define options/options - Tie a route to each option/action - Activate the route basec on the user action - Activating a route displayed the component's view</p> <p>Home // need to configure server to do url rewrites in the html5 format -- no old style internal links 'http://someurl.com/page#link'</p> <p>router looks for route definition --</p> <p>displays template whereever we specify with built in element  </p>"},{"location":"code/typescript/angular/angular-basics/#configuring-routes","title":"Configuring Routes","text":"<p>In app.module.ts -- import { RouterModule } from '@angular/router'; - registers the router service - declares the router directives - exposes configured routes</p> <p>We add RouterModule to imports array but also need to ensure the routes are available to the app by passing routes to RouterModule</p> <p>@NgModule({     imports: [         ...         RouterModule.forRoot([             { path: 'welcome', component: WelcomeComponent },             { path: '', redirectTo: 'welcome', pathMatch: 'full'}, // defines a default route             { path: '', redirectTo: 'welcome', pathMatch: 'full'} /* this is usually the following --             { path: '', component: PageNotFoundComponent } but for simple example it just redirects home */         ])     ...</p> <p>*Things to note -      - There are no leading '/'s     - Order matters. The router uses a 'first match wins' strategy. This means more specific routes should always come before less specific ones. </p> <p>There must be a  tag in the  of index.html. AngularCLI will do this for us. <p>Displaying Components Route or nest?     Nest-able components     - define a selector     - nest in another component     - no route     Routed components     - No selector     - Configure routes     - Tie routes to actions, which activate the route to display the view</p> <p>Doing Routing 1. Configuring the routes Define base element in the index.html file </p> <p>Add RouterModule to NgModule imports array     - Add each route (RouterModule.forRoot)     - Order matters     path: url segment for the rout         - No leading slash         - '' for default rout         - '**' for wildcard rout</p> <p>Most routes include a component; reference to component -- not a string name, not enclosed in quotes</p> <ol> <li>Tying routes to actions Add the [routerLnk] directive as an attribute<ul> <li>Clickable element</li> <li>Encose in square brackets</li> </ul> </li> </ol> <p>Bind to a link parameters array     - First element is the path     - All other elements are route parameters </p> <ol> <li>Placing the View Add the  directive<ul> <li>Identifies where to display the routed component's view</li> </ul> <p>More Routing Techniques</p>"},{"location":"code/typescript/angular/angular-basics/#passing-parameters-to-a-route","title":"Passing Parameters to a Route","text":"<p>Example: specific product detail page (product ID)</p>"},{"location":"code/typescript/angular/angular-basics/#activating-a-route-with-code","title":"Activating a Route with Code","text":""},{"location":"code/typescript/angular/angular-basics/#protcting-routes-with-guards","title":"Protcting Routes with Guards","text":"<p>Modules</p>"},{"location":"code/typescript/angular/angular-basics/#bootstrap-array","title":"Bootstrap Array","text":"<p>Defines the component that is the starting point of the application. - This array should only be used in the root application module, AppModule</p>"},{"location":"code/typescript/angular/angular-basics/#declarations-array","title":"Declarations Array","text":"<p>Every component, directive, and pipe we create is declared by a module.  We use this array of the NgModule decorator to define the components, directives, and pipes that belong to this module. - Every component, directive, and pipe we create must belong to ONE AND ONLY ONE module - Only declare components, directives and pipes. Don't add other classes, services, or modules to the declarations array. - Never re-declare components, drectives, or pipes that belong to another module - All declared components, directives, and pipes are private by default     - only accessible to other components, directives, and pipes declared in the same module - Modules provide the template resolution environment for its component templates</p>"},{"location":"code/typescript/angular/angular-basics/#exports-array","title":"Exports Array","text":"<p>Allows us to share components, directives, and pipes with other modules. - Export any component, directive, or pipe if other components need it - Re-export modules to re-export their components, directives, and pipes. This is useful if consolidating features for multiple modules to build a convenience or shared module. - We can re-export something without importing it first. A module only needs to import The 3 that are required by the components declared in the module, but the module can still provide capabilities to other modules that import it by re-exporting. - Never export a service. Services added to the Providers Array are registered with the root application injector, making them available for injection for any class in the app. </p>"},{"location":"code/typescript/angular/angular-basics/#imports-array","title":"Imports Array","text":"<p>We can extend a module by importing capabilities from other modules. - Importing a module makes available any exported components, directives, and pipes from that module. - Only import what this module needs.  - Importing a module does NOT provid access to its imported modules. In other words, imports are not inherited. However, if the shared module re-exports what it is importing, then it does pass on its imported modules. Think of the structure of module relationships as a box rather than a tree structure</p>"},{"location":"code/typescript/angular/angular-basics/#providers-array","title":"Providers Array","text":"<ul> <li>Any service provider added to the providers array is registered at the root of the application. If you want to encapsulate a service, add it to the component, not the module.</li> <li>Don't add services to the providers array of a shared module. Consider building a CoreModule for services and importing it once in the AppModule.</li> <li>Routing guards must be added to the providers array of a module. Must be at module level for navigation. </li> </ul> <p>Generate A Module via Angular CLI ng g m folder/name-of-module --flat -m some-name.module -m flag says to import it into \"some-name.module\" (assuming it exists already). Generated module then gets added to Imports Array and its file is created and placed in specified location</p> <p>In the Imports Array -- RouterModule.foroot: - Registers Router service - Declares router directives - Exposes configured routes RouterModule.forChild: - Declares router directives - Exposes configured routes</p> <p>Using the forChild, RouterModule knows not to reregister the router service.</p>"},{"location":"code/typescript/angular/angular-basics/#shared-modules","title":"Shared Modules","text":"<p>Saves us from having to continuously import CommonModule, FormsModule, RouterModule, etc</p>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/","title":"Unit Testing in Angular 12","text":"<p>by Joe Eames source: https://www.pluralsight.com/courses/unit-testing-angular</p>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#overview","title":"Overview","text":"<ul> <li> <p>2 major kinds of unit tests: isolated and integration tests</p> </li> <li> <p>e2e is against live running application (selenium)</p> </li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#unit-testing","title":"Unit Testing","text":"<p>A single \"unit\" of code</p> <ul> <li>accepted interpretation of \"unit\" generally is a class</li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#integration-and-functional-testing","title":"Integration and Functional Testing","text":"<p>More than a unit, less than the complete application</p> <ul> <li>Usually 2 units working together</li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#scope","title":"Scope","text":"<p>e2e: Frontend &gt; Webserver &gt; DB int: user component &gt; user service unit: user component  </p>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#angular-integration-tests","title":"Angular Integration tests","text":"<p>Component is made up of ts and html file, these are technically 2 units of code, so these would be int tests</p>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#mocking","title":"Mocking","text":"<p>Dummies: placeholder objects Stubs: object with controllable behavior Spy: keeps track of which of its methods were called, how many times, and what it was called with True mock: complex object that verifies it was used exactly in a specific way  </p>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#types-of-unit-tests","title":"Types of unit tests","text":""},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#isolated","title":"Isolated","text":"<ul> <li>Single unit of code: usually class</li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#integration-test","title":"Integration test","text":"<p>Test components in the context of a module (component + template)</p> <ul> <li>shallow</li> <li>deep (with child components)</li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#writing-unit-tests","title":"Writing unit tests","text":"<ul> <li>beforeEach must contain code to reset conditions for each new test. Prevents pollution and bleed-over</li> <li>Should be in the following order in each test (\"it\"): arrange -&gt; act -&gt; assert</li> <li>The act should generally not be in the beforeEach()</li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#structuring-tests","title":"Structuring tests","text":"<p>AAA Pattern:</p> <ul> <li>Arrange all necessary preconditions and inputs</li> <li>Act on the object or class</li> <li>Assert that the expected results have occured</li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#damp-vs-dry","title":"DAMP vs DRY","text":"<ul> <li>DRY (don't repeat yourself) remove duplication</li> <li>DAMP - mostly DRY but repeat yourself as necessary</li> <li>A test shoould be a complete story, all within the it()</li> <li>You shouldn't need to look around much to understand the test</li> <li>Techniques:<ul> <li>Move less interesting setup into beforeEach()</li> <li>Keep critical setup within the it()</li> <li>Include Arrange, Act, and Assert inside the it()</li> </ul> </li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#isolated-unit-tests","title":"Isolated Unit Tests","text":"<ul> <li>No template interaction. Purely calling on functions to test input/output behavior</li> <li>Separate out the 3 A's by grouping them together inside the it() and separating them by a blank space</li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#mock-services","title":"Mock Services","text":"<ul> <li>Can mock a service by creating jasmine \"spy\" object -&gt; <code>mockService = jasmine.createSpyObj('method1', 'method2', 'method3')</code></li> </ul> <p>You can do a \"state\" test that checks on the state of the component (x = y after z happens) or do an interaction test (x called y in v class/service with the right arguments after z happened)</p>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#shallow-integration-tests","title":"Shallow Integration Tests","text":"<ul> <li>Normally both shallow/isolated tests will be in the same .spec file. Only do one .spec file per component/template</li> <li>As of 09/2018 you still need to set source-maps to false in the test command because of how Karma and zone.js interact, producing misleading error messages that cover up the real cause of errors that may occur in unit tests</li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#the-testbed","title":"The TestBed","text":"<p>Core testing utility used to create special module just for testing.</p> <ul> <li>TestBed.createComponent() returns a ComponentFixture which is a wrapper for a component which includes more properties to help with testing</li> <li>fixture.componentInstance is the actual instance of the component within the fixture wrapper</li> <li>Only import the modules you need this will significantly speed up the time it takes for tests to run</li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#using-no_errors_schema","title":"Using NO_ERRORS_SCHEMA","text":"<p>Testing module doesn't know about routerLink directive. We don't want to have to deal with this for 2 reasons:</p> <ol> <li>Creating a mock will be a big headache because AppRoutingModule is complex</li> <li>We don't want a live AppRoutingModule because the app may try to route for real during tests</li> </ol> <p><code>NO_ERRORS_SCHEMA</code> tells angular to ignore unknown attributes and elements. USE SPARINGLY since this can very easily cover up issues and causes of issues. Other ways to deal with this routerLink error are mentioned below.</p>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#testing-rendered-html","title":"Testing Rendered HTML","text":"<pre><code>fixture = TestBed.createComponent(HeroComponent);\nfixture.nativeElement\n</code></pre> <p>nativeElement gets a handle to the DOM element that represents the container for the template</p> <ul> <li>Better to use toContain vs toEqual to prevent tests from being brittle </li> <li>After properties are set we need change detection to execute in order to pick up DOM properties</li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#nativeelement-vs-debugelement","title":"NativeElement vs DebugElement","text":"<ul> <li>NativeElement exposes DOM api</li> <li>DebugElement like NativeElement but is more of a wrapper that includes more properties to help with testing.</li> </ul> <pre><code>fixture.debugElement.query(By.css('a')).nativeElement.textContent\n// is the same as\nfixture.nativeElement.querySelector('a').textContent\n// they do the same thing but get there different ways\n</code></pre> <p>Why bother with DebugElement then? You have more properties to help with testing. For example, if you wanted to test routerLink, you could access that property via DebugElement</p>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#mocking-an-injected-services","title":"Mocking an Injected Services","text":"<p>How we've been doing it is good enough</p> <pre><code>TestBed.configureTestingModule({\n    ...\n    providers: [\n        { provide: RealService, useValue: mockService }\n    ]\n});\n</code></pre>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#deep-integration-tests","title":"Deep Integration Tests","text":"<p>Tests interaction between component and child components</p> <p>In angular, a component is actually a subclass of a directive. Directives seem to be attributes like routerLink and components seem to be elements <code>&lt;app-hero ..&gt;&lt;/app-hero&gt;</code> but in the inner workings of angular a directive is actually the parent class for attirbute directives and components.</p> <ul> <li>Adding child components to the declarations will call into question their dependences too, so it might be best to import module instead of each individual component (if it's not too much bloat)</li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#integration-tests-of-services","title":"Integration Tests of Services","text":"<ul> <li>Need to import HttpClientTestingModule and HttpTestingController</li> <li>Need to get handle to mock http handle service so we can adjust it and control it inside our test. This is where HttpTestingController comes in</li> </ul> <p>can do either of the following:</p> <pre><code>beforeEach(() =&gt; {\n    // ...\n    service = TestBed.get(YourService)\n    httpTestingController = TestBed.get(HttpTestingController)\n});\n\n// or \n\nit('should do something', \n    inject([YourService, HttpTestingController], \n        {service: YourService, controller: HttpTestingController} =&gt; {\n    ...        \n}));\n</code></pre> <p>The latter is disgustingly verbose so stick with using TestBed.get()</p>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#testing-dom-interaction-and-routing-components","title":"Testing DOM Interaction and Routing Components","text":""},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#triggering-events-on-elements","title":"Triggering Events on Elements","text":"<p>This is a deep integration thing, unless we include the element in a mock child component. </p> <ul> <li>Call spyOn component method you're testing </li> <li> <p>Call with .and.callThrough() if we're setting a return value in the service stub, else call with .and.returnValue() and pass in of(mockData) with mockData being a require('some-json-file'). That way when we perform the act we can assert that the method we're testing on is called with the correct data.</p> </li> <li> <p>Trigger click event in child component with something like the following:</p> </li> </ul> <pre><code>const heroComponents = fixture.debugElement.queryAll(By.directive(HeroComponent));\nheroComponents[0].query(By.css('button'))\n  .triggerEventHandler('click', {stopPropagation: () =&gt; {}})\n\nexpect(fixture.componentInstance.delete).toHaveBeenCalledWith(HEROES[0]);\n</code></pre> <p>The TOH app is used in the tutorial as an example and they call $event.stopPropagation(). We wont need that since we make all our button types=\"button\". We can just pass null instead of that empty stopPropagation dummy function inside triggerEventHandler.</p>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#emitting-events-from-children","title":"Emitting Events from Children","text":"<p>We care more about the bindings between the parent and child components, so triggering a button click in the child component is kinda redundant and unnecessary. </p> <p>A cleaner alternative is to trigger the event emit directly from the child's EventEmitter property:</p> <pre><code>const heroComponents = fixture.debugElement.queryAll(By.directive(HeroComponent));\n(&lt;HeroComponent&gt;heroComponents[0].componentInstance).delete.emit(undefined);\n</code></pre> <p>This produces the exact same results as the button click above.</p> <ul> <li>ultimately we don't care about the value that's emitted up because that has nothing to do with the current (parent) component. We can set that ourselves and verify that once the method we're testing is called it's being called with the value we set earlier in the test</li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#raising-events-on-child-directives","title":"Raising Events on Child Directives","text":"<p>3rd and final technique on testing interaction between parent and child components</p> <pre><code>const heroComponents = fixture.debugElement.queryAll(By.directive(HeroComponent));\nheroComponents[0].triggerEventHandler('delete', null)\n</code></pre> <ul> <li>This is the most shallow option since we're telling the DebugElement to trigger the event rather than telling the child component to raise or emit its event. We don't even have to know if the child component actually has a delete EventEmitter but we tell the DebugElement to call the delete EventEmitter, whether it's really there or not for the child, from the child. Another perk to using the DebugElement is the layer of abstraction gives us more flexibility so we can do things like this.</li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#interacting-with-input-boxes","title":"Interacting with Input Boxes","text":"<p>This is the kind of thing we do in both shallow and deep integration tests.</p> <p>Arrange:</p> <ul> <li>getHeroes</li> <li>fixture.detectChanges()</li> <li>set \"name\" value to whatever string</li> <li>inputElement -&gt; capture input box with fixture.debugElement.query(By.css(...)).nativeElement since we want the underlying DOM element</li> <li>addButton -&gt; capture button's debugElement (don't want underlying DOM element since we're going to call debugElement triggerEventHandler fixture.debugElement.queryAll(By.css('button'))[0];</li> </ul> <p>Act:</p> <ul> <li>call addHero mockHeroService.addHero.and.returnValue(of({id: 5, name: name, strength: 4}));</li> <li>inputElement.value = name</li> <li>addButton.triggerEventHandler('click', null)</li> <li>To check if html elements are updated we need to set off change detection: fixture.detectChanges()</li> </ul> <p>Assert:</p> <ul> <li>Grab all textContent of <code>&lt;ul&gt;</code> const heroText = fixture.debugElement.query(By.css('ul')).nativeElement.textContent;</li> <li>Check contents of returned string expect(heroText).toContain(name);</li> </ul>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#testing-with-activatedroute","title":"Testing with ActivatedRoute","text":"<p>Don't test the framework! Test your code.</p> <ul> <li>Assume the framework works correctly.</li> <li>You want to test to see if your code is interacting with the framework correctly.</li> <li>Test that you're calling or configuring routes correctly, not that the routes actually work.</li> <li>For this reason we don't let our code actually route</li> </ul> <p>If we wanted to mock ActivatedRoute we could do the following:</p> <p>To mock</p> <pre><code>+this.route.snapshot.paramMap.get('id');\n</code></pre> <p>we would write</p> <pre><code>const mockActivatedRoute = {\n  snapshot: { paramMap: { get: () =&gt; { return '3': }}}\n};\n</code></pre> <p>In the beforeEach(), then in the TestBed.configureTestingModule</p> <pre><code>providers: [\n  {provide: ActivatedRoute, useValue: mockActivatedRoute},\n  {provide: Location, useValue: mockLocation}\n]\n</code></pre> <p>Note: \"Location\" is a global variable in the javascript library (Window.location, Document.location). Don't use this since it's not part of the framework. Import it from @angular/common.</p> <p>Random Side Note: If using ngModel in any other templates and you get an error for it not being a property of \"input\", make sure you import FormsModule into your spec file. No configuration or anything should be needed.</p>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#mocking-the-routerlink","title":"Mocking the RouterLink","text":"<p>schemas: [NO_ERRORS_SCHEMA] in the TestBed.configureTestingModule method was stopping errors related to the routerLink. We should mock the routerLink instead of using this by creating a stub with a @Directive decorator:</p> <pre><code>@Directive({\n  selector: '[routerLink]',\n  host: { '(click)': 'onClick()' }\n})\n\nexport class RouterLinkDirectiveStub {\n  @Input('routerLink') linkParams: any;\n  navigatedTo: any = null;\n\n  onClick() {\n    this.naviagedTo = this.linkParams;\n  }\n}\n</code></pre> <p>This is a template noted in the official documentation to use for testing directives. You can put it in its own file like we've been doing or you can include it directly in the spec file above the first describe() but after the imports.</p> <p>Putting it to use:</p> <pre><code>    it('should have the correct route for the first hero', () =&gt; {\n      mockHeroService.getHeroes.and.returnValue(of(HEROES)); // set initial data\n      fixture.detectChanges(); // trigger change detection\n      const heroComponents = fixture.debugElement.queryAll(By.directive(HeroComponent));\n\n      let routerLink = heroComponents[0]\n                        .query(By.directive(RouterLinkDirectiveStub))\n                        .injector.get(RouterLinkDirectiveStub);\n\n      heroComponents[0].query(By.css('a')).triggerEventHandler('click', null);\n\n      expect(routerLink.navigatedTo).toBe('/detail/1');\n    });\n\n.query(By.directive(RouterLinkDirectiveStub)) gets DebugElement for the anchor tag that has this directive\n.injector.get(RouterLinkDirectiveStub) is a handle to the actual directive\n</code></pre>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#advanced-topics","title":"Advanced Topics","text":""},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#adding-async-code","title":"Adding Async Code","text":"<p>So far we've been making our observables behave syncronously so we haven't had to deal with testing asyncrounous code. </p> <p>We can pass \"done\" to our tests to tell jasmine we're dealing with asyncronous code. Here's one example if we were to use debounceTIme to prevent many clicks from causing a bunch of API calls for save(). save() calls heroService.updateHero but instead of calling it right away we'll wrap it in debounceTime with a 250ms delay. For the test we'd do something like the following:</p> <pre><code>it('should call updateHero when save is called', (done) =&gt; {\n  mockHeroService.updateHero.and.returnValue(of({})); \n  fixture.detectChanges();\n\n  fixture.componentInstance.save();\n\n  setTimeout(() =&gt; {\n    expect(mockHeroService.updateHero).toHaveBeenCalled();\n    done();\n  }, 300);\n}); \n</code></pre>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#fakeasync","title":"fakeAsync","text":"<p>The problem with putting setTimeout places is it'll dramatically slow down tests. Angular provides a solution to this by giving us fakeAsync from @angular/core/testing</p> <p>We would change the above to somethign like this:</p> <pre><code>it('should call updateHero when save is called', fakeAsync(() =&gt; {\n  mockHeroService.updateHero.and.returnValue(of({})); \n  fixture.detectChanges();\n\n  fixture.componentInstance.save();\n  tick(250);\n\n  expect(mockHeroService.updateHero).toHaveBeenCalled();\n})); \n</code></pre> <p>This is possible because Angular runs inside zone.js and the fakeAsync wrapped code runs inside a special zone that zone.js will create that allows us to control the clock inside of it.</p> <p>What if we don't know exactly how long we need to wait? We use flush() instead. It says \"look at the zone and see if there are any tasks that are waiting. If there are, fast forward the clock until those tasks have been executed\".</p>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#async","title":"async","text":"<p>This is meant to be used with Promises</p> <p>We would change the above to something like this:</p> <pre><code>it('should call updateHero when save is called', async(() =&gt; {\n  mockHeroService.updateHero.and.returnValue(of({})); \n  fixture.detectChanges();\n\n  fixture.componentInstance.save();\n\n  fixture.whenStable().then(() =&gt; {\n    expect(mockHeroService.updateHero).toHaveBeenCalled();\n  });\n})); \n</code></pre>"},{"location":"code/typescript/angular/pluralsight-unit-testing-in-angular/#course-summary-key-takeaways","title":"Course Summary: Key Takeaways","text":"<ul> <li>Isolated tests should be the preference because they're the quickest and least brittle to write</li> <li>If you want to test template interaction witck with shallow integration tests</li> <li>Only use deep integration tests when absolutely necessary</li> </ul>"},{"location":"code/typescript/angular/signals/","title":"Signals","text":"<p>2023-06-11</p> <p>A new way to handle to handle reactivity in Angular. </p>"},{"location":"code/typescript/angular/signals/#how-does-it-work","title":"How does it work?","text":"<p>Centers around 3 core concepts: signal, computed, and effect.</p> <p>Signal: As it sounds. Sends out a signal whenever the indicated value changes Computed: Gets a derived values (like what the pipe operators do in RxJS) Effects: Sets off side-effects (like tap does with RxJS)</p> <pre><code>export class AppComponent {\n    count = signal(0);\n    double = computed(() =&gt; this.count() * 2);\n\n    changeCount() {\n        this.count.set(5);\n    }\n}\n</code></pre>"},{"location":"code/typescript/angular/signals/#why-not-just-use-rxjs-subjects","title":"Why not just use RxJS Subjects?","text":"<ul> <li>The syntax is simpler</li> <li>There are no Observables, so no subscription </li> <li>It comparmentalizes change detection to that one component by default (Rather than messing with ChangeDetectionStrategy as is needed with RxJS)</li> <li>It closes the gap between imperative and reactive declarative coding styles in Angular<ul> <li>As of now you choose a path: imerative or declarative</li> <li>With signals you start simple with reactive/imperative and add complexity later with reactive/declarative</li> </ul> </li> </ul> <p>This is a big deal for the framework because it makes reactive programming more approachable and it opens up the possibility of no longer needing to rely on Zone.js which is kind of a hack solution to get the framework to detect changes based on communication with the browser (interfaces with browser which tells Zone something in the Event Loop is happening).</p>"},{"location":"code/typescript/angular/signals/#so-is-rxjs-going-away","title":"So is RxJS going away?","text":"<p>No</p> <p>Signals is good for synchronous reactivity (incrementation or updates when the value is immediately known) but RxJS still handles asynchronous reactivity better.</p> <p>There may be some cases where you want to translate a signal to an observable and vica-versa. That's possible:</p> <pre><code>// signal\nsearchTerm = signal('');\n\n// observable\nemployees$ = fromSignal(this.searchTerm.pipe(\n    debounceTime(300),\n    distinctUntilChanged(),\n    switchMap((switchMap) =&gt; this.employeeSearch.search(searchTerm)),\n    retryCount(5),\n));\n\n// signal (allows hooking back into signal change detection mechanism)\nemployees = fromObservable(this.employees$);\n</code></pre>"},{"location":"code/typescript/javascript/js-in-depth/","title":"JS In Depth","text":""},{"location":"code/typescript/javascript/js-in-depth/#can-you-name-two-programming-paradigms-important-for-javascript-app-developers","title":"Can you name two programming paradigms important for JavaScript app developers?","text":"<ul> <li>Prototypal inheritance (also: prototypes, OLOO).</li> <li>Functional programming (also: closures, first class functions, lambdas).</li> </ul>"},{"location":"code/typescript/javascript/js-in-depth/#functional-programming","title":"Functional Programming","text":"<ul> <li>Pure functions / function purity.</li> <li>Avoid side-effects.</li> <li>Simple function composition.</li> <li>Examples of functional languages: Lisp, ML, Haskell, Erlang, Clojure, Elm, F Sharp, OCaml, etc\u2026</li> <li>Mention of features that support FP: first-class functions, higher order functions, functions as arguments/values.</li> </ul> <p>A higher order function is a function that takes a function as an argument, or returns a function. Higher order function is in contrast to first order functions, which don\u2019t take a function as an argument or return a function as output.</p>"},{"location":"code/typescript/javascript/js-in-depth/#what-is-the-difference-between-classical-inheritance-and-prototypal-inheritance","title":"What is the difference between classical inheritance and prototypal inheritance?","text":"<p>classical is like all OOP but 'this' works weird in JS and the context can get lost if not used properly (same with 'new'). Luckily JS allows for prototypal inheritance which uses factory functions to avoid the need for the keyword 'this' or the .bind() function to lock in the context of 'this'</p>"},{"location":"code/typescript/javascript/js-in-depth/#memoization","title":"Memoization","text":"<p>A memoized function caches results from some set of specific inputs</p>"},{"location":"code/typescript/javascript/js-in-depth/#proxies","title":"Proxies","text":"<p>Fancy getters and setters</p>"},{"location":"code/typescript/javascript/js-in-depth/#pass-by-reference-or-value","title":"Pass by reference or value","text":"<p>JavaScript is always pass by value - they pass objects as references and those references as value. This can be explained by saying pass by reference is actually passing in the memory location of the object. If you change a property the original object changes because you changed it at the original memory location (that is not reassigned)</p>"},{"location":"code/typescript/javascript/js-in-depth/#call-apply-bind","title":"call, apply, bind","text":"<p>All 3 give a function context for \"this\".</p> <pre><code>const obj = { things: 3 }\n\nconst addThings = (a,b,c) =&gt; {\n  return this.things + a + b + c;\n}\n\nconsole.log( addThings.call(obj, 1,2,3) );\n// 9\n</code></pre> <p>.apply is the same except the 2nd argument is an array instead of individual arguments</p> <p>.bind doesn't execute but returns a function bound to an object</p> <pre><code>console.log( addThings.bind(obj, 1,2,3)() );\n</code></pre> <p>will execute returned function</p>"},{"location":"code/typescript/javascript/js-in-depth/#explain-the-event-loop","title":"Explain the event loop","text":"<p>Every function called gets put on the stack. Asyncronous functions like setTimeout or XHR functions or any Web API function pulls the function out of the stack into the Event table while the stack (queue) continues to execute. Once the stack is cleared the event loop throws the pulled out function back onto the stack</p>"},{"location":"code/typescript/javascript/js-in-depth/#arrow-functions","title":"Arrow functions","text":"<p>All arrow functions are anonymous so nothing is bound: not 'this' or arguments passed in. Benefits:</p> <ul> <li>smaller</li> <li>can be written inline and still highly readable</li> <li>easier to make single purpose</li> </ul>"},{"location":"code/typescript/javascript/js-in-depth/#this-keyword","title":"'this' keyword","text":"<p>is really weird in part from its origins starting out heavily influenced by functional language Scheme (where the concept doesn't exist) and then OOP language Java</p> <pre><code>const dog = {\n  sound: 'woof',\n  talk: function() {\n    console.log(this.sound)\n  }\n}\n\ndog.talk() //\"woof\"\nlet talkFunction = dog.talk\ntalkFunction() //undefined\n</code></pre> <p>'this' loses context when the dog's method is assigned to a variable. The 'this' word in functions gets context from where it's called rather than where it's defined. We can bind it to the context we want with the 'bind' function</p> <pre><code>let boundFunction = talkFunction.bind(dog)\nboundFunction() //\"woof\"\n</code></pre> <p>'this' can also be given context by assigning function to object:</p> <pre><code>let cat = {\n  sound: 'meow',\n  noise: talk\n}\ncat.noise() //\"meow\"\n</code></pre> <p>These are all referencing the same function; they are not making copies let yowl = cat.noise yowl() //undefined, because it's pointing to the original talk() function but without context now. This is because we never explicitly binded talk() to cat.  let bark = boundFunction() //\"woof\"</p>"},{"location":"code/typescript/javascript/js-in-depth/#js-is-an-interpreted-language","title":"JS is an interpreted language","text":"<p>so everything is a runtime error - there is no such thing as compile time errors. Typescript is compiled (transpiled) down to JS so it can have compile errors like assignment to wrong type or tslint failure</p>"},{"location":"code/typescript/javascript/js-in-depth/#var-vs-let","title":"'var' vs 'let'","text":"<p>Main difference is scoping rules. Variables declared by var keyword are scoped to the immediate function body (hence the function scope) while 'let' variables are scoped to the immediate enclosing block denoted by { } (hence the block scope).</p> <p>let cannot be used to create a property on the global object (window) but var can</p> <p>var can be redeclared with var but let cannot and will raise SyntaxError</p>"},{"location":"code/typescript/javascript/js-in-depth/#hoisting","title":"Hoisting","text":"<p>functions can be called before declaration.</p> <p>variables declared with var keyword are \"hoisted\" to the top of the block. Note only the declaration is hoisted but the value assignment stays put. They will be 'undefined'. Doing the same with 'let' will give a ReferenceError</p>"},{"location":"code/typescript/javascript/js-in-depth/#async-await","title":"async, await","text":"<p>Inside a function marked as 'async', you're allowed to place the 'await' keyword in front of an expression that returns a promise. When you do, the execution of the 'async' function is paused until the promise is resolved. Allows the user to write asyncronous code in a more syncronous manner. Allows user to have promises resolve sequentially instead of in parallel like raw promises.</p>"},{"location":"code/typescript/javascript/js-in-depth/#linkedlist-vs-array","title":"LinkedList vs Array","text":"<p>They're stored in memory differently</p> <ul> <li>Arrays are stored contiguously so they're very easy to traverse</li> <li>LL's have sequential access and may be stored anywhere in memory, therefore must be traversed 1 at a time</li> </ul> <p>The main advantage of a LL is adding/removing items is much faster. If something needs to be added to or removed from an array everything to the right of it must be moved over. LL you just change where pointers are pointing</p>"},{"location":"code/typescript/javascript/js-in-depth/#prototypes","title":"Prototypes","text":"<p>Classes are like blueprints while prototypes are like delegates in politics. Representative of the masses. You basically make an object as the prototype and bind it to another object using <code>Object.setPrototypeOf(newObj, oldObj)</code>.  It's important to note that we're not instantiating a new object from a blueprint or making a copy like we do with classical inheritance. In this case we're delegating access between objects by creating a reference to an original object. We're saying newObj has access to oldObj properties.</p> <pre><code>function talk() {\n  console.log(this);\n  console.log(this.sound);\n}\nlet animal = {\n  talk: talk\n}\nlet cat = {\n  sound: 'meow!'\n}\nObject.setPrototypeOf(cat, animal);\nanimal.talk = function() {\n  console.log('Hello, I am animal');\n}\ncat.talk(); // prints \"Hello, I am animal\" because 'animal.talk' changed\n</code></pre> <p>See /home/swhitmore/Desktop/protoype.js for an example. Note: Object.setPrototypeOf isn't typically used in real apps. It's usually 'new' or 'Object.create'</p>"},{"location":"code/typescript/javascript/js-in-depth/#new-keyword","title":"'new' keyword","text":"<p>This was made for people who are more familiar with classical inheritance.</p> <pre><code>// We're pretending this is ES5\nfunction Person(saying) {\n  this.saying = saying\n}\nPerson.prototype.talk = function() {\n  console.log('I say:', this.saying)\n}\n\n// recreate 'new' to understand what it does\nfunction new(constructor) {\n  var obj {}\n  Object.setPrototypOf(obj, constructor.prototype)\n  var argsArray = Array.from(arguments)\n  //var argsArray = Array.prototype.slice.apply(arguments) //ES5 since Array.from is ES6+\n  constructor.apply(obj, argsArray.slice(1))\n  return obj\n}\n\nvar crockford = new Person('SEMICOLONS!!!!!')\ncrockford.talk()\n</code></pre> <p>the 'new' keyword creates an object. JS looks at the name of the function you're calling new on and uses it as a constructor. Adding a method or property to that object after the fact you use the 'prototype' keyword. 'new' does 4 things:</p> <ol> <li>Creates a new object</li> <li>Sets the prototype</li> <li>Execute constructor with \"this\"</li> <li>Return the created object (assuming the constructor DOES NOT return something)</li> </ol>"},{"location":"code/typescript/javascript/js-in-depth/#proto","title":"proto","text":"<p>points to object's prototype object <code>crockford.__proto__</code> will be the simple object the Person function creates. The 'prototype' property (like in Person() above) only exists for constructor functions.</p>"},{"location":"code/typescript/javascript/js-in-depth/#objectcreate","title":"Object.create","text":"<p>A static method on the Object prototype that creates a new object with the prototype set to a certain object. Why does it exist when there's the 'new' keyword? Because it's more natural to the prototype model. The problem with 'new' is it doesn't behave exactly the same as how you'd expect if you're used to regular OOP languages. We should be using this instead of 'new' and 'Object.setPrototypeOf()' because the performance is bad compared to 'Object.create'  One idea for prototypes is to have an 'init' function that returns the object. This will allow for easy chaining:</p> <pre><code>const cat = {\n  init: function(sound) {\n    this.sound = sound\n    return this\n  },\n  makeSound: function() {\n    console.log(this.sound)\n  }\n}\nconst waffles = Object.create(cat).init('meeooww')\nwaffles.makeSound() //\"meeooww\"\n</code></pre>"},{"location":"code/typescript/javascript/js-in-depth/#class-keyword","title":"'class' keyword","text":"<p>Does everything above under the hood and acts as syntatic sugar for OOP programmers Why use this keyword? Depending on how you design things. If you're making things with an 'is a' perspective then it should be classical inheritance. If it's a 'has a' perspective then it should be prototypal structure. Working with OOP devs (Java shop) and it being Angular which uses Typescript we always go with classical inheritance. </p>"},{"location":"code/typescript/javascript/random-javascript-goodies/","title":"Random JavaScript Goodies","text":"<p>Things I dont use often but come in handy.</p> <pre><code>// get font size of selected element\nconst a = document.querySelectorAll('nav ul li a')[0]\nconst fontSize = window.getComputedStyle(a, null).getPropertyValue('font-size');\nconsole.log(fontSize); // \"16px\"\n</code></pre>"},{"location":"code/typescript/javascript/pwa/pwa-overview/","title":"Progressive Web Apps","text":"<p>A regular web app with a few layers added on</p> <ul> <li>Service Workers</li> <li>App shell</li> <li>Manifest</li> </ul>"},{"location":"code/typescript/javascript/pwa/pwa-overview/#service-workers","title":"Service Workers","text":"<p>Runs independently of frontend and server so it can operate even if the app is closed. </p> <p>Requires HTTPS (unless running locally)</p> <p>The Fetch API &amp; Promises are an essential part of working with Service Workers in a modern Progressive Web App.</p>"},{"location":"code/typescript/javascript/pwa/pwa-overview/#registering-a-service-worker","title":"Registering a service worker","text":"<pre><code>if (navigator.serviceWorker) {\n\n  // Register the SW\n  navigator.serviceWorker.register('/sw.js').then(function(registration){\n\n    console.log('SW Registered');\n\n  }).catch(console.log);\n\n}\n</code></pre>"},{"location":"code/typescript/javascript/pwa/pwa-overview/#intercepting-requests","title":"Intercepting requests","text":"<p>main.js</p> <pre><code>fetch('camera_feed.html')\n  .then((res) =&gt; {\n  return res.text();\n}).then((html) =&gt; {\n  document.getElementById('camera').innerHTML = html;\n});\n</code></pre> <p>sw.js</p> <pre><code>  if (e.request.url.endsWith('/camera_feed.html')) {\n\n    e.respondWith(\n      fetch(e.request)\n        .then((res) =&gt; {\n\n          if( res.ok ) return res;\n\n          return new Response('Camera feed currently not available.')\n        })\n    )\n  }\n</code></pre> <p>The above will intercept the fetch request, return it if 200 or print alternative text if failed.</p>"},{"location":"code/typescript/javascript/pwa/pwa-overview/#scope","title":"Scope","text":"<p>Service Workers can be scoped to certain directories. For example, say you've got the following project structure:</p> <pre><code>|-- posts\n  |-- index.html\n  |-- post1.html\n|-- index.html\n|-- main.js\n|-- style.css\n|-- sw.js\n</code></pre> <p>This will scope it to just \"posts\"</p> <pre><code>if (navigator.serviceWorker) {\n\n  // Register the SW\n  navigator.serviceWorker.register('/sw.js', {scope: '/posts/'}).then(function(registration){\n\n    console.log('SW Registered');\n\n  }).catch(console.log);\n\n}\n</code></pre>"},{"location":"code/typescript/javascript/pwa/pwa-overview/#serviceworkerregistration","title":"ServiceWorkerRegistration","text":"<p>Represents the service worker registration and has methods to tap into the Service Worker lifecycle.</p> <p>main.js</p> <pre><code>if (navigator.serviceWorker) {\n\n  // Register the SW\n  navigator.serviceWorker.register('/sw.js').then(function(registration){\n\n    registration.onupdatefound = () =&gt; {\n      console.log(\"New SW Found\");\n      let newSW = registration.installing;\n\n      newSW.onstatechange = () =&gt; {\n        console.log(newSW.state);\n      }\n    };\n\n  }).catch(console.log);\n\n}\n</code></pre> <p>sw.js</p> <pre><code>self.addEventListener('install', (e) =&gt; {\n\n  e.waitUntil(new Promise((resolve) =&gt; {\n    setTimeout(resolve, 5000);\n  }))\n});\n\nself.addEventListener('activate', () =&gt; {\n  console.log('SW2 Active');\n});\n</code></pre> <p>Outputs:</p> <p>New SW Found // After 5 seconds: SW2 Active installed activating activated</p>"},{"location":"code/typescript/javascript/pwa/pwa-overview/#messages","title":"Messages","text":"<p>Service Workers can send messages to their clients.</p> <p>main.js</p> <pre><code>if (navigator.serviceWorker) {\n\n  // Register the SW\n  navigator.serviceWorker.register('/sw.js').then(function(registration){\n\n    if (registration.active) {\n      registration.active.postMessage('respond to this');\n    }\n\n  }).catch(console.log);\n\n  navigator.serviceWorker.addEventListener('message', (e) =&gt; {\n    console.log(e.data);\n  });\n\n}\n</code></pre> <p>sw.js</p> <pre><code>self.addEventListener('message', (e) =&gt; {\n\n  // Respond to all clients\n  self.clients.matchAll().then((clients) =&gt; {\n\n    clients.forEach((client) =&gt; {\n\n      client.postMessage('moo')\n\n      // Only respond to sending client\n      if (e.source.id === client.id) {\n        client.postMessage(\"Private Hello from Service Worker\");\n      }\n    });\n  });\n\n});\n</code></pre> <p>The above will output \"moo\" to all clients (tabs or windows). The final if block will only send a message to the active tab.</p>"},{"location":"code/typescript/javascript/pwa/pwa-overview/#push-notifications","title":"Push Notifications","text":"<p>There are usually 3 settings for Notification API:</p> <ol> <li>granted</li> <li>denied</li> <li>default (whatever the browser default is)</li> </ol> <p>main.js</p> <pre><code>if (window.Notification) {\n\n  function showNotification() {\n\n    let notificationOpts = {\n      body: 'Some notification information.',\n      icon: '/icon.png'\n    }\n\n    let n = new Notification('My new Notification.', notificationOpts);\n\n    n.onclick = () =&gt; {\n      console.log('Notification Clicked');\n    }\n  }\n\n  // Manage permission\n  if (Notification.permission === 'granted') {\n    showNotification();\n\n  } else if (Notification.permission !== 'denied') {\n\n    Notification.requestPermission( (permission) =&gt; {\n\n      if (permission === 'granted') {\n        showNotification();\n      }\n    });\n\n  }\n}\n</code></pre>"},{"location":"code/typescript/javascript/pwa/pwa-overview/#server-push","title":"Server Push","text":"<p>Client has a public key. You subscribe to the service worker's push manager where you recieve a private key. Post it to the push server for it to use to push notifications:</p> <p>Generate the keys with the <code>web-push</code> package:</p> <pre><code>$ npm i web-push\n</code></pre> <p>Then generate the keys by calling its CLI script directly:</p> <pre><code>node_modules/web-push/src/cli.js generate-vapid-keys --json\n</code></pre> <p>main.js</p> <pre><code>if (navigator.serviceWorker) {\n\n  // Convert key to Uint8Array\n  function urlBase64ToUint8Array(base64String) {\n    const padding = '='.repeat((4 - base64String.length % 4) % 4);\n    const base64 = (base64String + padding)\n      .replace(/\\-/g, '+')\n      .replace(/_/g, '/');\n\n    const rawData = window.atob(base64);\n    const outputArray = new Uint8Array(rawData.length);\n\n    for (let i = 0; i &lt; rawData.length; ++i) {\n      outputArray[i] = rawData.charCodeAt(i);\n    }\n    return outputArray;\n  }\n\n  // Register the SW\n  navigator.serviceWorker.register('/sw.js').then((registration) =&gt; {\n\n    // Server public key\n    let pubKey = 'BA_kcDJ9MyfRQ1QBYmrrBv-PzcUfmBFfm_9UebAp1nm5WK5VFgUgLYsMgda0539pVuUXMf3O4gHfUI5kjHGNteM';\n\n    registration.pushManager.getSubscription().then((sub) =&gt; {\n\n      // If subscription found, return\n      if (sub) return sub;\n\n      let applicationServerKey = urlBase64ToUint8Array(pubKey);\n\n      // Subscribe\n      return registration.pushManager.subscribe({userVisibleOnly: true, applicationServerKey});\n\n    }).then( sub =&gt; sub.toJSON() )\n      .then(console.log)\n      .catch(console.log);\n\n  }).catch(console.log);\n\n}\n</code></pre> <p>sw.js</p> <pre><code>self.addEventListener('push', (e) =&gt; {\n\n  let n = self.registration.showNotification('A notification from the SW.');\n  e.waitUntil(n);\n});\n</code></pre> <p>A simple example of a push server would be the following:</p> <p>push-server.js</p> <pre><code>const webpush = require('web-push');\nconst vapid = require('./vapid.json');\n\n// Configure keys\nwebpush.setVapidDetails(\n  'mailto:ray@stackacademy.tv',\n  vapid.publicKey,\n  vapid.privateKey\n);\n\nconst pushSubscription = {\n  // these values are from the console output provided by main.js\n  endpoint: 'https://fcm.googleapis.com/fcm/send/dGXO0NzfwcI:APA91bHrEAx4iw5gstMW_532y2UlphKe2cIb2jI4ukhgbZ6VLWmHy4-AmFPyHxWYJdV3nS7VTmfxElI9Fh7W9hf8PudqFtuXEyRxxVXXp7TWfYzYM_f889bnVoyvvHEve512_BqCeyNA',\n  keys: {\n    auth: 'aAScQk97nu6MBAeC4PTWkw',\n    p256dh: 'BPrlv6gx-ij00Z6c0HG5xtPl0nvwNqsV1lsCly6xq-ckKW-GpMltph6Pd5fK7f0aTUjzJS8OdWt36rIjDKA_suo'\n  }\n};\n\nwebpush.sendNotification(pushSubscription, 'A notification from the push server');\nconsole.log('Push sent to client');\n</code></pre> <p>Run <code>node push-server.js</code> and a push notification will be sent to the client.</p>"},{"location":"code/typescript/javascript/pwa/pwa-overview/#caching","title":"Caching","text":"<p>Don't use localStorage because it's a synchronous store.</p> <p>pouchdb (based off of CouchDB) will allow you to sync data with the cloud and in turn other devices.</p> <p>localForage is good at handling data too large for the SW and works well with older devices.</p> <p>Cache API is the newest from HTML5. Will work only with browsers supporting SWs.</p>"},{"location":"code/typescript/javascript/pwa/pwa-overview/#chachestorage","title":"ChacheStorage","text":"<p>Interface for Cache APi.</p> <pre><code>if (window.caches) {\n  caches.open('test');\n}\n</code></pre> <p>The above creates a cache.</p> <p>Print out the cache keys with <code>caches.keys();</code>.</p> <p>Check for existing keys with <code>caches.has('test');</code>.</p> <p>Delete cache with <code>caches.delete('test1');</code> or <code>caches.deleteAll([ ... ]);</code>.</p> <p>These all return promises, so print out the results by appending <code>.then(console.log)</code>;</p> <p>You can add stuff to the cache with the following:</p> <pre><code>if (window.caches) {\n  caches.open('pwa-v1.1').then((cache) =&gt; {\n    cache.addAll([\n      '/index.html',\n      '/style.css',\n      '/main.js'\n    ]);\n\n    // cache.add('/index.html');\n  });\n}\n</code></pre> <p>These resources need to be present because if the promise fails, it fails and there's no recourse. Best to only use this with resources located in the same origin.</p> <p>You can control the responses by directly calling <code>put</code>: </p> <pre><code>if (window.caches) {\n\n  caches.open('pwa-v1.1').then((cache) =&gt; {\n\n    cache.put('input.html', new Response('My own HTML'));\n\n    // cache.match('index.html').then((res) =&gt; {\n    //    res.text().then(console.log);\n    // });\n  });\n}\n</code></pre> <p>When the commented out code above is uncommented, then the output will be \"My own HTML\".</p>"},{"location":"code/typescript/javascript/pwa/pwa-overview/#caching-in-the-storage-worker","title":"Caching in the Storage Worker","text":"<p>sw.js</p> <pre><code>const pwaCache = 'pwa-cache-2';\n\nself.addEventListener('install', (e) =&gt; {\n\n  let cacheReady = caches.open(pwaCache).then((cache) =&gt; {\n\n    console.log('New cache ready.');\n    return cache.addAll([\n      '/',\n      'style.css',\n      'thumb.png',\n      'main.js'\n    ]);\n  });\n\n  e.waitUntil(cacheReady);\n});\n\n\nself.addEventListener('activate', (e) =&gt; {\n\n  let cacheCleaned = caches.keys().then((keys) =&gt; {\n\n    keys.forEach((key) =&gt; {\n      if( key !== pwaCache ) return caches.delete(key);\n    });\n  });\n\n  e.waitUntil(cacheCleaned);\n\n});\n\n\nself.addEventListener('fetch', (e) =&gt; {\n\n  // Skip for remote fetch\n  if ( !e.request.url.match(location.origin) )  return;\n\n  // Serve local fetch from cache\n  let newRes = caches.open(pwaCache).then((cache) =&gt; {\n    return cache.match(e.request).then((res) =&gt; {\n\n      // Check request was found in cache\n      if (res) {\n        console.log(`Serving ${res.url} from cache.`);\n        return res;\n      }\n\n      // Fetch on behalf of client and cache\n      return fetch(e.request).then((fetchRes) =&gt; {\n\n        cache.put(e.request, fetchRes.clone());\n        return fetchRes;\n      });\n    });\n  });\n\n  e.respondWith(newRes);\n\n});\n</code></pre>"},{"location":"code/typescript/javascript/pwa/pwa-overview/#caching-strategies","title":"Caching Strategies","text":"<p>It's a good idea to have a network fallback if resources get deleted from the cache from the user or the OS</p> <pre><code>// 1. Cache only. Static assets - App Shell\n e.respondWith(caches.match(e.request));\n\n// 2. Cache with Network Fallback\n e.respondWith(\n  caches.match(e.request).then( (res) =&gt; {\n    if(res) return res;\n\n    // Fallback\n    return fetch(e.request).then( (newRes) =&gt; {\n      // Cache fetched response\n      caches.open(pwaCache).then( cache =&gt; cache.put(e.request, newRes) );\n      return newRes.clone();\n    })\n  })\n);\n\n// 3. Network with cache fallback\ne.respondWith(\n  fetch(e.request).then( (res) =&gt; {\n\n    // Cache latest version\n    caches.open(pwaCache).then( cache =&gt; cache.put(e.request, res) );\n    return res.clone();\n\n  // Fallback to cache\n  }).catch( err =&gt; caches.match(e.request) )\n);\n\n// 4. Cache with Network Update\ne.respondWith(\n  caches.open(pwaCache).then( (cache) =&gt; {\n\n    // Return from cache\n    return cache.match(e.request).then( (res) =&gt; {\n\n      // Update\n      let updatedRes = fetch(e.request).then( (newRes) =&gt; {\n        // Cache new response\n        cache.put(e.request, newRes.clone());\n        return newRes;\n      });\n\n      return res || updatedRes;\n    })\n  })\n);\n\n// 5. Cache &amp; Network Race with offline content\nlet firstResponse = new Promise((resolve, reject) =&gt; {\n\n  // Track rejections\n  let firstRejectionReceived = false;\n  let rejectOnce = () =&gt; {\n    if (firstRejectionReceived) {\n\n      if (e.request.url.match('thumb.png')) {\n        resolve(caches.match('/placeholder.png'));\n      } else {\n        reject('No response received.')\n      }\n    } else {\n      firstRejectionReceived = true;\n    }\n  };\n\n  // Try Network\n  fetch(e.request).then( (res) =&gt; {\n    // Check res ok\n    res.ok ? resolve(res) : rejectOnce();\n  }).catch(rejectOnce);\n\n  // Try Cache\n  caches.match(e.request).then( (res) =&gt; {\n    // Check cache found\n    res ? resolve(res) : rejectOnce();\n  }).catch(rejectOnce);\n\n});\ne.respondWith(firstResponse);\n</code></pre>"},{"location":"code/typescript/javascript/pwa/pwa-overview/#native-app-features","title":"Native App Features","text":"<p>This part is pretty simple. It just needs a <code>manifest.json</code> with contents like below:</p> <pre><code>{\n  \"name\": \"Progressive Web App\",\n  \"short_name\": \"PWA\",\n  \"start_url\": \"/index.html\",\n  \"display\": \"standalone\",\n  \"background_color\": \"#2D91F8\",\n  \"description\": \"Progressive Web Apps - The Complete Guide\",\n  \"icons\": [\n    {\n      \"src\": \"/icons/icon-72.png\",\n      \"sizes\": \"72x72\",\n      \"type\": \"image/png\"\n    },\n    {\n       \"src\": \"/icons/icon-96.png\",\n       \"sizes\": \"96x96\",\n       \"type\": \"image/png\"\n     },\n     {\n       \"src\": \"/icons/icon-128.png\",\n       \"sizes\": \"128x128\",\n       \"type\": \"image/png\"\n     },\n     {\n       \"src\": \"/icons/icon-144.png\",\n       \"sizes\": \"144x144\",\n       \"type\": \"image/png\"\n     },\n     {\n       \"src\": \"/icons/icon-152.png\",\n       \"sizes\": \"152x152\",\n       \"type\": \"image/png\"\n     },\n     {\n       \"src\": \"/icons/icon-192.png\",\n       \"sizes\": \"192x192\",\n       \"type\": \"image/png\"\n     }\n  ]\n}\n</code></pre> <p>You can generate a manifest at https://app-manifest.firebaseapp.com/.</p>"},{"location":"conferences/","title":"Conference Notes","text":"<p>Notes and materials from conferences attended over the years</p>"},{"location":"conferences/2019/angular-kc-2019/graphql-2019-11-12/","title":"GraphQL in Angular &amp; Upgrading AngularJS to Angular","text":""},{"location":"conferences/2019/angular-kc-2019/graphql-2019-11-12/#graphql","title":"GraphQL","text":"<p>A well defined contract that defines shape of data on the client side for percision GET requests</p> <p>Apollo Library Suite: for handling Graph QL in Angular</p> <ul> <li>really good for schematics</li> <li>utilizes observables for reactivity</li> </ul> <p>Allows for mutations (PUT/POST)</p> <ul> <li>Provides and optimistic response</li> </ul> <p>GraphQL \"Playground\": like an interactive Swagger</p> <p>See Nx and Monorepos (from Nrwall)</p>"},{"location":"conferences/2019/angular-kc-2019/graphql-2019-11-12/#upgrading-angularjs-to-angular","title":"Upgrading AngularJS to Angular","text":"<p>If v1.5.3 you can use ngUpgrade to make frankenstein app that runs bothand allows you to upgrade slowly in pieces</p> <p>3 options for upgrade: Dont, rewrite, or franken-app</p>"},{"location":"conferences/2019/angular-kc-2019/lightning-talks-2019-12-12/","title":"Lightning Talks and Holiday Meetup","text":""},{"location":"conferences/2019/angular-kc-2019/lightning-talks-2019-12-12/#rustkc-group","title":"RustKC group","text":"<p>rust-lang.org \"The Book\" Axiom Library</p> <p>Cargo similar to npm for Rust</p>"},{"location":"conferences/2019/angular-kc-2019/lightning-talks-2019-12-12/#quantum-computing","title":"Quantum Computing","text":"<p>ibm.com for sandbox learning</p> <p>Quantumcountry/qcvc</p> <p>Infinite number of states vs binary. Good for modeling really complex interactions like medications and theory.</p>"},{"location":"conferences/2019/angular-kc-2019/lightning-talks-2019-12-12/#ui-for-devs","title":"UI for Devs","text":"<ul> <li>Complementary fonts along w/colors</li> <li>The Non-Designers Design Book by Robin Williams</li> <li>Design Academy.io Lara Elizabeth</li> <li>Dribble for design ideas</li> <li>Paletton</li> </ul>"},{"location":"conferences/2019/angular-kc-2019/lightning-talks-2019-12-12/#do-a-project-in-graphql","title":"Do a project in GraphQL","text":"<ul> <li>Single source of truth for expected data</li> <li>Can version w/deprecation to field -&gt; addds warning for devs to give them a chance to adjust</li> <li>++ perforamnce imporovements -&gt; No more over/under fetching</li> <li>Error handling sucks, returns 200 even w/error. Appollo helps parse errors</li> <li>File uploading sucks too. Need Appollo upload server or equivilant</li> </ul>"},{"location":"conferences/2019/angular-kc-2019/lightning-talks-2019-12-12/#vscode-docker-extension-devin-kelly-collins","title":"VsCode Docker Extension - Devin Kelly-Collins","text":"<ul> <li>Opens vscode instance inside dfocker container</li> <li>project must have .devcontainer.json</li> <li>dockercompose.com</li> <li>can instanstantly connect to DB</li> </ul>"},{"location":"conferences/2019/angular-kc-2019/lightning-talks-2019-12-12/#tailwind-css-alicia-duncan","title":"Tailwind CSS - Alicia Duncan","text":"<ul> <li>Use a custom webpack biulder (angular.json)</li> <li>like bootstrap (?)</li> </ul>"},{"location":"conferences/2019/angular-kc-2019/ngrx-animations-2019-10-9/","title":"Ngrx Store &amp; Angular Animations","text":"<p>Adaptive (sponsor) and Artisan (talk hosts) are both hiring</p> <p>@angularkc is on slack, GitHub, and twitter</p> <p>November 12th presentation on Apollo and GraphQL</p>"},{"location":"conferences/2019/angular-kc-2019/ngrx-animations-2019-10-9/#ngrx-store","title":"Ngrx Store","text":"<p>Redux truths</p> <ul> <li>1 source of truth</li> <li>state is read-only - updated via dispatching actions</li> <li>state is updated via pure functions called reducers - initial state and an action and returns new state.</li> </ul> <p>Look into Redux Chrome extension for debugging NgRx </p> <p>Make sure store is set up in unit tests or error messages will make no sense Look at code to see how she handled subscriptions/subjects in product component</p> <p>backend services need effects that kick off a dispatch/action from within effect so that reducer is able to update state</p> <ul> <li>Look up NgRx docs for more explanation and diagrams</li> </ul> <p>\"If you're not sure if you need Redux, then you don't need Redux\" - Guy who came up with Redux</p> <p>createSelector are memoized - they won't fire again unless state changes. More performant</p>"},{"location":"conferences/2019/angular-kc-2019/ngrx-animations-2019-10-9/#angular-animations","title":"Angular animations","text":"<ul> <li>See https://github.com/alisaduncan/angular-animations for slides</li> <li>Look into RouterOutlet class used programmatically (outside of just <code>&lt;router-outlet&gt;</code> tags</li> </ul>"},{"location":"conferences/2019/angular-kc-2019/ngrx-animations-2019-10-9/#links","title":"Links","text":"<p>https://github.com/johnpapa/angular-ngrx-data</p> <p>https://github.com/jeansandatee/StoreSample</p> <p>https://angular.io/api/router/RouterOutlet#description</p> <p>https://github.com/alisaduncan/angular-animations-code/blob/master/package.json</p> <p>https://github.com/alisaduncan/angular-animations</p> <p>https://www.meetup.com/angularkc/events/265123424/</p>"},{"location":"conferences/2019/kcdc-2019/electronics-for-programmers/","title":"Electronics for Programmers","text":""},{"location":"conferences/2019/kcdc-2019/electronics-for-programmers/#resources-to-check-out","title":"Resources to check out","text":"<ul> <li>https://github.com/pulcher - has slides there </li> <li>Also has a YT channel where he gives tutorials</li> <li>Getting started in Electronics by Forrest M Mims III</li> </ul>"},{"location":"conferences/2019/kcdc-2019/electronics-for-programmers/#basic-tools","title":"Basic Tools","text":"<ul> <li>Digital multimeter - DMM</li> <li>Power source</li> <li>Bread board (positive/negative charges on slides)</li> <li>Wire and things to cut wire</li> <li>Eagle (software)</li> <li>Fritzing software</li> </ul> <p>SparkFun and Arduino to get started (DO IT!!)</p>"},{"location":"conferences/2019/kcdc-2019/electronics-for-programmers/#resistor","title":"Resistor","text":"<p>Slows down current that passes through it</p> <ul> <li>Bands indicate resistance</li> </ul>"},{"location":"conferences/2019/kcdc-2019/electronics-for-programmers/#copneitors-sp","title":"Copneitors (sp?)","text":"<ul> <li>LIke tiny batteries</li> <li>Measured in Farads</li> <li>Rated by Voltage</li> </ul> <p>Look into Series vs Parallel circuitry</p>"},{"location":"conferences/2019/kcdc-2019/electronics-for-programmers/#diodes-one-way-valve","title":"Diodes: One way valve","text":"<p>Rated on forward and reverse currents</p> <p>Voltage Triangle</p> <pre><code>/ \\\n</code></pre> <p>/ V \\   /     \\  / I   R \\</p> <p>Power Triangle</p> <pre><code>/ \\\n</code></pre> <p>/ P \\   /     \\  / I   V \\</p> <p>V = Voltage I = Current R = Resistance P = Watts</p>"},{"location":"conferences/2019/kcdc-2019/encryption/","title":"Not Screwing Up Encryption","text":"<p>http://github.com/sstur for slides</p> <p>Encryption is a subset of cryptography</p>"},{"location":"conferences/2019/kcdc-2019/encryption/#look-into","title":"Look Into","text":"<ul> <li>hashing for pwd but also service interaction for SBS</li> <li>Hashing vs Encryption</li> <li>Async vs Syncronous Encryption</li> <li>DKIM for email verification of sender identity</li> <li>SSL cert 2048 and RSA key</li> </ul> <p>Always use strong salt for hashing pwd</p>"},{"location":"conferences/2019/kcdc-2019/js-dark-arts/","title":"Defense Against the JavaScript dark arts","text":"<p>http://bit.ly/js-dark-arts - slides</p>"},{"location":"conferences/2019/kcdc-2019/js-dark-arts/#equality","title":"Equality","text":"<ul> <li>Double equals checks for loose equality (falsy == falsy, etc)</li> <li>Uses type coersion ('5' == 5 //true)</li> <li>Triple equals for deep equality</li> </ul>"},{"location":"conferences/2019/kcdc-2019/js-dark-arts/#passing","title":"Passing","text":"<ul> <li>Pass by value on primitives</li> <li>Pass by reference on objects (kind of)</li> </ul>"},{"location":"conferences/2019/kcdc-2019/js-dark-arts/#shallow-copy-vs-deep-copy-in-js","title":"Shallow copy vs deep copy in JS","text":"<p>Lodash is commonly used for deep copies (nested objects)</p> <p>Poor man's deep copy <code>JSON.parse(JSON.stringify(obj))</code></p>"},{"location":"conferences/2019/kcdc-2019/js-dark-arts/#default-params","title":"Default params","text":"<p>ES5 use || in assignment. Es6 can do in fn definition <code>const func = (x = 1) =&gt; { ... }</code> Can also default value of fn instead of primitive or object <code>const func = (x = otherFunc()) =&gt; { ... }</code></p>"},{"location":"conferences/2019/kcdc-2019/js-dark-arts/#asyncronous-js","title":"Asyncronous JS","text":"<ul> <li>JS is single threaded</li> </ul>"},{"location":"conferences/2019/kcdc-2019/js-dark-arts/#other-good-resources-to-look-into","title":"Other good resources to look into","text":"<ul> <li>\"JavaScript 30\" by Wes Bos (challenges in vanilla js)</li> <li>Youtube: \"What the heck is the event loop anyway?\"</li> </ul>"},{"location":"conferences/2019/kcdc-2019/js-dark-arts/#promises","title":"Promises","text":"<ul> <li>Use axios library for http requests - better than callbacks becausecan chain promises</li> </ul>"},{"location":"conferences/2019/kcdc-2019/js-dark-arts/#formatting-and-linting","title":"Formatting and Linting","text":"<ul> <li>Can prevent push to repo if set up</li> </ul> <p>linting setup: <code>install eslint</code> <code>run -eslint --init</code> -&gt; preconfiged w/big 3 frameworks</p>"},{"location":"conferences/2019/kcdc-2019/microservices/","title":"Microservices - The Easy Way Is the Wrong Way","text":"<p>Definition: Mark Feller Quote</p> <ul> <li>Should always be treated as independent application</li> <li>Communicates over network</li> <li>Single Responsibility -&gt; only one reason to change</li> <li>Owned by one team</li> <li>Individually deployable</li> </ul> <p>Monoliths are not inherently bad: only when you can't isolate services.</p> <p>Microservices ventures into Distributed Computing (this has been around for 50+ years - think SmallTalk)</p>"},{"location":"conferences/2019/kcdc-2019/microservices/#why-use-microservices","title":"Why Use Microservices?","text":"<p>Deliver business functionality:</p> <ul> <li>Quicker</li> <li>Safer</li> <li>More Stable</li> <li>Team Autonomy</li> <li>Best language for the job</li> <li>Focused development</li> <li>Agile works well</li> <li>Service Autonomy</li> <li>Single Responsiblity</li> <li>Deployable with other services</li> <li>Scalability</li> <li>Scale independently of others</li> <li>Choice of servers</li> <li>Fault Isolation</li> <li>Your problems arn't my problems \"Stay in your lane bro\"</li> </ul>"},{"location":"conferences/2019/kcdc-2019/microservices/#why-not-to-use-microservices","title":"Why NOT To Use Microservices?","text":"<ul> <li>Monolith is too hard</li> <li>Too much coupling - need refactoring first</li> <li>Team unwilling or not ready</li> <li>No support structure</li> <li>High cost of time and money</li> </ul> <p>Network layer 4 and 7 -&gt; OSI Model</p> <ul> <li>1 is the hardware</li> <li>4 is the IP calls access control/firewall</li> <li>7 is the http call</li> </ul> <p>Beware of shared libraries!</p> <p>Encrypt all service communication even if it's behind a firewall</p> <p>Use single repo for each microservice</p>"},{"location":"conferences/2019/kcdc-2019/microservices/#decentralize-data","title":"Decentralize Data","text":"<ul> <li>Single relational DB is a coupling point</li> </ul> <p>How to deal with it?</p> <ul> <li>Duplicate parts that need to be shared</li> </ul>"},{"location":"conferences/2019/kcdc-2019/microservices/#decomposing-monolith-to-microservice","title":"Decomposing Monolith to Microservice","text":"<ul> <li>Monolith needs to have clear boundaries, DI, and actually needs refactoring before split up</li> <li>Start small</li> </ul>"},{"location":"conferences/2019/kcdc-2019/microservices/#good-resources-to-consider","title":"Good Resources To Consider","text":"<p>Eric Evans - DDD (Domain Driven Design)</p> <ul> <li>Start in the middle</li> </ul> <p>Vahn Vernon - Implementing Domain Driven Design</p> <p>Chris Richardson - Microservices Patterns</p>"},{"location":"conferences/2019/kcdc-2019/microservices/#identify","title":"Identify","text":"<ul> <li>Figure out Bounded Contexts (dictated by business needs)</li> <li>Identify possiblej services. Bounded Contexts must not span multiple microservices</li> </ul> <p>Web and API should be loosely coupled - Web would be \"wiki\" while API not backend mirror of that necessarily. Could do menu, content, login microservices on the backend. Think smaller for APIs</p>"},{"location":"conferences/2019/kcdc-2019/pwa/","title":"Progressive Web Apps with Angular","text":"<p>Not a standard or web spec Term coined by Google devs</p>"},{"location":"conferences/2019/kcdc-2019/pwa/#what-is-it","title":"What is it?","text":"<ul> <li>Fast</li> <li>Integraged</li> <li>Reliable -&gt; offline or bad connection</li> <li>Engaging -&gt; native feel, push content# Progressive Web Apps with Angular</li> </ul>"},{"location":"conferences/2019/kcdc-2019/pwa/#fast","title":"Fast","text":"<ul> <li>53% of users leave if &gt; 3 seconds load time</li> <li>3G is still prevelant</li> <li>Mobile devices in general have limited CPU and memory</li> </ul>"},{"location":"conferences/2019/kcdc-2019/pwa/#integrated","title":"Integrated","text":"<ul> <li>Add to home screen (if meets standard)</li> <li>Geolocation</li> <li>Camera/mic</li> <li>App manifest</li> </ul>"},{"location":"conferences/2019/kcdc-2019/pwa/#reliable","title":"Reliable","text":"<ul> <li>App shell platform -&gt; cache or install static content</li> <li>Handle offline routing</li> <li>Dynamic caching -&gt; cache api connection</li> </ul>"},{"location":"conferences/2019/kcdc-2019/pwa/#engaging","title":"Engaging","text":"<ul> <li>Web payment API -&gt; Google Pay</li> <li>Push notifications (done by Service Workers)</li> <li>Web Sockets / Live updates</li> </ul> <p>See pwastats.com</p> <p>In general PWAs are way smaller than native apps</p> <p>Lighthouse is a great tool for PWA development. Tests performance from simulated 3G network.</p>"},{"location":"conferences/2019/kcdc-2019/pwa/#service-workers","title":"Service Workers","text":"<ul> <li>Proxy-ish JS file -&gt; sits between application and network. Monitors/intercepts all network traffic for session.</li> <li>Enables add to home screen</li> <li>HTTPS</li> <li>Key feature for PWAs</li> </ul> <p>If working on a PWA, Go to Chrome devtools -&gt; Application -&gt; check the service worker refresh box</p>"},{"location":"conferences/2019/kcdc-2019/pwa/#downsides","title":"Downsides","text":"<ul> <li>Typical caching problems</li> <li>\"Breaking\" the refresh button</li> <li>Kill switch is needed for SWs that refuse to die and reinstall with fresh updates</li> </ul> <p>Service workers dont disrupt browsers (like IE) that can't use them. They just go dormant. </p> <p>Look at Workbox and @angular/service-worker</p>"},{"location":"conferences/2019/kcdc-2019/pwa/#how-to-develop-pwa-in-angular","title":"How to develop PWA in Angular?","text":"<p><code>ng new my-pwa</code> <code>ng add @angular/pwa</code> (?)</p> <p>ngsw-config.json</p> <p>No impact on angular app, the app behaves as usual. SW sits between app and network.</p>"},{"location":"conferences/2019/kcdc-2019/pwa/#performance","title":"Performance","text":"<p>Toughest thing to perfect</p> <ul> <li>ng build --prod</li> <li>lazy routing /code splitting</li> <li>tree shakeable providers -&gt; angular has certain syntax to weed out unused services</li> <li>Service Worker caching</li> <li>Server side rendering with Angular Universal</li> </ul> <p>Update notification from SW instead of refresh button.</p> <p>See https://github.com/coryrylan/ng-pwa-lunch</p>"},{"location":"conferences/2019/kcdc-2019/refactoring-js/","title":"Refactoring JS","text":"<p>Presenter: Joe Morgan</p> <p>Do it little by little, piece by piece</p> <p>2 types of refactoring</p> <ul> <li>Cleanup</li> <li>Abstractions (Avoid Hasty Abstractions coding style)</li> </ul>"},{"location":"conferences/2019/kcdc-2019/refactoring-js/#start-peeling-out-pieces","title":"Start peeling out pieces","text":"<ol> <li>Isolate</li> <li>Purify</li> <li>Combine</li> </ol> <p>Many/robust tests will minimize risk of introducting new bugs</p> <ul> <li>Mostly shallow integration tests</li> <li>isolateed tests can change since code is changing</li> <li>shallow int tests should not change to ensure UX isn't broken</li> </ul> <p>Layer functionality based on display vs data vs utilities</p> <p>See \"Age\" slide for example on how to cvreate objects conditionally</p> <p>Fail Early Fail Often</p> <p>Look at using spread operators in fn arguments for variable number of arguments</p> <ul> <li>See slide <code>limits.every(limit =&gt; {...})</code></li> </ul> <p>Keep things short and flat and isolated</p> <ul> <li>Keep it SOLID w/emphasis on Single Responsibility</li> </ul>"},{"location":"conferences/2019/kcdc-2019/rest-apis-long-haul/","title":"REST APIs For The Long Haul","text":"<p>API design is UX for Devs</p> <p>REST is a set of architectural constraints</p>"},{"location":"conferences/2019/kcdc-2019/rest-apis-long-haul/#six-constraints","title":"Six Constraints","text":"<ul> <li>Client server</li> <li>Cacheable</li> <li>Stateless</li> <li>Layered</li> <li>Code on Demand (optional)</li> <li>Uniform Interface</li> </ul> <p>Avoid maintaining State - big nono</p> <ul> <li>Should always be stateless</li> </ul>"},{"location":"conferences/2019/kcdc-2019/rest-apis-long-haul/#a-few-simple-rules","title":"A Few Simple Rules","text":"<ul> <li>Dont be creative, be predictible</li> <li>Be consistent</li> <li>Get the important stuff right - avoid bikeshedding</li> </ul> <p>The act of wasting time on trivial details while important matters are inadequately attended is sometimes known as bikeshedding</p>"},{"location":"conferences/2019/kcdc-2019/rest-apis-long-haul/#basic-considerations","title":"Basic Considerations","text":"<ul> <li>Security</li> </ul> <p>Twilio (for texts) uses API keys</p> <p>Basic Auth</p> <ul> <li>Hash un/pwd</li> <li>API key -&gt; x-application-key: this is a secrete header</li> </ul>"},{"location":"conferences/2019/kcdc-2019/rest-apis-long-haul/#beware-of-leaky-abstractions","title":"Beware of Leaky Abstractions","text":"<p>Join tables instead of multiple requests for 1 result</p>"},{"location":"conferences/2019/kcdc-2019/rest-apis-long-haul/#general-error-handling-guyidlines","title":"General Error Handling Guyidlines","text":"<ul> <li>Be as consistent as possible throughout API</li> <li>Provide enough info for caller to fix issue</li> <li>log info</li> </ul>"},{"location":"conferences/2019/kcdc-2019/rest-apis-long-haul/#versioning","title":"Versioning","text":"<p>header, url query string, or role (best)</p>"},{"location":"conferences/2019/kcdc-2019/solid/","title":"SOLID \u2013 The five commandments of good software","text":""},{"location":"conferences/2019/kcdc-2019/solid/#s-single-responsibility-principle","title":"S - Single responsibility principle","text":"<p>Everything should do one thing. There can only be one requirement when changed will cause a class to change.</p> <p>Context can change the definition of this - e.g. repositories. Depends specifically on the  project you're working on.</p> <p>Benefits?</p> <ul> <li>Smaller classes which means less room for bugs and speghetti growth</li> </ul>"},{"location":"conferences/2019/kcdc-2019/solid/#o-openclose","title":"O - Open/Close","text":"<p>Once it's done it's done. Only changes to fix bugs. Changing requirements should mean new classes. </p> <p>Meyer vs Polymorphic</p> <ul> <li>Says to inherit existing (was before interfaces)</li> <li>Inheritence is easily brittle</li> <li>Polymorphic tends to lean towards interfaces</li> </ul> <p>Benefits?</p> <ul> <li>By not changing code you're not able to break what exists</li> </ul>"},{"location":"conferences/2019/kcdc-2019/solid/#l-liskovs-substitution-prinicipal","title":"L - Liskovs Substitution Prinicipal","text":"<p>A subclass should behave in such a way that it will not cause problems when used instead of a superclass.</p> <p>Rules</p> <ul> <li>Contravariance (?) of method arguments in sub class</li> <li>Covariance (more precise) of return types in the sub class</li> <li>No new exception types are allowed to be thrown, unless they are sub classes of previously used ones</li> <li>Preconditions cannot be strengthened in a subtype</li> <li>Postconditions cannot be weakened in a subtype</li> <li>The history constraint - if you have a mutable object and you inherit it, you'r enot allowed to make it immutable and visa-versa</li> </ul> <p>Clarification: should square inherit from rectangle or rectangle inherit from square? Neither, they should inherit from super class.</p> <p>What about abstract base classes and interfaces? No proof of implemenetation so it's hard to follow this rule.</p> <p>Benefits?</p> <ul> <li>Exception handling is more straight forward, so less bugs.</li> </ul>"},{"location":"conferences/2019/kcdc-2019/solid/#i-interface-implementation-primcipal","title":"I - Interface Implementation Primcipal","text":"<p>Breaking down interfaces in smallest peices possible to make easier to implement, and offers more control over who sees what.</p>"},{"location":"conferences/2019/kcdc-2019/solid/#d-dependency-inversion-principal","title":"D - Dependency Inversion Principal","text":"<p>By making sure classes dont depend on specific implementaitons, it becomes easy to change things around.</p> <p>Look into Dependency Inversion solutions</p>"},{"location":"conferences/2021/angular-kc-2021/angular-world-tour-2021-07-13-TUES/","title":"Angular World Tour: Meetup AngularKC","text":"<p>Observables do nothing until we subscribe to them.</p> <p>Routing emits params via paramMap Observable FormControl emits value via valueChanges Observable Http client emits the response via an Observable</p> <p>What are observables looking for? next, error, and complete</p> <pre><code>const sub = source$.subscribe({\n  next: apple =&gt; console.log()\n  err: \n})\n</code></pre> <pre><code>// procedural approach to observables\nthis.sub = this.productData.getProducts().subscribe((data) =&gt; {\n  this.products = data;\n});\n\n// declarative approach\nthis.products$ = this.productService.getProducts();\n</code></pre> <p>How to pass data in a declarative pattern?</p>"},{"location":"conferences/2021/angular-kc-2021/angular-world-tour-2021-07-13-TUES/#the-state-of-angular","title":"The State of Angular","text":"<p>In v12 <code>ng build</code> by default does production mode (finally)</p> <p>goo.gle/angular-enableing-best-practices</p> <p>View Engine is deprecated. Everything needs to migrate over to Ivy (this is more relavant for library authors) Look into how to do this - goo.gle/improving-library-distribution</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021-performance-workshop/","title":"Ng Conf 2021: Performance Workshop","text":"<p>UAAO Performance Improvement Understand, Audit, Analyze, and Optimize - Basically the scientific method applied to performance analysis</p> <p>Example app with more resources in the README - https://github.com/jeffbcross/victor-videos</p> <p>Don't test <code>ng serve</code> unless you run it in production mode.</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021-performance-workshop/#understand","title":"Understand","text":""},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021-performance-workshop/#understanding-priorities","title":"Understanding priorities","text":"<p>Users should be able to see video content ASAP, and be able to click on a video</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021-performance-workshop/#key-content","title":"Key Content","text":"<p>This exercise will focus on loading the home page quickly, and getting users to the video they want to see.</p> <p>URLs - home is how most users arrive at the site - detail/id is what users navigate to</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021-performance-workshop/#audit","title":"Audit","text":""},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021-performance-workshop/#audit-configuration","title":"Audit Configuration","text":"<p>All measurements collected via Lighthouse in an incognito window of Chrome v 87.0.4280.141 with Desktop configuration (Category: Performance, Device: Desktop) on Jeff Cross's Mac Pro (2019 Model) at his office (3.3GHz 12-core Intel Xeon W, 768 GB RAM).</p> <p>It's also a good idea to make sure any extensions are not enabled in incognito mode</p> <p>All unnecessary programs and processes were shut down to increase consistency and reliability of measurements.</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021-performance-workshop/#baseline-audit","title":"Baseline Audit","text":"<p>The median of 3 samples is used for each audit</p> Home Page Video Detail Page First Contentful Paint (seconds) Largest Contentful Paint (seconds) Time to Interactive (seconds) Lighthouse Performance Score <p>Other metrics can be captured but generally if the above are improved then many others will as well.</p> <p>You can set \"budgets\" in <code>angular.json</code> to keep bundle sizes in check. Angular CLI will refuse to build if budgets are exceeded.</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021-performance-workshop/#performance-tab-chrome-dev-tools","title":"Performance Tab (Chrome Dev Tools)","text":"<p>Has everything in the context of your app. Has: - Network - Heap - JS Call Tree - Screenshots</p> <p>Shows it all in a unified timeline where you can see things in context to better determine how things are related to each other. (which things happen, in what order, and dependant on what other things)</p> <p>Repeating patterns usually indicate a loop of some kind, such as CD getting called repeatedly or something happening repeatedly with a bunch of elements in it that's running an expensive task.</p> <p>Be weary of calling methods on the async pipe. It creates a lot more processes to use up memory than normal method calls.</p> <p>Pipes are memoized by default which helps with memory consumption.</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021-performance-workshop/#analyze","title":"Analyze","text":"<p>In the Network tab, check if  - requests are sent over http/1.1 or http2 in the \"Protocol\" column. http2 is much better because all requests share a single connection instead of doing back and forth of initiating a new connection for each resource being loaded - Look for compression under the \"Size\" column.</p> <p>Differential loading is for old ES versions/browsers (such as IE) and doesn't necessarily guarentee a performance boost. See about disabling it in Angular apps.</p> <p>You can also emulate throttling in the Network tab (such as disabled cache, Fast 3G, etc)</p> <p>Consider adding the <code>defer</code> attribute to scripts in the index.html to stop scripts from blocking rendering.</p> <p>CTRL + P in Chrome will open a prompt. You can type <code>&gt;coverage</code> and a new tab will open. Click the refresh button in the new tab space and it'll show you where you have dead html/css/js. If it's compressed you can use the npm package <code>source-map-explorer</code> to navigate through compressed files that pop up here.</p> <pre><code>$ source-map-explorer dist/path/to/offending/file.js\n</code></pre> <p>Be weary of stylesheets getting included more than once or styles being included that do not need to be included.</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/","title":"ng-conf 2021","text":""},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#keynote","title":"Keynote","text":"<p>Stop using node-sass. It will be gone in Angular v12. Switch to just npm package sass.</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#why-dont-you-just-add-types-to-forms","title":"Why don't you just add types to forms?","text":"<p>Most of hte pain of the last 5 releases has been TypeScript, not Angular.</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#why-dont-you-just-make-angular-moreless-reactive","title":"Why don't you just make Angular more/less reactive?","text":"<p>Different devs have different needs - Experience level - Comfort zone - Project size</p> <p>They decided to stay in the middle because removing reactivity loses out on benefits but going all the way reactive is not approachable to new/novice devs.</p> <p>You can go all in either on your own or use stuff like NgRx or other tools</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#why-not-get-rid-of-zones-and-modules","title":"Why not get rid of zones and modules","text":"<ul> <li>Would leave all existing code behind</li> <li>Easier debugging</li> <li>Less magic</li> <li>Less complex simple projects (Module)</li> <li>More comples simple projects (Zones</li> <li>More complex components</li> </ul> <p>They want to eventually move to optional zones and modules</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#opinionated-where-it-counts","title":"Opinionated where it counts","text":"<p>Angular team has weak opinions about things that matter less (css/linting/testing frameworks)</p> <p>They're trying to move forward towards more consistency and inclusion of users (encourage RFC's etc)</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#increase-your-productivity-with-component-sandboxes-and-stories","title":"Increase Your Productivity with Component Sandboxes and Stories","text":"<p>Angular Playground allows you to isolate components while testing in real time</p> <p>\"An open source tool for building enterprise angular components, directives, and pipes in isolation\"</p> <p>Storybook is basically a web app that's more immersive and takes it a bit further.</p> <p>Use teh <code>npx sb init</code> command to install Storybook into an existing project</p> <pre><code>npx -p @storybook/cli sb init --type angular\n</code></pre> <p>A Storybook script will be added to package.json</p> <pre><code>npm run storybook\n</code></pre>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#routeception-how-to-use-the-angular-router-in-microfrontends","title":"Routeception: How to use the Angular router in microfrontends","text":"<p>https://github.com/kylecannon/angular-dream-stack</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#federated-angular-why-and-how","title":"Federated Angular - Why and how","text":""},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#microfrontends","title":"Microfrontends","text":""},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#advantages","title":"Advantages","text":"<ul> <li>Helps with lots of business domains where multiple teams are needed to handle everything.</li> <li>Good for huge product/product suite</li> <li>Scaling teams &amp; domains</li> <li>Frees us up to switch out technologies</li> <li>Accommodating for rapidly changing requirements</li> </ul>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#disadvantages","title":"Disadvantages","text":"<ul> <li>Lots of effort around building a \"meta framework\" to handle all the microfrontends</li> <li>Sharing code at runtime and bundle size - they want to load Angular 1 time, not 5, etc</li> <li>Difficult to achieve common look and feel (not super valid of a concern)</li> </ul>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#module-federation","title":"Module Federation","text":"<p>Webpack 5 Module Federation (Angular 12 will natively support Webpack 5)</p> <p>In theory we can load a module from another domain but in practice it doesn't work because of how the current bundling solutions work. Even lazy loaded parts must be known at compile time (think of tree shaking). </p> <p>Shell (Host) -&gt; Microfrontend (Remote)</p> <p>With module federation Angular thinks that these federated modules are lazy loaded modules - it doesn't realize webpack is doing the heavy lifting.</p> <p>This removes the need for a \"meta framework\" to manage microfrontends.</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#using-with-angular","title":"Using with Angular","text":"<ol> <li>ng add @angular-architects/module-federation</li> <li>Adjust generated configuration</li> <li>ng serve</li> </ol>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#using-webassembly-to-build-fast-secure-web-apps","title":"Using Webassembly to build fast &amp; secure web apps","text":"<p>This dude used his 20 minutes for a shameless plug for Stackblitz. TLDW; Q2 will introduce backend environments entirely contained in a browser window. Really cool, but not quite what they were advertising for this talk...</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#reactive-error-handling-in-angular","title":"Reactive error-handling in Angular","text":"<p>What to tell the end user about the errors? - Known (validation) errors: the user has the chance to fix it by re-entering correct data -&gt; Be as specific as possible. Let the user know what and how to correct. - Known errors: the expected data data cannot be loaded/updated -&gt; Explain what was not successful - Known errors: the user wouldn't notice the error -&gt; Poker face (don't show any error message) - Unkown errors: yes, they do exist! -&gt; Fall-back scenario (e.g. redirect to an error page)</p>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#why-reactive","title":"Why reactive?","text":"<ul> <li>In reactive programming, everything can be a stream of data</li> <li>Data is being pushed so that you have to react to it (instead of intentionally pulling it in imperative programming)</li> <li>Errors are undesirable interruption of the data stream - you react to it too!</li> </ul>"},{"location":"conferences/2021/ng-conf-2021/ng-conf-2021/#prefer-template-driven-forms","title":"Prefer template-driven forms","text":"<p>See https://open.spotify.com/episode/0z6vROLFGNv7VGiu9I4Z1c</p> <p>Ward Bell makes a strong case for template-driven forms. He presents it as way simpler and easier to maintain.</p>"},{"location":"conferences/2022/2022-11-03-writing-for-profressionals/","title":"Writing Tips For Professionals","text":""},{"location":"conferences/2022/2022-11-03-writing-for-profressionals/#the-first-sentence-matters-most","title":"The first sentence matters most","text":"<p>We learned the following structure for a report/document:</p> <ol> <li>Intro</li> <li>Guts</li> <li>Conclusion     a. BAM! (the most important message of the document)</li> </ol> <p>People today want it inverted. Try to fit the 5 W's in your first sentence. </p>"},{"location":"conferences/2022/2022-11-03-writing-for-profressionals/#email-subjects","title":"Email subjects","text":"<p>The highest email open/response rates use:</p> <ul> <li>Subject line = 4 words</li> <li>Length = 100 words (or less)</li> <li>Casual greeting = \"Hi Paula\" or \"Hey Paula\"</li> <li>Reading level = 3rd grade</li> </ul>"},{"location":"conferences/2022/2022-11-03-writing-for-profressionals/#email-hacks","title":"Email hacks","text":"<ol> <li>Ask a question in the subject line<ul> <li>This is mostly a problem with spam if you use certain keywords with it (like \"credit card\" or \"invoice\", etc)</li> </ul> </li> <li>Stickk to 4 sentences<ul> <li>For busy executive - just 2 sentences (or 50 words)</li> </ul> </li> <li>Try multiple choice<ul> <li>Or make a recommendation</li> </ul> </li> <li>Push pleasantries to the bottom</li> </ol>"},{"location":"conferences/2022/2022-11-03-writing-for-profressionals/#simplify-your-sentences","title":"Simplify your sentences","text":"<p>Learn to edit yourself. Can it be shorter without losing the content?</p> <p>\"Managers should consistently attempt to limit their discretionary expenditures to no more than 10% over budgeted levels\"</p> <p>vs</p> <p>\"Managers shouldn't go more than 10% over budget\"</p> <p>Shortest, simplest sentence possible: 5-12 words.</p>"},{"location":"conferences/2022/2022-11-03-writing-for-profressionals/#use-lowest-possible-grade-level","title":"Use lowest possible grade level","text":"<ul> <li>Office docs: 10th grade</li> <li>Public: 6th grade</li> <li>Email: 3rd grade</li> </ul>"},{"location":"conferences/2022/2022-11-03-writing-for-profressionals/#use-active-voice","title":"Use active voice","text":"<p>\"The new invoicing policy was approved by the client\"</p> <p>vs</p> <p>\"The client approved the new invoicing policy\"</p> <p>Get rid of passive verbs (words ending with \"-ing\" or \"-ed\") Put the subject at the start.</p> <p>This explains it a bit more -&gt; https://www.youtube.com/watch?v=AKHgGnxbj4o</p>"},{"location":"conferences/2022/2022-11-03-writing-for-profressionals/#get-rid-of-word-echo","title":"Get rid of \"word echo\"","text":"<p>\"You will love our customer support specialists. Our customer support specialists offer complete software troubleshooting by phone. These customer support specialists are available 24/7.\"</p> <p>vs</p> <p>\"You'll love our customer support specialists who offer comprehensive help over the phone 24/7.\"</p>"},{"location":"conferences/2022/2022-11-03-writing-for-profressionals/#write-faster","title":"Write faster","text":"<ol> <li>Plan (how long it'll be, what'll be in the doc, resources, etc)</li> <li>Research</li> <li>Prioritize (most important at the top, least at the bottom)</li> <li>Write</li> <li>Format</li> <li>Edit</li> <li>Proof</li> </ol> <p>This will be much faster than winging it and then revising it a lot (which I do).</p> <p>5 sentences to start a report</p>"},{"location":"conferences/2022/2022-11-03-writing-for-profressionals/#editing","title":"Editing","text":"<p>Good writing principles:</p> <ul> <li>Shorter + simpler = better</li> <li>Casual narrative voice</li> <li>Get rid of \"academic speak\"</li> <li>Lower the reading level</li> <li>Active voice</li> <li>Bullets</li> </ul>"},{"location":"conferences/2022/2022-11-03-writing-for-profressionals/#proof","title":"Proof","text":"<ul> <li>Punctuation</li> <li>Spelling</li> <li>Grammar</li> <li>Capitalization</li> <li>Sentence structure</li> <li>Flow</li> <li>Active</li> <li>Word echo</li> <li>Tense</li> <li>Style</li> <li>Header Consistency</li> <li>Names</li> <li>Titles</li> <li>Address</li> <li>Acronyms</li> </ul>"},{"location":"conferences/2022/2022-11-03-writing-for-profressionals/#see-the-pomodoro-technique","title":"See the Pomodoro technique","text":"<ol> <li>Plan your tasks</li> <li>Do 1 pomodoro (25 minutes than 5 minute break)</li> <li>Repeat x4 pomodoros</li> </ol>"},{"location":"conferences/2023/kcdc-2023/workshops/java-idioms/java-idioms/","title":"KCDC 2023 - Java Idioms","text":"<p>Notes that are not included in slides or example code </p> <p>Speaker: Jeanne Boyarsky  </p> <p>https://github.com/boyarsky/2023-kcdc-java-idioms https://speakerdeck.com/boyarsky/java-idioms-for-becoming-a-more-powerful-developer?slide=61 </p> <p>Try to use methods that are english-y (<code>.equals()</code> vs <code>==</code>, <code>== 0</code> vs <code>.isEmpty()</code> etc)  </p> <ul> <li>If you see yourself using a for loop over an integer, there is likely an \"english-y\" solution that's built into the language.  </li> </ul> <p>Use <code>strip()</code> instead of <code>trim()</code>. It covers more languages than just English.  </p> <p>Idioms can change over time as improved solutions are introduced to the language.  </p> <p>Records came about in 16 for read only pojos</p>"},{"location":"conferences/2023/kcdc-2023/workshops/java-idioms/2023-kcdc-java-idioms/","title":"Index","text":"<p>Lecture and labs for 2023 KCDC half day training</p> <p>(adapted/updated from https://github.com/boyarsky/2020-devnexus-java-idioms)</p>"},{"location":"conferences/2023/kcdc-2023/workshops/java-idioms/2023-kcdc-java-idioms/#general","title":"General","text":"<p>Open the lab/src/main/java file for the current lab and implement the methods so the tests pass using the contraints in the comments/instructions. Do not delete the \"start\" and \"end\" comments as they are used by the tests.</p> <p>Run the equivalent lab/src/test/java file to see if you got it right.</p> <p>JUnit 5.9.3 is provided in the lib folder in case your IDE doesn't work with the default version of JUnit you have installed. Add all jars in lib to your project's classpath to use. </p>"},{"location":"conferences/2023/kcdc-2023/workshops/java-idioms/2023-kcdc-java-idioms/#open-in-intellij","title":"Open in IntelliJ","text":"<ol> <li>File &gt; Open</li> <li>Choose the folder you cloned (or downloaded as a zip)</li> <li>Click the option to trust the project</li> <li>Choose the latest SDK/version of Java available in your version of IntelliJ</li> </ol>"},{"location":"conferences/2023/kcdc-2023/workshops/java-idioms/2023-kcdc-java-idioms/#open-in-eclipse","title":"Open in Eclipse","text":"<ol> <li>File &gt; Import</li> <li>Existing Projects into Workspace</li> <li>In the project properties, set Java to the latest version</li> </ol>"},{"location":"conferences/2023/kcdc-2023/workshops/java-idioms/2023-kcdc-java-idioms/#open-in-vs-code","title":"Open in VS Code","text":"<ol> <li>File &gt; Open folder</li> <li>Trust files</li> </ol> <p>Note:  Running the tests using test explorer (open a test and click the test tube) works</p>"},{"location":"conferences/2023/kcdc-2023/workshops/spring/spring-workshop/","title":"KCDC 2023 - Spring Workshop","text":"<p>Notes in addition to those provided by the speakers </p> <p>Speakers: Dan Vega and DeShaun Carter  </p> <p>https://github.com/KCDC-2023-Spring-Workshop </p> <p>Office hours - talk to Spring devs about issues Dan has blog and YouTube channel</p> <p>Spring has JPA and JDBC  </p> <p>Spring Shell can be used to write commandline programs that compile down to native instead of needing to run in a JVM  </p> <p>\"Make jar not war\" -&gt; jar files ship with server bundled with it while war files ship TO server  </p> <p>Look into spring.io and spring.academy </p> <p>Command to start up Spring app in terminal: <code>./mvnw spring-boot:run</code> </p> <p>JDK comes with <code>jps</code> command that shows process PIDs for easy killing.  </p> <p><code>SERVER_PORT=8081 ./mvnw spring-boot:run</code> to start it up on a different port.  </p> <p>Once deployed you can hit <code>http://localhost:8080/actuator/health</code> and it'll show JSON output.  </p> <p>Don't add <code>@Authowired</code> to dependency declaration. Do it on the constructor instead.  </p> <p>You don't have to use the @Autowired annotation - it's happening under the hood.  </p> <p>Jackson in the Java world is a way to serialize/deserialize JSON to Java.  </p>"},{"location":"conferences/2023/kcdc-2023/workshops/spring/spring-workshop/#bootstrapping-data","title":"Bootstrapping data","text":"<p>To bootstrap the app with some data you can use the CommandLineRunner in the main application class.</p> <pre><code>@Bean\nCommandLineRunner commandLineRunner() {\n    return args -&gt; System.out.println(\"hello from the command line runner!')\n}\n</code></pre> <p> Look into ListCrudRepository vs CrusRepository</p> <p>There's no need to add <code>@Repository</code> to our interfaces which extend a repository class. It does nothing.  </p> <p>Spring has an image from <code>./mvnw spring-boot:build-image</code> -&gt; don't use custom Dockerfile </p> <p><code>./mvnw -Pnative spring-boot:build-image</code> to make a native executable with Spring Shell  </p> <p>arm64 is much cheaper than x86. Java apps will work on either.  </p> <p> Look into arm64 deployments</p> <p>Cloudflare tunnel to access home computer without exposing ports</p>"},{"location":"conferences/2023/kcdc-2023/workshops/spring/runnerz-main/","title":"Runnerz","text":"<p>The application you will build during this workshop revolves around the domain of tracking physical fitness. In the first iteration of it we will use it track indoor and outdoor runs.</p>"},{"location":"conferences/2023/kcdc-2023/workshops/spring/runnerz-main/HELP/","title":"Getting Started","text":""},{"location":"conferences/2023/kcdc-2023/workshops/spring/runnerz-main/HELP/#reference-documentation","title":"Reference Documentation","text":"<p>For further reference, please consider the following sections:</p> <ul> <li>Official Apache Maven documentation</li> <li>Spring Boot Maven Plugin Reference Guide</li> <li>Create an OCI image</li> <li>Spring Web</li> </ul>"},{"location":"conferences/2023/kcdc-2023/workshops/spring/runnerz-main/HELP/#guides","title":"Guides","text":"<p>The following guides illustrate how to use some features concretely:</p> <ul> <li>Building a RESTful Web Service</li> <li>Serving Web Content with Spring MVC</li> <li>Building REST services with Spring</li> </ul>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/","title":"Angular For Architects Workshop","text":""},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#types-interfaces-classes-and-enums","title":"Types, Interfaces, Classes, and Enums","text":"<ul> <li> <p>https://quicktype.io/typescript paste in json to generate interface</p> </li> <li> <p>using <code>const enum</code> translates each value to numerical value</p> </li> </ul> <p> Look into utility types ReadOnly and Partial.</p>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#using-types-vs-interfaces","title":"Using Types vs Interfaces","text":"<p>interface definitions can be used to represent the shape of an object-like data structure</p> <pre><code>interface Account {\n    accountNumber: number;\n    balance: number;\n}\n</code></pre> <p>type alias declarations can be used for primitive types and object-like data structures</p> <pre><code>type account = string | { accountNumber: string, balance: number };\n</code></pre>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#additional-uses-for-type","title":"Additional Uses for type","text":"<p>A Mapped Type</p> <p>Computed properties can be used to create a mapped type</p> <pre><code>type Keys = \"firstname\" | \"lastname\"\n\ntype PersonType = {\n  [key in Keys]: string\n}\n\nconst person: PersonType = {\n  firstname: \"John\",\n  lastname: \"Doe\"\n}\n</code></pre> <p>Types are good for union types e.g. <code>type Pet = 'dog' | 'cat' | 'fish'</code>.</p> <p>You could use a union in a type instead of an optional property in an interface.</p>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#benefits-of-interfacestypes-vs-classes","title":"Benefits of interfaces/types vs classes","text":"<ul> <li>Can describe the \"shape\" of data for properties, parameters, custom types, and more</li> <li>Can help drive consistency</li> <li>Types can define primitives AND object-like data structures</li> <li>Neither create instances</li> <li>No impact to bundle size</li> </ul>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#component-view-models","title":"Component View Models","text":"<p>2-way data binding can make model/view too tightly coupled - especially in more complex applications. One way around this is cloning the model. Creates a ViewModel to bind to instead of the actual data model. </p> <p>Spread operator only does shallow clones. You can use Lodash or Clone-Deep npm package for deep clone. !! It will \"copy\" all levels but still points to original object's nested properties. This means any changes made to the nested property will update the ORIGINAL object as well as the copy.  Look into using \"copy-deep\" package for deep clone - or lodash if app already has it installed (but only import \"cloneDeep()\" function.</p> <p>ViewModel class (rather than a full clone) is good for parsing out pieces of the api response data or adding properties to it for what's needed in the view.  </p> <ul> <li>Assuming you have a model for the full response dataset, you can use Partial in your ViewModel class.   <p>ViewModel acts as a scratchpad until the data is in a state ready for sending up to the api. The model is the single source of truth. Don't change it until it's ready to persist.</p>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#viewmodel-services","title":"ViewModel Services","text":"<p>Good for when your component represent several connected models</p> <ul> <li>Keep all the logic in it for a single source of truth</li> <li>Hides assembly of complex ViewModels</li> <li>Shares ViewModels with collaborating components</li> <li>Easier to test than a component</li> <li>Lets the Presenters focus on presenting</li> <li>Keeps the Views and ViewModels clean</li> </ul>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#httpclient-and-rxjs-operators","title":"HttpClient and RxJS operators","text":"<p>When possible, make the response data how it needs to be on the backend and avoid using RxJS  </p> <p>For making http calls that rely on the response from other http calls - switchMap and mergeMap are commonly used for this</p>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#understanding-switchmap","title":"Understanding switchMap","text":"<pre><code> this.searchCharacters$ = this.formGroup.get('characterName').valueChanges.pipe( // &lt;-- outer observable\n    debounceTime(500), // this is necessary in this example because it lets the user type more before an http call is made\n    switchMap(name =&gt; {\n      return this.dataService.getCharacter(name); // &lt;-- inner observable (the data we actually care about)\n    })\n );\n</code></pre> <p>switchMap cancels last response and returns latest.  </p> <p>This is especially useful when you need individual values from an array.  </p> <p>Switch between streams with switchMap - go from outer observable to inner.</p>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#mergemap-concatmap-and-forkjoin","title":"mergeMap, concatMap, and forkJoin","text":"<p>mergeMap doesn't preserve order while concatMap does.  </p> <p>forkJoin no longer uses array syntax - instead uses object syntax -</p> <pre><code>return forkJoin({\n    characters: this.getCharacters(),\n    planets: this.getPlanets(),\n})\n</code></pre> <p>switchMap: Flattens most recent inner observable.  </p> <p>concatMap: Flattens inner observables, order guarantee.  </p> <p>mergeMap: Flattens inner observables, no order guarantee.  </p> <p>forkJoin: Waits for all observables to complete, emits result.  </p> <p> Look up differences between forkJoin and combineLatest</p>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#rxjs-subjects","title":"RxJS Subjects","text":"<p>Subject: Only subscribers receive data  </p> <p>BehaviorSubject: Note how this picks up the last value emitted event though it subscribed after the value was sent out. That's because BehaviorSubject allows an initial value to be sent to an observer as they subscribe. - This is especially useful if timing issues happen in the app if you have a seed value. Otherwise use ReplaySubject in this case.</p> <p>ReplaySubject: Note how this stays in sync with everything above even though it subscribes 10 seconds after the subject. That's because it's replaying everything up to that point from a cache it maintains.  </p> <p>AsyncSubject: This only plays the last item before it completes - nothing before that. It \"completes\" in the data service once the customers array length is greater than 5.</p>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#observable-services","title":"Observable Services","text":"<ul> <li>Send data to observers/subscribers</li> <li>Follows the \"observer pattern\"</li> <li>Provides a simple way to keep multiple observers up-to-date</li> <li>Service can use RxJS Subject objects and observables</li> <li>You may have many customized observable services</li> </ul> <pre><code>@Injectable()\nexport class DataService {\n  private customers: ICustomer[];\n  private customersSubject$ = new \n      BehaviorSubject&lt;ICustomer[]&gt;(this.customers);\n  customersChanged$ = this.customersSubject$.asObservable(); // initial data to send to new observer\n\n  addCustomer() : Observable&lt;ICustomer[]&gt; {\n    ...\n    // Send customers data to any observers\n    this.customersSubject$.next(this.customers); // send data to any observers\n    return of(this.customers); // the caller of the function gets the customers and the subscribers get them too\n  }\n}\n</code></pre> <p>Subsciptions actually do act like an array - <pre><code>subs = new Subscription();\n\nthis.subs.add(...)\n\nngOnDestroy() { this.subs.unsubscribe(); }\n</code></pre></p> <p>The downside to this approach is it's not immediately apparent that the Subject is an array. SubSink has easier to understand syntax.</p>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#preload-strategies","title":"Preload Strategies","text":"<p>Addressing user experience by timing when to load JavaScript bundles</p> <p>When a User Navigates to a Route:</p> <ol> <li>The router makes a network request to download a module</li> <li>The router checks if any modules should be preloaded</li> <li>This causes users to wait</li> </ol> <p>Navigate to app (Initial bundles) -&gt; TIme to Interactive (preload module bundle)</p> <p>If you can anticipate the user flow you can preload module bundles so users expeirence no lag.</p>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#types-of-preload-strategies","title":"Types of preload strategies","text":"<p>None: The default behavior All: Useful, but aggressive and taxing on network Custom: Preload based on your custom logic</p>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#custom-preload-strategies","title":"Custom preload strategies","text":"<ol> <li>Implement the PreloadingStrategy interface</li> <li>Write custom logic describing how it should behave</li> <li>Apply it to the routes</li> </ol> <p> Look into this in more detail in the future</p>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#module-architecture","title":"Module Architecture","text":"<ul> <li>Each feature should have a module and routing module (as appropriate)</li> <li>Allows module to be independently developed and self-contained</li> <li>Supports lazy loading</li> </ul> <p>Shared folder should contain reusable components, pipes, directives  </p> <p>The \"core\" folder should contain singleton services shared throughout app  </p>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#standalone","title":"Standalone","text":"<p>Simplify your applications (while still retaining support for Angular features). Organize components, pipes, directives, services using feature folders.  </p> <pre><code>@Component({\n  standalone: true,\n  selector: 'app-customers',\n  imports: [GridComponent, NgIf, NgFor, FormsModule],\n  templateUrl: './customers.component.html'\n})\nexport class CustomersComponent {\n}\n</code></pre>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#migrating-from-modules-to-standalone","title":"Migrating from Modules to Standalone","text":"<ol> <li>Run <code>ng g @angular/core:standalone</code> and select Convert all components, directives and pipes to standalone</li> <li>Run <code>ng g @angular/core:standalone</code> again and select Remove unnecessary NgModule classes</li> <li>Run <code>ng g @angular/core:standalone</code> a 3rd time and select Bootstrap the project using standalone APIs</li> <li>Run any linting and formatting checks, fix any failures, and commit the result</li> </ol> <p>The philosophy of the Ng team is this is a subjective decision. Do what your team agrees on and stay consistent.</p> <p><code>ng new my-project --standlone</code></p>"},{"location":"conferences/2023/ng-conf-2023/angular-for-architects-workshop/#misc","title":"Misc","text":"<ul> <li>Look into using the CoPilot VSCode plugin to ask questions e.g. // q: what's the difference between mergeMap and concatMap</li> <li>Look into Design Patterns book (Gang of 4 book)</li> </ul>"},{"location":"environments/","title":"Environments Overview","text":"<p>Notes on all things related to where applications may be deployed</p>"},{"location":"environments/LAMP/","title":"LAMP stack","text":"","tags":["WordPress","PHP","MySQL","Apache","LAMP"]},{"location":"environments/LAMP/#lamp-setup","title":"LAMP setup","text":"","tags":["WordPress","PHP","MySQL","Apache","LAMP"]},{"location":"environments/LAMP/#install-apache-and-mysql","title":"Install Apache and MySQL","text":"<pre><code>sudo apt-get install apache2\nsudo apt-get install mysql-server\n</code></pre> <p>If secure db setup, do the following (dont bother with local it's a bit of a PITA -</p> <pre><code>sudo mysql_secure_installation\n</code></pre>","tags":["WordPress","PHP","MySQL","Apache","LAMP"]},{"location":"environments/LAMP/#test-mysql","title":"Test MySQL","text":"<p><code>sudo mysql -u root -p</code></p>","tags":["WordPress","PHP","MySQL","Apache","LAMP"]},{"location":"environments/LAMP/#install-php","title":"Install PHP","text":"<pre><code>sudo apt-get install php libapache2-mod-php\n</code></pre> <p>libapache2-mod-php provides the PHP module for the Apache 2 webserver</p>","tags":["WordPress","PHP","MySQL","Apache","LAMP"]},{"location":"environments/LAMP/#testing-php","title":"Testing PHP","text":"<p>Create <code>/var/www/html/phpinfo.php</code> with the contents</p> <pre><code>&lt;?php\n    phpinfo();\n?&gt;\n</code></pre>","tags":["WordPress","PHP","MySQL","Apache","LAMP"]},{"location":"environments/LAMP/#mysqlapache-operations","title":"MySQL/Apache operations","text":"<p><pre><code>sudo /etc/init.d/mysql start\nsudo /etc/init.d/mysql stop\nsudo /etc/init.d/mysql restart\nsudo /etc/init.d/mysql status\n</code></pre> Replace \"mysql\" with whatever service name for the same operations on that service (e.g. apache2, bluetooth, etc)</p>","tags":["WordPress","PHP","MySQL","Apache","LAMP"]},{"location":"environments/LAMP/#running-php-locally","title":"Running PHP locally","text":"<p>Assuming you have php installed, run</p> <pre><code>$ php -S localhost:8000\n</code></pre>","tags":["WordPress","PHP","MySQL","Apache","LAMP"]},{"location":"environments/LAMP/#wordpress","title":"WordPress","text":"","tags":["WordPress","PHP","MySQL","Apache","LAMP"]},{"location":"environments/LAMP/#useful-queries","title":"Useful Queries","text":"<pre><code>--Empty trash\nDELETE p\nFROM wp_posts p\nLEFT OUTER JOIN wp_postmeta pm ON pm.post_id = p.ID\nWHERE post_status = 'trash';\n\n--Update with trash status\nUPDATE wp_posts p \nJOIN wp_postmeta pm ON pm.post_id = p.ID\nSET p.post_status = 'trash'\nWHERE pm.meta_key = '_sku'\nAND pm.meta_value IN ();\n\n-- Verify product is present\nSELECT *\nFROM wp_posts p\nJOIN wp_postmeta pm ON pm.post_id = p.ID\nWHERE pm.meta_key = '_sku'\nAND pm.meta_value = '';\n\n-- Get all product eIDs\nSELECT DISTINCT pm.meta_value\nFROM wp_postmeta pm\nJOIN wp_posts p ON p.ID = pm.post_id\nWHERE pm.meta_key = '_sku'\nAND p.post_status = 'publish';\n\n-- Find dupicates\nSELECT pm.meta_value, p.id, COUNT(*)\nFROM wp_postmeta pm\nJOIN wp_posts p ON p.ID = pm.post_id\nWHERE pm.meta_key = '_sku'\nAND p.post_status = 'publish'\nGROUP BY pm.meta_value\nHAVING COUNT(*) &gt; 1;\n</code></pre>","tags":["WordPress","PHP","MySQL","Apache","LAMP"]},{"location":"environments/cloud/","title":"Cloud Overview","text":"<p>A.K.A. other people's servers</p>"},{"location":"environments/cloud/docker-crash-course/","title":"Docker Crash Course","text":"<p>From Udemy's Docker &amp; Kubernetes: The Practical Guide (2022 Edition)</p> <p>[[TOC]]</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#the-basics","title":"The Basics","text":"<p>Docker is a container technology: A tool for creating and managing containers.</p> <p>Container is a standardized unit of software - a package of code and dependencies to run that codce. Container always yields the exact same application and execution behavior.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#virtual-machines-vs-virtual-operating-systems","title":"Virtual Machines vs Virtual Operating Systems","text":"<p>VMs can do a lot of what containers can do BUT they have way more overhead than containers. Wastes a lot of space on hard drive and tends to be slow.</p> <p>Instead of emulating an entire system, containers have OS built-in emulated container support and a docker engine to spin up containers. Much more light weight (by orders of magnitude). The container may have a small OS layer but it's still way smaller than a VM.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#docker-tools-building-blocks","title":"Docker Tools &amp; Building Blocks","text":"<p>The following are installed when Docker app is installed</p> <ul> <li>Docker Engine</li> <li>Docker Desktop (Daemon and CLI)</li> </ul>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#dockerfile-vs-docker-compose","title":"Dockerfile vs Docker Compose","text":"","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#dockerfile","title":"Dockerfile","text":"<p>A Dockerfile is a simple text file that contains the commands a user could call to assemble an image.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#docker-compose","title":"Docker Compose","text":"<ul> <li>is a tool for defining and running multi-container Docker applications.</li> <li>define the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.</li> <li>get an app running in one command by just running docker-compose up</li> </ul>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#docker-images-containers-the-core-building-blocks","title":"Docker Images &amp; Containers: The Core Building Blocks","text":"<p>Dockerfile in a nodejs app</p> <pre><code>FROM node\n\n# Context is set so all work done in this file will happen here\nWORKDIR /app\n\n# Can also be \"COPY . .\" since the WORKDIR was set\nCOPY . /app\n\nRUN npm install\n\n# This instruction doesn't do anything but it's best practice to add it for readability\n# You need to expose the port via the `docker run` command:\n#   1. `docker build` and point to dir with Dockerfile. Output has image image name that'll be a sha.\n#   2. `docker run -p &lt;local port&gt;:&lt;container port&gt; &lt;image name&gt; e.g. `docker run -p 3000:80 d31c8fdfbcd8`\nEXPOSE 80\n\n#RUN node server.js\nCMD [\"node\", \"server.js\"]\n\n# The difference between RUN and CMD is the latter will only happen once a container is run based on the image.\n# If you don't specify a CMD, the CMD of the base image will be executed. With no base image and no CMD, you'd get an error.\n\n# - List running containers with `docker ps`\n# - Show all docker containers with `docker ps -a`\n# - Stop a running container with `docker stop &lt;container name&gt;`\n</code></pre>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#managing-images-containers","title":"Managing Images &amp; Containers","text":"","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#images","title":"Images","text":"<p>Is the \"blueprints\" of a Docker container. It builds in layers so it may have a layer for OS, dependencies, and commands.</p> <ul> <li>Can be tagged (named) <code>-t, docker tag...</code></li> <li>Can be listed <code>docker images</code></li> <li>Can be analyzed <code>docker image inspect</code></li> <li>Can be removed <code>docker rmi, docker prune</code></li> </ul>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#containers","title":"Containers","text":"<ul> <li>Can be named <code>--name</code></li> <li>Can be configured in detail <code>see --help</code></li> <li>Can be listed <code>docker ps</code></li> <li>Can be removed <code>docker rm</code></li> </ul>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#attached-and-detached-states","title":"Attached and Detached states","text":"<p>The <code>docker run</code> command starts a new container based on the image in the foreground. To start up a stopped container, you simply run <code>docker start &lt;container id or name&gt;</code>. However, the container will be detached in this state and running in the background. You can start a stopped container in attached mode by using the <code>-d</code> flag: <code>docker start -a &lt;container id or name&gt;</code>.</p> <p>You can run a container as detached right away by running <code>docker run -p &lt;local port&gt;:&lt;container port&gt; -d &lt;image name&gt;</code>.</p> <p>You'll miss the logs if you detatch and reattach using the above methods. To reattach AND see the logs you can use <code>docker logs -f &lt;container id or name&gt;</code> to \"follow\" the past and future logs. This essentially is the same as reattaching.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#entering-interactive-mode","title":"Entering interactive mode","text":"<p>Some projects require user interaction. You can start a stopped container in interactive mode by using the <code>-i</code> flag: <code>docker start -i &lt;container id or name&gt;</code></p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#deleting-images-and-containers","title":"Deleting images and containers","text":"<p>We need to stop the container before we can remove it; we can't remove running containers. Run <code>docker ps -a</code> to get all existing container names. Run <code>docker rm &lt;container id or name&gt;</code> with each id/name seperated by a space. Alternatively, you can run <code>docker container prune</code> to get rid of all stopped containers.</p> <p>Listing images is similar to listing containers. You use <code>docker images</code> instead of <code>docker ps</code>. When you delete an image, you run <code>docker rmi &lt;image id&gt;</code> instead of <code>docker rm &lt;container id or name&gt;</code>. It'll output the layers it deleted when you delete an image.</p> <p>You can only remove images that are not being used by a container. That includes containers that are stopped.</p> <p>Remove all non-tagged images not being used with <code>docker image prune</code>. You can remove unused tagged images with <code>docker image prune -a</code>.</p> <p>It's not uncommon to start a container you intend to delete after it stops running. A shortcut for this would be <code>docker run -p 3000:80 -d --rm &lt;image id&gt;</code> which translates to \"docker run on exposed local port 3000 in detached mode and remove container when it's stopped\"</p> <p>Once built upon, images become read-only because it contains layers that the container lays on top of. Many containers can be made from a single image. A container is just one layer of the whole stack.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#copying-to-and-from-a-container","title":"Copying to and from a container","text":"<p><code>docker cp &lt;to&gt; &lt;from&gt;</code> which looks like the following in practice: <code>docker cp my_folder container_name:/destination</code>.</p> <p>Works both ways just like with <code>scp</code> command.</p> <p>This isn't super useful for updating code since the container will need to stop/start again but it could be very useful for retrieving things like log files from a container to inspect locally.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#naming-tagging-containers-and-images","title":"Naming &amp; Tagging Containers and Images","text":"<p>Name a container with the <code>--name</code> flag followed by the name you want and the image id: <code>docker run -p 3000:80 -d --rm --name foobar d31c8fdfbcd8</code>. </p> <p>Images follow the <code>name:tag</code> format. The name defines a group of possible more specialized images whereas the tag defines a specialized image within a group of images.</p> <p>To name and tag an image, run <code>docker build -t &lt;name&gt;:&lt;tag&gt; .</code>.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#sharing-image-containers","title":"Sharing Image &amp; Containers","text":"<p>You dont share containers, you share images. You can do this 1 of 2 ways:</p> <ol> <li>Share the Dockerfile. You'll need any accompanying files and to run <code>docker build .</code>.</li> <li>Share the build image. You can directly run a container based on it. Nothing else needed. You do this via Dockerhub or a private repository.</li> </ol>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#pushing-to-dockerhub","title":"Pushing to Dockerhub","text":"<ol> <li>In Dockerhub click on \"Repositories\"</li> <li>In Dockerhub click \"Create Repository\". Follow prompts.</li> <li>Build image or rename existing one locally: <code>docker tag oldname:tag yourusername/reponame</code>. Adding <code>:tag</code> after <code>yourusername/reponame</code> is optional.</li> <li>Run <code>docker login</code>. Log in</li> <li>Run <code>docker push yourusername/reponame</code>.</li> </ol>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#pulling-from-docker","title":"Pulling from docker","text":"<p>If it's public you can simply run <code>docker pull yourusername/reponame</code>. </p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#managing-data-working-with-volumes","title":"Managing Data &amp; Working with Volumes","text":"","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#types-of-data","title":"Types of data","text":"<p>There are 3 kinds: Application, Temporary, and Permanent</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#application","title":"Application","text":"<p>Code + environment (everything we've covered up until now)</p> <ul> <li>Written &amp; provided by developer</li> <li>Added to image and container in build phase</li> <li>Fixed: can't be changed once image is built</li> <li>Read-only, hense stored in images</li> </ul>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#temporary","title":"Temporary","text":"<p>e.g. entered user input</p> <ul> <li>Fetched/produced in running container</li> <li>Stored in memory or temporary files</li> <li>Dynamic and changing but cleared regularly</li> <li>Read + write, temporary, stored in containers</li> </ul>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#permanent","title":"Permanent","text":"<p>e.g. user accounts</p> <ul> <li>Fetched/produced in running container</li> <li>Stored in files or a database</li> <li>Must not be lost if container stops/restarts</li> <li>Read + write, permanent, stored with containers &amp; volumes</li> </ul> <p>Files written to during container run is lost when containers are removed, but not if container is simply stopped and started again. This is where volumes come in.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#volumes","title":"Volumes","text":"<p>Volumes are folders on our host machine hard drive which are mounted (mapped or made available) into containers. It acts as a 2-way sync between container and host.</p> <p>Volumes persist if a container shuts down. If a container (re)starts and mounts a volume, any data inside of that volume is available in the container.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#two-types-of-external-data-storages","title":"Two types of external data storages","text":"<ul> <li>Volumes (managed by Docker)<ul> <li>Anonymous</li> <li>Named</li> </ul> </li> <li>Bind Mounts (managed by you)</li> </ul> <p>Managed via <code>docker volume</code> commands.</p> <p>Anonymous volumes are created if your Dockerfile only specifies the path for the volume <code>VOLUME [ \"/app/feedback\" ]</code>. Docker sets up a folder or path on your host machine; exact location is unknown to you. These are deleted when the container is deleted, so they're not great for persisting data.</p> <p>Named volumes are great for data which should be persistent but you don't need to edit directly. They remain after a container is removed.</p> <p>Running a container with <code>-v local/path:container/path</code> is the trick. As long as a new container points to the same named volume the data will persist. </p> <p><code>docker run -d -p 3000:80 --rm --name feedback-app -v feedback:/app/feedback feedback-node:volumes</code></p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#bind-mounts","title":"Bind Mounts","text":"<p>Similar to Volumes but you define a folder/path on your host machine. Bind Mounts are perfect for development because instead of copying a snapshot of the code it syncs your changes to the code without you needing to stop, build again, run a new container based off that new build. Named Volumes are good for persisting data but are no good for ongoing development because you have no idea where the local copy is stored.</p> <p>You can set up Bind Mounts by adding another <code>-v</code> flag and point to the absolute path for your local directory:</p> <p><code>docker -run -d -p 3000:80 --rm --name feedback-app -v feedback:/app/feedback -v \"/full/path/to/local/project:/app\" feedback-node:volumes</code></p> <p>You don't have to write out the full path. Use <code>-v $(pwd):/app</code> for sanity</p> <p>The syntax of the second <code>-v</code> flag means you're copying all contents of that folder to the container's <code>/app</code> folder.</p> <p>This renders the following steps in the Dockerfile moot:</p> <pre><code>COPY package.json .\n\nRUN npm instal\n\nCOPY . .\n</code></pre> <p>This is because the second <code>-v</code> flag overwrites everything in the container's <code>/app</code> folder.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#summing-up-anonymousnamed-volumes-vs-bind-mounts","title":"Summing up Anonymous/Named Volumes vs Bind Mounts","text":"<pre><code>docker run -v /app/data ... # Anonymous Volume\ndocker run -v data:/app/data ... # Named Volume\ndocker run -v /path/to/code:/app/code ... # Bind Mount\n</code></pre> <p>Anonymous Volumes - Created specifically for a single container - Survives container shutdown/restart unless --rm is used - Cannot be shared across containers - Since it's anonymous, it can't be re-used (even on same image)</p> <p>Named Volumes - Created in general - not tied to any specific container - Survives container shutdown/restart - removal via Docker CLI - Can be shared across congtainers - Can be reused for same container (across restarts)</p> <p>Bind Mounts - Location on host file syustem, not tied to any specific container - Survives container shutdown/restart - removal on host fs - Can be shared across containers - Can be reused for same container (across restarts)</p> <p>Internal container paths take precedence so for <code>-v $(pwd):/app</code> your local changes will overwrite the container's files, where as <code>-v logs:/app/logs</code> your local will not. If you have a situation like you don't have <code>node_modules</code> locally but you do in the container, you can add an anonymous volume to protect the folder in the container <code>-v /app/node_modules</code>.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#read-only-volumes","title":"Read-Only Volumes","text":"<p>We can enforce read-only on a container level by adding <code>:ro</code> to the end of the container path. We'll still be able to write from the local fs but not from the container:</p> <pre><code>docker run -d --rm -p 3000:80 --name feedback-app -v feedback:/app/feedback -v \"$(pwd):/app:ro\" -v /app/node_modules -v feedback-node:volumes\n</code></pre>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#managing-volumes","title":"Managing Volumes","text":"<p>View the volumes created with <code>docker volume ls</code>. This shows the volumes but NOT the bind mounts, because bind mounts are managed by the user, not Docker.</p> <p>The volumes are created automatically with the <code>-v</code> flag, so it's not usually necessary to manually create them with <code>docker volume create</code>.</p> <p>Volumes in use cannot be removed. You need to stop the container using the volume before it can be removed. You can get rid of unused volumes with <code>docker volume prune</code>.</p> <p>If a volume is removed, the data in it will be lost. Recreating one with the same name will not bring back the data.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#dockerignore","title":".dockerignore","text":"<p>Acts like <code>.gitignore</code> but for docker. In general, you want to add anything which isn't required by your application to execute correctly (such as <code>Dockerfile</code> or <code>.git</code>).</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#working-with-environment-variables-files","title":"Working with Environment Variables &amp; Files","text":"<p>Docker supports build-time ARGuments and runtime ENVironment variables</p> <p>ARG: - Available inside of Dockerfile, NOT accessible in CMD or any application code - Set an iamge build (docker build) via --build-arg</p> <p>ENV: - Available inside of Dockerfile and in application code - Set via ENV in Dockerfile or via <code>--env</code> or <code>-e</code> on docker run</p> <pre><code>ENV PORT 80 # This will be accessible from within the application\n\nEXPOSE $PORT\n</code></pre> <p>You can also point to a file to define environment variables</p> <p>.env</p> <pre><code>PORT=8000\n</code></pre> <p><code>docker run -d -p 3000:8000 --env-file ./.env ...</code></p> <p>This is advantageous because it allows you to run the same command over and over and you only need to update a file if vars need to change.</p> <p>One important note about environment variables and security: Depending on which kind of data you're storing in your environment variables, you might not want to include the secure data directly in your Dockerfile.</p> <p>Instead, go for a separate environment variables file which is then only used at runtime (i.e. when you run your container with docker run).</p> <p>Otherwise, the values are \"baked into the image\" and everyone can read these values via docker history . <p>For some values, this might not matter but for credentials, private keys etc. you definitely want to avoid that!</p> <p>If you use a separate file, the values are not part of the image since you point at that file when you run docker run. But make sure you don't commit that separate file as part of your source control repository, if you're using source control.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#using-build-arguments-arg","title":"Using build arguments (ARG)","text":"<pre><code>ARG DEFAULT_PORT=80\n\nENV PORT $DEFAULT_PORT\n\nEXPOSE $PORT\n</code></pre>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#networking-cross-container-communication","title":"Networking &amp; Cross-Container Communication","text":"<p>Containers can commiunicate with the www out of the box.</p> <p>Instead of using <code>localhost</code> when talking to another service on the same machine, use <code>host.docker.internal</code> if a domain is needed. This will be recognized by Docker as a service on the host machine.</p> <p>Point to the IP address of the other container. To find out the IP address run <code>docker container inspect &lt;container name&gt;</code>. You will also need to point to the port which can be found in the same output as the inspect command or <code>docker ps -a</code> with the port being listed next to the name of the container.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#container-networks","title":"Container Networks","text":"<p>Within a Docker network, all containers can commuincate with each other and IPs are automatically resolved.</p> <p>They need to be manually set up (unlike volumes). <code>docker network create &lt;network name&gt;</code> and then you can run a container with it <code>docker run &lt;options&gt; --network &lt;network name&gt;</code>.</p> <p>If more than one container are part of the same network, you can plug in the name of the container for the domain when trying to connect. Example:</p> <p>Say you want to connect a node app to a mongodb instance. You can put the mongodb instance in a container and change the node code to the following:</p> <pre><code>mongoose.connect(\n  'mongodb://mongodb-instance:27017/swfavorites',\n  { useNewUrlParser: true },\n  (err) =&gt; {\n    if (err) {\n      console.log(err);\n    } else {\n      app.listen(3000);\n    }\n  }\n);\n</code></pre> <p>Followed by the below commands:</p> <pre><code>docker network create favorites-net\ndocker run -d --name mongodb-instance --network favorites-net mongo\ndocker run -d -p 3000:3000 --rm --name favorites --network favorites-net favorites-node\n</code></pre> <p>There's no need to expose ports if you're using a container network. It's only needed if you're trying to connect to the container from outside the container (like from local machine) or container network. This is mostly helpful for server-side microservices. If you have a backend service talking with a frontend app, you'll need to expose ports to the frontend app since it runs in a browser and you'll need to expose ports for the backend because the browser will make requests to it. You can use a container network AND expose ports in this case.</p> <p>When using a container network or host.docker.internal, Docker will NOT replace your source code. It semply detects outgoing requests and resolves the IP for such requests.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#docker-network-drivers","title":"Docker Network Drivers","text":"<p>Docker Networks actually support different kinds of \"Drivers\" which influence the behavior of the Network.</p> <p>The default driver is the \"bridge\" driver - it provides the behavior shown in this module (i.e. Containers can find each other by name if they are in the same Network).</p> <p>The driver can be set when a Network is created, simply by adding the <code>--driver</code> option.</p> <pre><code>docker network create --driver bridge my-net\n</code></pre> <p>Of course, if you want to use the \"bridge\" driver, you can simply omit the entire option since \"bridge\" is the default anyways.</p> <p>Docker also supports these alternative drivers - though you will use the \"bridge\" driver in most cases:</p> <ul> <li>host: For standalone containers, isolation between container and host system is removed (i.e. they share localhost as a network)</li> <li>overlay: Multiple Docker daemons (i.e. Docker running on different machines) are able to connect with each other. Only works in \"Swarm\" mode which is a dated / almost deprecated way of connecting multiple containers</li> <li>macvlan: You can set a custom MAC address to a container - this address can then be used for communication with that container</li> <li>none: All networking is disabled.</li> <li>Third-party plugins: You can install third-party plugins which then may add all kinds of behaviors and functionalities</li> </ul> <p>As mentioned, the \"bridge\" driver makes most sense in the vast majority of scenarios.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#building-multi-container-applications","title":"Building Multi-Container Applications","text":"","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#adding-data-persistence-to-mongodb-with-volumes","title":"Adding Data Persistence to MongoDB with Volumes","text":"<p>Look up where MongoDB stores things and then created a named volume for it <code>docker run --name mongodb -v data:/data/db --rm -d --network goals-net mongo</code>.</p> <p>You can add credentials by using environment variables.</p> <p>The rest of this section was rehashing what was already taught above.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#using-docker-compose-for-multi-container-orchestration","title":"Using Docker Compose for Multi-Container Orchestration","text":"<p>Great for managing multiple containers on the same host. You can build and run with a single command as well as stop with a single command.</p> <p>Docker compose is NOT:</p> <ul> <li>A replacement for Dockerfiles for custom images</li> <li>A replacement for images or containers</li> <li>Suited for managing multiple containers on different hosts</li> </ul> <p>Compose files must define services (a.k.a. containers) and also define published ports, environment variables, volumes, and networks.</p> <p>docker-compose.yaml </p> <pre><code>version: \"3.8\" # This is the version of the docker compose specification we want to use, NOT the app.\nservices:\n  mongodb:\n    image: \"mongo\"  # This is a premade image from Dockerhub so no need to build\n    volumes:\n      - data:/app/data\n    # environment:\n    #   - MONGO_INITDB_ROOT_USERNAME: foo  # Can also use the syntax MONGO_INITDB_ROOT_USERNAME=foo\n    #   - MONGO_INITDB_ROOT_password: password \n    env_file:\n      - ./env/mongo.env  # This needs the \"key=value\" syntax rather than \"key: value\"\n    # networks:  # You can specify your network, but Docker will do this automatically with compose. Specifying will add this service to both this and the auto-generated network\n    #   - goals-net\n  backend:\n    build: ./backend  # Point to the folder where the Dockerfile is. Short form.\n    # build:\n    #   context: ./backend\n    #   dockerfile: Dockerfile-dev  # Only needed if named something other than \"Dockerfile\"\n    #   args:\n    #     some-arg: 1\n    ports:\n      - \"80:80\"  # Can expose multiple ports per container\n    volumes:\n      - logs:/app/logs\n      - ./backend:/app  # Relative paths allowed for bind mounts in a compose file\n      - /app/node_modules\n    env_file:\n      - ./env/backend.env\n    depends_on:\n      - mongodb\n  frontend:\n    build: ./frontend\n    ports: \n      - '3000:3000'\n    volumes: \n      - ./frontend/src:/app/src\n    stdin_open: true  # the \"-i\" in the \"docker run\" command\n    tty: true  # the \"-t\" in the \"docker run\" command\n    depends_on: \n      - backend\n\n# If you have named volumes you need to add volumes to the top level and specify the name with no value. It's a thing Docker needs to recognize named volumes.\n# You can share the volume between containers if you specify i8t for each service.\n# Anonymous volumes and bind mounts dont need to be specified here.\nvolumes:\n  data:\n  logs:\n</code></pre> <p>By default the <code>--rm</code> flag applies when using <code>docker compoase</code>.</p> <p>You still need to add the <code>-d</code> flag to run in detached mode: <code>docker-compose up -d</code>.</p> <p>The <code>docker-compose down</code> command will NOT remove volumes. For that you need to run <code>docker-compose down -v</code>.</p> <p>You can use docker-compose to just build but not run by using <code>docker-compose build</code>.</p> <p>If you want docker-compose to build a fresh image you can force it with the <code>--build</code> flag. That's for when you have an existing image but maybe have had a code change since the last build. <code>docker-compose up --build</code>.</p> <p>You can also force a container name through the yaml file by using <code>container_name</code>:</p> <pre><code>...\nservices:\n    mongodb:\n        image: \"mongo\"\n        volumes:\n            - data:/data/db\n        container_name: mongodb\n\n    ...\n</code></pre> <p>This will cause the container to be named \"mongodb\" instead of the auto-generated name \"project-name_mongodb_01\".</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#working-with-utility-containers-executing-commands-in-containers","title":"Working with Utility Containers &amp; Executing Commands In Containers","text":"<p>A container with an environment but no application. Useful for setting up a dev environment without having to install a bunch of stuff on your host machine. - Use a bind mount to save files from the container to the local machine. You could make a super slim node image, run <code>npm init</code>, and have the newly generated package.json file appear in your local directory.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#executing-commands","title":"Executing Commands","text":"<p><code>docker exec -it &lt;container name&gt; &lt;command&gt;</code></p> <p>To \"ssh\" into a container, simply run <code>docker exec -it &lt;container name&gt; /bin/bash</code>.</p> <p>Using <code>ENTRYPOINT</code> is very similar to <code>CMD</code> in the Docker file. The main difference being that if you use <code>CMD</code>, you can override the default <code>CMD</code> by adding it to the end of the command <code>docker run -it node npm init</code>. With <code>ENTRYPOINT</code> whatever you add to the end of your run command is appended to whatever's inside ENTRYPOINT.</p> <p>Example:</p> <pre><code>FROM node:14-alpine\n\nWORKDIR /app\n\nENTRYPOINT [ \"npm\" ]\n</code></pre> <p>Then you can run whatever npm command you want with the run command: <code>docker run -it -v $(pwd):/app myimage init</code> will run <code>npm init</code> on startup.</p> <p>We can do this with <code>docker-compose</code>:</p> <pre><code>version: \"3.8\"\nservices:\n    node:\n        build: ./\n        stdin_open: true\n        tty: true\n        volumes:\n            - ./:/app\n</code></pre> <p>The command you'd use is <code>docker-compose run npm init</code>.</p> <p>NOTE: Containers will not be removed if you use <code>run</code>. You still need that <code>--rm</code> flag: <code>docker-compose run --rm npm init</code>.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#deploying-docker-containers","title":"Deploying Docker Containers","text":"<p>This is the key selling point of Docker. What works on your machine (in a container) will also work after deployment. Everything's exactly the same. so there are no surprises.</p> <p>Things to look out for:</p> <ul> <li>Bind mounts shouldn't be used in Production</li> <li>Containerized apps might need a build step (e.g. React apps)</li> <li>Multi-container projects might need to be split (or should be split) across multiple hosts/remote machines</li> <li>Tradeoffs between control and responsibility might be worth it</li> </ul> <p>Example: Deploy to AWS EC2:</p> <ol> <li>Create and launch an EC2 instance, VPC, and security group</li> <li>Configure security group to expose all required ports to WWW</li> <li>Connect to instance (SSH), install Docker, and run container</li> </ol>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#bind-mounts-volumes-copy","title":"Bind Mounts, Volumes, &amp; COPY","text":"<p>In Development:</p> <ul> <li>Containers should encapsulate the runtime environment but not necessarily the code</li> <li>We use bind mounts to provide our local host project files to the running container</li> <li>Allows for instant updates without restarting the container</li> </ul> <p>In Production:</p> <ul> <li>A container should really work standalone, you should NOT have source code on your remote machine</li> <li>Image/Container is \"single source of truth\"</li> <li>Use COPY instead of bind mounts to copy a code snapshot into the image</li> <li>Ensures that every image runs without any extra, surrounding configuration or code</li> </ul>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#deploying-to-ec2","title":"Deploying to EC2","text":"<p>After creating an EC2 instance and SSHing in you'll need to install docker.</p> <p>Assuming you're using Amazon Linux:</p> <pre><code>sudo yum update -y\nsudo amazon-linux-extras install docker\nsudo service docker start\n</code></pre>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#deploy-source-code-vs-image","title":"Deploy Source Code vs Image","text":"<p>We can either deploy the source code or the built image.</p> <p>Deploying source you'd push code to remote machine, run <code>docker build</code> and then <code>docker run</code>. This really isn't necessary and just creates more steps.</p> <p>Alternatively we can prep everything locally and get the built image to the destination without having to do anything there.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#running-publishing-the-app-on-ec2","title":"Running &amp; Publishing the App on EC2","text":"<p>Once a built image is pushed to a repository you can simply do <code>docker run &lt;username/image name&gt;</code> to run a container based off of that image. No login required if it's a public image.</p> <p>You can see the public IP address under Instances in the EC2 section of the AWS console. However, by default just entering the IP address in the browser will NOT load the application. You need to update the security group first.</p> <p>You can see what security groups an instance has by clicking on the Security tab while viewing the instance in the AWS console. Click on the link for the security group (usually named launch-wizard-somenumber) and you'll see the security group settings. By default Inbound rules only allows SSH but Outbound is all traffic. This is why downloading the image from Dockerhub works but it wont load in the browser.</p> <p>Edit the Inbound rules and add one for HTTP. Only allows port 80 but that's fine. Allow any IP to access.</p> <p>Be sure to run <code>docker pull</code> if you update the published image. Otherwise Docker will use the already existing image that it's using.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#disadvantages-of-our-current-approach","title":"Disadvantages of our Current Approach","text":"<p>How we did this was fine but there are some downsides:</p> <ul> <li>Lots of manual steps for setting it up </li> <li>The way we set up EC2 we fully \"owned\" it</li> <li>We were responsible for it and its security </li> <li>Had to keep software updated and manage network/security groups firewall</li> </ul> <p>A better alternative might be using a managed service, specifically ECS (Elastic Container Service). Azure and GCP have equivalent services as well.</p> <p>However, ECS is not available for the free tier so it will cost money.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#deploying-to-ecs","title":"Deploying to ECS","text":"<p>You'll want to use a load balancer that your domain points to and Elastic File System (EFS) Volumes to manage persisted data.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#understanding-a-common-problem","title":"Understanding a Common Problem","text":"<p>Some apps/projects require a build step, such as an optimization script that need sto be executed AFTER development but BEFORE deployment (e.g. Angular apps).</p> <p>This means the development and production setup are NOT equal. This can be a problem and works against what Docker's trying to do.</p> <p>We need to build a container that's meant for the build-optimized code files, not the dev version (what we use when actively developing with <code>npm start</code>).</p> <p>This is when we'll likely need 2 Dockerfiles. One will need to be named something else like <code>Dockerfile.prod</code>.</p> <pre><code>FROM node:14-alpine\n\nWORKDIR /app\n\nCOPY package.json .\n\nRUN npm install\n\nCOPY . .\n\nCMD [ \"npm\", \"run\", \"build\" ]\n</code></pre> <p>This isn't enough though. We need a server for the build files. This is where multi-stage builds come in.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/docker-crash-course/#mutli-stage-builds","title":"Mutli-Stage Builds","text":"<p>The above yaml can be seen as the first stage. We'll only need the node image to build but we can use a different image to serve up the files.</p> <p>Every FROM instruction creates a new stage in your Docker file. Even if you use the same image as in the previous step.</p> <pre><code># Adding \"as build\" names the stage and makes it possible to copy contents from one stage to another\nFROM node:14-alpine as build\n\nWORKDIR /app\n\nCOPY package.json .\n\nRUN npm install\n\nCOPY . .\n\nRUN npm run build\n\nFROM nginx:stable-alpine\n\n# The example app we're using is made with React which puts its build files in \"build\" instead of \"dist\"\n# See nginx image documentation on dockerhub. It explains the default directory which is what we're copying to below\nCOPY --from=build /app/build /usr/share/nginx/html\n\nEXPOSE 80  # Because port 80 is default for nginx\n\nCMD [ \"nginx\", \"-g\", \"daemon off;\" ]\n</code></pre> <p>If you're deployng a frontend app to the same server as your backend code you can remove \"localhost\" in the API request URLs.</p> <p>To build off of another file instead of the default Dockerfile you need the <code>-f</code> flag when running <code>docker build</code>:</p> <p><code>docker build -f frontend/Dockerfile.prod -t academind/goals-react ./frontend</code></p> <p>This gives the Docker engine the context in which it will be building from.</p> <p>We can also target build stages and only have them build by using the <code>--target</code> flag:</p> <p><code>docker build --target build -f frontend/Dockerfile.prod -t academind/goals-react ./frontend</code></p> <p>This will cause Docker to skip the <code>FROM nginx:stable-alpine</code> stage while building.</p>","tags":["Cloud","Docker"]},{"location":"environments/cloud/kubernetes-crash-course/","title":"Kubernetes Crash Course","text":"<p>From Udemy's Docker &amp; Kubernetes: The Practical Guide (2022 Edition)</p> <p>[[TOC]]</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#the-basics","title":"The Basics","text":"<p>Manual deployment of containers is hard to maintain, error-prone, and annoying (even beyond security and configuration concerns!)</p> <p>Other issues:</p> <ul> <li>Containers might crash/go down and need to be replaced.</li> <li>We might need more container instances upon traffic spikes</li> <li>Incoming traffic should be distrubted equally</li> </ul> <p>AWS ECS can help. It provides the following:</p> <ul> <li>Container health checks and automatic re-deployment</li> <li>Autoscaling</li> <li>Load balancer</li> </ul> <p>The problem is, this locks us into a specific service by a specific cloud provider. If you want to migrate to something else you need to learn about the specifics, services, and config options of another provider.</p> <p>This is where Kubernetes comes to the rescue.</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#what-exactly-is-kubernetes","title":"What exactly is Kubernetes?","text":"<p>It's an open source (and de-facto standard) for orchestrating container deployments. It helps with:</p> <ul> <li>Automatic deployment</li> <li>Scaling and load balancing</li> <li>Management</li> </ul> <p>Also known as K8s. It's derived by replacing the eight letters of \u201cubernete\u201d with the digit 8 because less typing.</p> <p>You can set up the Kubernetes configs via a file, pass that to some cloud provider or even your own datacenter, and you're good to go.</p> <p>You can also include cloud-provider-specific settings in your Kubernetes configs which makes it all the more flexible/powerful</p> <p>You define the end result in your Kubernetes configs and the cloud provider will take care of it for you.</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#what-it-is-and-is-not","title":"What it IS and IS NOT","text":"<ul> <li>IT'S NOT a cloud service provider<ul> <li>IT IS an open source project</li> </ul> </li> <li>IT'S NOT a service by a cloud service provider<ul> <li>IT IS something that can be used by any provider</li> </ul> </li> <li>IT'S NOT just an application you run on some machine<ul> <li>IT IS a collection of concepts and tools</li> </ul> </li> <li>IT'S NOT an alternative to Docker<ul> <li>IT IS something that works with Docker containers</li> </ul> </li> <li>IT'S NOT a paid service<ul> <li>It's free and open source</li> </ul> </li> </ul> <p>One way to think of it: Kubernetes is like Docker-compose for multiple machines.</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#core-concepts-and-architecture","title":"Core Concepts and Architecture","text":"<p>Pod: The smallest possible unit in the k8s world which you can create in a config file. A pod simply holds one more more containers.</p> <p>Pods run in Worker Nodes.</p> <p>Worker Node: run the containers of your application. Nodes are your machines/virtual instances. An EC2 instance could be a worker node.</p> <p>Proxy/Config: Another tool k8s sets up for you on a Workker Node to control the network traffic of the pods on that Worker Node.</p> <p>You'll typically have multiple worker nodes for larger applicaitons because you might need more than one server to have enough compute power to run your application.</p> <p>Multiple pods can be created and removed to scale your app</p> <p>Master Node: Has a Control Plane that orchestrates all the Worker Nodes. Does the heavy lifting with scaling and replacing pods.</p> <p>The Control Plane is a collection of tools/componsnets to help with managing the worker nodes.</p> <p>All of this together (Master Node and Worker Nodes) forms a Cluster. The Master Node sends instructions to the CLoud Provider API about what resources are needed to support the Cluster network.</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#your-work-vs-k8s-work","title":"Your work vs k8s work","text":"<p>What you need to do / set up (i.e. what k8s requires):</p> <ul> <li>Create the Cluster and the Node Instances (Worker and Master Nodes)</li> <li>Set up API server, kubelet and other k8s services/software on Nodes</li> <li>Create other (cloud) provider resources that might be needed (e.g. Load Balancer, Filesystems)</li> </ul> <p>What k8s will do:</p> <ul> <li>Create your objects (e.g. Pods) and manage them</li> <li>Monitor Pods and re-create them, scale Pods, etc</li> <li>Kuberenetes utilizes the provided (cloud) resource to apply your configuration/goals</li> </ul> <p>It's important to remember that Kubernetes does not create resources for you. You need to do this first and k8s will utilize it. </p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#a-closer-look-at-worker-nodes","title":"A closer look at Worker Nodes","text":"<p>Think of them as a computer/machine/virtual instance. It's managed by the Master Node.</p> <p>The Pod inside hosts one or more application containers and their resources (volumes, IP, run config). They are created and managed by Kubernetes.</p> <p>You would have Docker, kubelet, and kube-proxy installed on the Worker Node. </p> <p>A kubelet handles communication between Master and Worker Nodes</p> <p>A kube-proxy us used to manage Node and Pod network communication (traffic to and from the Pod and Worker Node)</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#a-closer-look-at-the-master-node","title":"A closer look at the Master Node","text":"<p>The most important software in the Master Node is the API Server. This is the counterpart to the kubletes in the Worker Node.</p> <p>Scheduler: Watches for new Pods, selects Workerk Nodes to run them on.</p> <p>Kube-Controller-Manager: Watches and controsl Worker Nodes, correct number of Pods, and more.Scheduler: Watches for new Pods, selects Workerk Nodes to run them on.</p> <p>Cloud-Controller-Manager: Like Kube-Controller-Manager BUT for a specific Cloud Provider: Knows how to interact with Cloud Provider resources.</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#important-terms-and-concepts","title":"Important Terms and Concepts","text":"<ul> <li>Cluster: A set of Node machines which are running the containerized application (Worker Nodes) or control other Nodes (Master Node)</li> <li>Nodes: Physical or virtual machine with a certain hardware capacity which hosts one or multiple Pods and communicates with the Cluster<ul> <li>Master Node: Cluster Control Plane, managing the Pods across Worder Nodes</li> <li>Worker Node: Hosts Pods, running app container (+ resources)</li> </ul> </li> <li>Pods: Hold the actual running app containers and their required resources (e.g. volumes)</li> <li>Containers: Normal (Docker) containers</li> <li>Services: A logical set (group) of Pods with unique, Pod and Container independent IP addresses</li> </ul>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#deep-dive-into-core-concepts","title":"Deep Dive Into Core Concepts","text":"","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#installation","title":"Installation","text":"<p>You need two tools to get started: kubectl and a cluster.</p> <p>kubectl: A tool for sending instructions to the cluster (e.g. a new deployment). kubectl sends commands to the Master Node which then passes them on to the Worker Nodes.</p> <p>You can use a tool called Minikube to get things going locally and creating clusters.</p> <p>Install the tools by following the docs:</p> <ul> <li>kubectl</li> <li>minikube</li> </ul> <p>Once both are installed you can verify with <code>kubectl cluster-info</code> and <code>minikube status</code>. The two tools work hand in hand so you can just run <code>kubectl</code> commands and Minikube will automatically be utilized.</p> <p>You can use the Docker driver as the hypervisor to create a cluster and this is the default for Linux machines. Run <code>docker ps</code> and you'll see a minicube container. </p> <p>Minikube also provides a web interface that runs on your localhost. Run <code>minikube dashboard</code> and a new tab will open in your default browser with info about your new cluster.</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#understanding-kubernetes-objects","title":"Understanding Kubernetes Objects","text":"<p>K8s works with and sees all resources as Objects. Pods, Deployments, Services, Volumes, etc are all objects.</p> <p>Objects can be created in two ways: Imperatively or Declaratively.</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#the-pod-object","title":"The Pod Object","text":"<p>The smallest \"unit\" k8s interacts with. They act as a tiny wrapper for containers.</p> <ul> <li>Contains and runs one more multiple containers though it's most common for there to be one container per Pod.</li> <li>Contain shared resources (e.g. volumes) for all Pod containers.</li> <li>Has a cluster-internal IP by default<ul> <li>Containers inside a Pod can communicate via localhost</li> </ul> </li> </ul> <p>Pods are designed to be ephemeral: K8s will start, stop, and replace them as needed.</p> <p>For Pods to be managed for you, you need a \"Controller\" (e.g. a \"Deployment\")</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#the-deployment-object","title":"The Deployment Object","text":"<p>Controls one or more Pods</p> <ul> <li>You set a desired state, K8s then changes the actual state<ul> <li>Define which Pods and containers to run and the number of instances</li> </ul> </li> <li>Deployments can be paused, deleted, and rolled back</li> <li>Deployments can be scaled dynamically and automatically</li> </ul> <p>Deployments manage a Pod for you, you can also create multiple Deployments.</p> <p>To clarify: One kind of Pod (e.g. a Pod with two specific containers). Multiple instances of that Pod are possible.</p> <p>You typically don't directly control Pods, instead you use Deployments to set up the desired end state.</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#deployment-with-the-imperative-approach","title":"Deployment with the Imperative Approach","text":"<p>First you need to build your Docker image. Then you use <code>kubectl</code> to deploy your built image to your cluster. </p> <p><code>kubectl create deployment app-name --image=some-image-name</code></p> <p>This isn't quite right though. It'll show a success message after creation but it'll show that the deployment failed when you check on the deployments in your cluster with <code>kubectl get deployments</code></p> <pre><code>$ kubectl get deployments\nNAME        READY   UP-TO-DATE   AVAILABLE   AGE\nfirst-app   0/1     1            0           6s\n</code></pre> <p>Check on the Pods for a more precise picture:</p> <pre><code>$ kubectl get pods\nNAME                         READY   STATUS             RESTARTS   AGE\nfirst-app-7cbbc8669d-tg268   0/1     ImagePullBackOff   0          2m57s\n</code></pre> <p>By default Minikube looks for images in an image registery so we'll need to pull one down.</p> <p>It is possible to use a locally built image and that'll be covered a bit later</p> <p>Delete the Pod, create a new repository in Dockerhub (or whereever), then tag the Docker image to be pushed up.</p> <pre><code>$ kubectl delete deployment first-app                                                                                         \ndeployment.apps \"first-app\" deleted\n$ docker tag kub-first-app stevewhitmore/kub-first-app\n$ docker push stevewhitmore/kub-first-app\n</code></pre> <p>Now it'll actually deploy successfully:</p> <pre><code>$ kubectl create deployment first-app --image=stevewhitmore/kub-first-app\ndeployment.apps/first-app created\n$ kubectl get deployments\nNAME        READY   UP-TO-DATE   AVAILABLE   AGE\nfirst-app   1/1     1            1           37s\n$ kubectl get pods\nNAME                         READY   STATUS    RESTARTS   AGE\nfirst-app-8575b449bf-vr48r   1/1     Running   0          76s\n</code></pre> <p>Now would be a good time to run <code>minkube dashboard</code> to monitor your new cluster.</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#kubectl-behind-the-scenes","title":"kubectl: Behind the Scenes","text":"<p><code>kubectl create deployment --image ...</code> is sent to the Master Node (Control Plane) where the Scheduler analyzes currently running Pods and finds the best Node for the new Pod(s). It sends it to the Worker Node where the kubelet manages the Pod and Containers. </p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#the-service-object","title":"The Service Object","text":"<p>Exposes Pods to the Cluster or externally.</p> <ul> <li>Pods have an internal IP address by default - it changes whena  Pod is replaced<ul> <li>Finding Pods is hard if the IP changes all the time</li> </ul> </li> <li>Services group Pods with a shared IP</li> <li>Services can allow external access to Pods<ul> <li>The default (internal only) can be overwritten</li> </ul> </li> </ul> <p>Without Services, Pods are very hard to reach and communication is difficult.</p> <p>Reaching a Pod from outside a Cluster is not possible at all wihtout Services.</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#exposing-a-deployment-with-a-service","title":"Exposing a Deployment with a Service","text":"<p>You can expose a deployment with the following command: <code>kubectl expose deployment app-name --port=8080</code>. You can also pass in a <code>--type</code> flag which has the default value <code>ClusterIP</code>. Other choices would be <code>NodePort</code> and <code>LoadBalancer</code>.</p> <p>LoadBalancer is the most common for external access</p> <p>You'll see 2 services when you check. One was created by K8s with the default ClusterIP type. The other is the one we made:</p> <pre><code>$ kubectl get services\nNAME         TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nfirst-app    LoadBalancer   10.103.106.200   &lt;pending&gt;     8080:32187/TCP   13s\nkubernetes   ClusterIP      10.96.0.1        &lt;none&gt;        443/TCP          4h\n</code></pre> <p>External IP would have a different value if this were deployed somewhere remote like AWS. Since we're using Minikube it has the <code>&lt;pending&gt;</code> value.</p> <p>An extra command is needed since we're using Minikube locally to expose a port to a custom local IP address: <code>minikube service first-app</code>. If no output is automatically showed you can run <code>minikube service list</code> to get the custom address (e.g. http://192.168.49.2:32187)</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#restarting-containers","title":"Restarting Containers","text":"<p>If a container crashes k8s will automatically restart it. We can see this with the example app we deployed <code>first-app</code> which returns an exit code 1 if the endpoint \"error\" is hit:</p> <pre><code>$ kubectl get pods\nNAME                         READY   STATUS    RESTARTS   AGE\nfirst-app-8575b449bf-vr48r   1/1     Running   0          110m\n$ curl http://192.168.49.2:32187/error\ncurl: (52) Empty reply from server\n$ kubectl get pods\nNAME                         READY   STATUS    RESTARTS     AGE\nfirst-app-8575b449bf-vr48r   1/1     Running   1 (5s ago)   111m\n</code></pre> <p>Each time it crashes there will be a longer delay before a restart to avoid infinite loops in case there is a container that always crashes for some reason. Saves resources that way. </p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#scaling-in-action","title":"Scaling in Action","text":"<p>Suppose we don't have auto-scaling set up. We can manually do so by running something like <code>kubectl scale deployment/first-app --replicas=3</code>. A replica is simply an instance of a Pod/Container. 3 replicas means that the same Pod/Container is running 3 times.</p> <pre><code>$ kubectl scale deployment/first-app --replicas=3\ndeployment.apps/first-app scaled\n$ kubectl get pods\nNAME                         READY   STATUS    RESTARTS      AGE\nfirst-app-8575b449bf-c8cmv   1/1     Running   0             4s\nfirst-app-8575b449bf-qh7fq   1/1     Running   0             4s\nfirst-app-8575b449bf-vr48r   1/1     Running   2 (11m ago)   124m\n</code></pre> <p>This is very useful because if one crashes the other 2 will keep going while the crashed one restarts.</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#updating-deployments","title":"Updating Deployments","text":"<p>If you have a change to some code you'll of course rebuild the image, push it up, then run  the <code>set</code> comand.</p> <p>New images are only pulled if there is a new tag</p> <pre><code>$ docker build -t stevewhitmore/kub-first-app:2 .\nSending build context to Docker daemon   5.12kB\nStep 1/7 : FROM node:14-alpine\n ---&gt; a310e5ff0582\nStep 2/7 : WORKDIR /app\n ---&gt; Using cache\n ---&gt; e75867f2c578\nStep 3/7 : COPY package.json .\n ---&gt; Using cache\n ---&gt; e61d6b9449ce\nStep 4/7 : RUN npm install\n ---&gt; Using cache\n ---&gt; bd7bf69b3e48\nStep 5/7 : COPY . .\n ---&gt; Using cache\n ---&gt; 258e32889b80\nStep 6/7 : EXPOSE 8080\n ---&gt; Using cache\n ---&gt; f550f81c3a4f\nStep 7/7 : CMD [ \"node\", \"app.js\" ]\n ---&gt; Using cache\n ---&gt; 0c57ed032282\nSuccessfully built 0c57ed032282\nSuccessfully tagged stevewhitmore/kub-first-app:2\n$ docker push stevewhitmore/kub-first-app:2\nThe push refers to repository [docker.io/stevewhitmore/kub-first-app]\nc31229021370: Layer already exists \n52968ea7d37a: Layer already exists \n21a603489ada: Layer already exists \nd496fe641acc: Layer already exists \n05accea593f7: Layer already exists \n542676b737d2: Layer already exists \n27d39fe7f7fa: Layer already exists \na1c01e366b99: Layer already exists \n2: digest: sha256:985609b9873e5af5f46e0878856f36db25752a7b23a61def1610ab486a86d6c3 size: 1989\n$ kubectl set image deployment/first-app kub-first-app=stevewhitmore/kub-first-app:2\ndeployment.apps/first-app image updated\n</code></pre>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#deployment-rollbacks-history","title":"Deployment Rollbacks &amp; History","text":"<p>If a Pod fails to start for some reason you can roll it back by running <code>kubectl rollout undo deployment/first-app</code>. This will undo the last Pod deployed.</p> <p>You can see the rollout history with <code>kubectl rollout history deployment/first-app</code>. To see specifics about a previous rollout you can use the <code>--revision</code> flag: <code>kubectl rollout history deployment/first-app --revision=1</code></p> <p>You can go back to a specific rollout by running <code>kubectl rollout undo deployment/first-app --to-revision=1</code></p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#imperative-vs-declarative-deployments","title":"Imperative vs Declarative Deployments","text":"<p>Like with Docker you can use a yaml file to make it easier to control things. You create a resource definition file to make working with your cluster easier.</p> <p>Imperative:</p> <ul> <li>You use commands like <code>kubectl create deployment ...</code></li> <li>Individual commands are executed to trigger certain Kubernetes actions</li> <li>Comparable to using <code>docker run</code> only</li> </ul> <p>Declarative:</p> <ul> <li>You use commands like <code>kubectl apply -f config.yaml</code></li> <li>A config file is defined and applied to change the desired state</li> <li>Comparable to use Docker Compose with compose files</li> </ul>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#creating-a-deployment-configuration-file-declarative-approach","title":"Creating a Deployment Configuration File (Declarative Approach)","text":"<p>First, make sure there are no Deployments, Pods, or Services (outside of the default ClusterIP one)</p> <pre><code>$ kubectl delete service first-app\nservice \"first-app\" deleted\n$ kubectl delete deployment first-app\ndeployment.apps \"first-app\" deleted\n$ kubectl get deployments\nNo resources found in default namespace.\n$ kubectl get pods\nNo resources found in default namespace.\n$ kubectl get services\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.96.0.1    &lt;none&gt;        443/TCP   100m\n</code></pre> <p>Next, create a yaml file. It can be called anything. There is a specific syntax you need to be conscious of. Below is the bare minimum needed to create a Deployment with one Pod with one Container:</p> <pre><code>apiVersion: apps/v1  # You can find versions in the docs. Be careful about pointing to the latest version for obvious reasons.\nkind: Deployment  # You need to specify what kind of resource you're defining. It could also be Service or Job or something like that.\nmetadata:\n  name: second-app-deployment  # This is the equivalent of `kubectl create deployment app-name ...`\nspec:\n  replicas: 1  # Defaults to 1. 0 is also a possibility if you don't want any Pods right away.\n  selector:  # This is essential for declaritively deployed Pods. A Deployment continuously watches the Pods out there and sees which ones it should control. It selects these pods with the selector. This is true for all K8s resources. The \"matchLabels\" below are for the Deployment to know which labels to match against. These labels are defined below under \"template\" for the Pods. If other labels are below \"template\" but not also defined under \"matchLabels\" then they will not be controlled by this Deployment.\n    matchLabels:\n      app: second-app\n      tier: backend\n  template:  # This is where we define the Pod that should be created. You don't use \"kind\" here because a template of a Deployment will always be a Pod. Pod templates can only have 2 next level keys: metadata and spec.\n    metadata:  # We're creating a new Object here so we need new metadata.\n      labels:  # We can put \"name\" here instead but we chose label\n        app: second-app  # This key can be \"deployment\" or \"depl\" instead.\n        tier: backend\n    spec: \n      containers:  #  You can have multiple Containers per Pod. Each defined below with a dash.\n        - name: second-node  # Arbitrary name of app\n          image: stevewhitmore/kub-first-app:3\n        # - name: ...\n        #   image: ...\n</code></pre> <p>Execute this with <code>kubectl apply -f=/path/to/file.yaml</code>.</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#creating-a-service-declaratively","title":"Creating a Service Declaratively","text":"<p>You can have multiple config files. In our case we have a <code>deployment.yaml</code> and a <code>service.yaml</code> which is defined below.</p> <pre><code>apiVersion: v1  # This is just \"v1\" since it comes from the core package (unlike the one in \"apps\" like we used for deployments.yaml)\nkind: Service\nmetadata:\n  name: backend\nspec:\n  selector:  # This is a bit different than for Deployment objects because this API is a bit older. The labels are defined directly instead of using \"matchLabel\" like we do with `deployment.yaml`.\n    app: second-app\n  ports:  # We use the below instead of `kubectl expose deployment first-app --port=... --type=LoadBalancer`\n    - protocol: 'TCP'  # Defaults to \"TCP\"\n      port: 80  # The port within the container\n      targetPort: 8080  # The port on the host machine\n    # - protocol: 'TCP'\n    #   port: 443\n    #   targetPort: 443\n  type: LoadBalancer\n</code></pre> <p>Deploy the Pod and expose it with the service by running the following: <code>kubectl apply -f=deployment.yaml -f=service.yaml</code>. Find the service name with <code>kubectl get services</code> and then the local IP with <code>minikube service (service name)</code>, followed by <code>minikube list services</code>:</p> <pre><code>$ kubectl apply -f=deployment.yaml -f=service.yaml\ndeployment.apps/second-app-deployment unchanged\nservice/backend created\n$ kubectl get services\nNAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nbackend      LoadBalancer   10.96.108.148   &lt;pending&gt;     80:31665/TCP   14s\nkubernetes   ClusterIP      10.96.0.1       &lt;none&gt;        443/TCP        3h8m\n$ minikube service backend\n$ minikube service list\n|----------------------|---------------------------|--------------|---------------------------|\n|      NAMESPACE       |           NAME            | TARGET PORT  |            URL            |\n|----------------------|---------------------------|--------------|---------------------------|\n| default              | backend                   |           80 | http://192.168.49.2:31665 |\n| default              | kubernetes                | No node port |\n| kube-system          | kube-dns                  | No node port |\n| kubernetes-dashboard | dashboard-metrics-scraper | No node port |\n| kubernetes-dashboard | kubernetes-dashboard      | No node port |\n|----------------------|---------------------------|--------------|---------------------------|\n</code></pre>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#updating-and-deleting-resources","title":"Updating and Deleting Resources","text":"<p>It's really easy to update resources. All you do is update the yaml file and then rerun the <code>kubectl apply -f=/path/to/config.yaml</code> command.</p> <p>Similarly, deleting is super easy too. Run <code>kubectl delete -f=/path/to/config.yaml</code>.</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#using-a-single-config-file","title":"Using a Single Config File","text":"<p>It's also perfectly acceptable to have all configs in a single file instead of multiple. You'll just need to separate the objects with three dashes <code>---</code>.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend\nspec:\n  selector: \n    app: second-app\n  ports:\n    - protocol: 'TCP'\n      port: 80\n      targetPort: 8080\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: second-app-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: second-app\n      tier: backend\n  template:\n    metadata: \n      labels:\n        app: second-app\n        tier: backend\n    spec: \n      containers:\n        - name: second-node\n          image: stevewhitmore/kub-first-app:2\n</code></pre> <p>It's also considered best practice to define a Service object before a Deployment one because resources are created top to bottom, so the selector in that object will dynamically create the Pods defined thereafter.</p>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#more-on-labels-selectors","title":"More on Labels &amp; Selectors","text":"<p>There's an alternative to <code>matchLabels</code> called <code>matchExpressions</code>. This is a more powerful option that allows for more complex configuration.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: second-app-deployment\nspec:\n  replicas: 1\n  selector:\n    # matchLabels:\n    # app: second-app\n    # tier: backend\n  matchExpressions:\n    - {key: app, operator: In, values: [second-app, first-app]}\n...\n</code></pre> <p>You can also delete resources by selector. Add a label under metadata tag:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: backend\n  labels:\n    group: example  # key and value are arbitrary here\nspec:\n  selector: \n    app: second-app\n  ports:\n    - protocol: 'TCP'\n      port: 80\n      targetPort: 8080\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: second-app-deployment\n  labels:\n    group: example\nspec:\n...\n</code></pre> <p>You use the <code>-l</code> flag for this and specify what resources as an added security measure to avoid accidental deletions.</p> <pre><code>$ kubectl delete deployments,services -l group=example\ndeployment.apps \"second-app-deployment\" deleted\nservice \"backend\" deleted\n</code></pre>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#liveness-probes","title":"Liveness Probes","text":"<p>We can add some configs to adjust the default check on the health of our resources by adding the <code>livenessProbe</code> property under <code>containers</code>.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: second-app-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: second-app\n      tier: backend\n  template:\n    metadata: \n      labels:\n        app: second-app\n        tier: backend\n    spec: \n      containers:\n        - name: second-node\n          image: stevewhitmore/kub-first-app:2\n          livenessProbe:\n            httpGet:\n              path: /\n              port: 8080\n            periodSeconds: 10\n            initialDelaySeconds: 5\n</code></pre>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/kubernetes-crash-course/#imagepullpolicy","title":"ImagePullPolicy","text":"<p>By default the image pull policy is set to <code>IfNotPresent</code> if the image is not explicitly specified. This means that if you have an existing tag but a new <code>latest</code> then it will not pull the newest copy of that tag. You can override this by adding <code>imagePullPolicy: Always</code> under the <code>containers</code> property:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: second-app-deployment\n  labels:\n    group: example\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: second-app\n      tier: backend\n  template:\n    metadata: \n      labels:\n        app: second-app\n        tier: backend\n    spec: \n      containers:\n        - name: second-node\n          image: stevewhitmore/kub-first-app:3\n          imagePullPolicy: Always\n</code></pre>","tags":["Cloud","K8s","AWS"]},{"location":"environments/cloud/aws/","title":"AWS Overview","text":"<p>Because they will have all our base</p>"},{"location":"environments/cloud/aws/What_is_cloud_computing/","title":"What is Cloud Computing?","text":"<p>\"A remote virtual pool of on-demand shared resources offering Compute, Storage, and Network services that can be rapidly deployed at scale\"</p>"},{"location":"environments/cloud/aws/What_is_cloud_computing/#understanding-virtualization","title":"Understanding Virtualization","text":"<p>Allows for multiple VMs in one phsyical server. No crossover- VMs are unaware of eachother.</p> <ul> <li>Hypervisor sits between VMs and physical resources and creates a \"pool\" of them for the VMs to use as needed.</li> <li>Requests to the hardware from the VMs go through the Hypervisor.</li> </ul>"},{"location":"environments/cloud/aws/What_is_cloud_computing/#benefits-of-virtualization","title":"Benefits of Virtualization","text":"<ul> <li>Reduced capital expenditure</li> <li>Less hardware needed</li> <li>Reduced operational costs</li> <li>Less space required</li> <li>Optimization of resources</li> </ul> <p>\"Instance\" is often referring to a specific VM.</p>"},{"location":"environments/cloud/aws/What_is_cloud_computing/#compute","title":"Compute","text":"<ul> <li>The 'brains' to process your workload</li> <li>Virutalized hosts</li> <li>Computational ability to process requests</li> <li>CPU/RAM -- Classic environment comparison: Physical Server</li> </ul>"},{"location":"environments/cloud/aws/What_is_cloud_computing/#storage","title":"Storage","text":"<ul> <li>Acts as non-volitile/secondary storage</li> <li>Save your data across a shared environment</li> <li>Logically attach to instances</li> <li>Separate object for backup/DR -- Classic environment comparision: Server disk storage/NAS (Network Attached Storage)/SAN (Storage Area Network)</li> </ul>"},{"location":"environments/cloud/aws/What_is_cloud_computing/#network","title":"Network","text":"<ul> <li>Provide connectivity for all resources to communicate with each other</li> <li>IP Subnets (Network segmentation)</li> <li>Route Tables</li> <li>Network Access Controll Lists (Security)</li> <li>Network Address Translation -- Classic environment comparision: Routers/Switches/Firewalls</li> </ul>"},{"location":"environments/cloud/aws/cli_intro/","title":"CLI Intro","text":""},{"location":"environments/cloud/aws/cli_intro/#user","title":"User","text":"<p>Create user and go to security credentials for that user. Click Create keys and save CSV file.</p>"},{"location":"environments/cloud/aws/cli_intro/#ec2-instance","title":"EC2 Instance","text":"<p>After creating an EC2 instance you'll be prompted to download a set of keys. It will come as a PEM file.</p> <p>Make sure to update the permissions of this file to 400. Anything else and the AWS CLI will reject your attempt at an ssh connection</p> <p><code>ssh -i ~/Downloads/keys.pem ec2-user@54.185.196.255</code></p> <p>Once logged in type <code>aws configure</code> to set the access ID and secrets along with the default region and output format.</p>"},{"location":"environments/cloud/aws/intro/","title":"AWS Fundamentals","text":"<p>S3 created in 2006. Preceded by Simple Queue Service (SQS) in 2004.</p>"},{"location":"environments/cloud/aws/intro/#4-major-categories","title":"4 Major categories:","text":"<ul> <li>Compute</li> <li>Network and CDN</li> <li>Storage</li> <li>Database</li> </ul> <p>AWS offers storage services that replicates data to 3 different data centers in a region.</p>"},{"location":"environments/cloud/aws/intro/#accessing-aws-services","title":"Accessing AWS services","text":"<ul> <li>AWS Managmenet Console (web interface)</li> <li>Most commonly used</li> <li>AWS CLI</li> <li>APIs</li> <li>All AWS services were designed to be accessed via API</li> <li>AWS SDKs</li> <li>Third party tools</li> <li>S3Fox Organizer: Like a browser-based FTP client. Browser plugin.</li> <li>AWS mobile applications</li> </ul>"},{"location":"environments/cloud/aws/intro/#aws-regions-and-availability-zones","title":"AWS Regions and Availability Zones","text":"<p>One of the most important concepts in AWS.</p> <ul> <li>Independent collections of data centers within a specific geographic area.</li> </ul> <p>GovCloud and China are not automatically accessible for new users.</p>"},{"location":"environments/cloud/aws/intro/#geolocation","title":"Geolocation","text":"<p>May want to choose a region close to users, or close to you if you have to upload a lot of things.</p> <ul> <li>Not all services are available in every region. There's a list under \"Products and Services by Region\"</li> </ul>"},{"location":"environments/cloud/aws/intro/#availability-zone","title":"Availability Zone","text":"<ul> <li>A data center, basically.</li> <li>Every region has at least 2 availability zones (except Beijing, which has 1)</li> </ul>"},{"location":"environments/cloud/aws/intro/#availability-zone-destinations","title":"Availability Zone Destinations","text":"<p>ap-southeast-1a ap-southeast-1b</p> <p>Each AZ is a separate datacenter but they're closely interconnected because:</p> <ul> <li>Low-latency connections</li> <li>Fault tolerant deployments</li> </ul>"},{"location":"environments/cloud/aws/intro/#the-aws-acccount-and-teh-shared-security-responsibility-model","title":"The AWS acccount - and teh shared security responsibility model","text":"<ul> <li>Requires credit card on file but you can request regular invoices from AWS instead of auto-billing.</li> </ul> <p>Team-based accounts</p> <ul> <li>Dev</li> <li>QA</li> <li>Production</li> </ul> <p>AWS consolidated billing.</p>"},{"location":"environments/cloud/aws/intro/#shared-security-responsibility-model","title":"Shared Security Responsibility Model","text":"<p>AWS:</p> <ul> <li>Physical data center security</li> <li>Performs backups, patches</li> </ul> <p>Us:</p> <ul> <li>Access (Security Groups, ACLs, routing policies)</li> </ul> <p>Varies by service</p>"},{"location":"environments/cloud/aws/intro/#using-aws-tools-and-next-steps","title":"Using AWS tools - and next steps","text":"<p>Most corporate intranet environements use N-Tier web application model</p> <p>Can also set up Disaster Recovery models</p> <ul> <li>Raw data available to rebuild (cold site)</li> <li>Partially deployed data (warm site)</li> <li>Fully replicated data available to depoy (hot site)</li> </ul> <p>Business Impact Assessment would be the best way to determine which to go with. Can go with multiple.</p>"},{"location":"environments/cloud/aws/intro/#courses-to-to-next","title":"Courses to to next","text":"<ul> <li>AWS 110: Technical Foundation</li> <li>AWS 120: Compute Fundamentals</li> <li>AWS 140: Storage Fundamentals</li> <li>AWS 160: Networking Fundamentals</li> <li>AWS 180: Database Fundamentals</li> <li>AWS 190: Security Fundamentals</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/","title":"Identity &amp; Access Management","text":"<ul> <li>Identity is for Authentication</li> <li>Access Management is for Authorization (what they can access/do)</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#features-of-iam","title":"Features of IAM","text":"<ul> <li>Users</li> <li>Groups</li> <li>Roles</li> <li>Policy Permissions</li> <li>Access Control Mechanisms</li> </ul> <p>IAM is a Global Service (as opposed to regional). It's the first service a user will interact with when using AWS whether its via browser or api.</p> <p>Without IAM there is no way of maintaining security or control the access to your resources.</p> <p>Provides the components to manage access but is only as strong as you configure it.</p> <p>The responsibility of implementing secure, robust, and tight security within IAM is on us.</p> <p>Settings can be found under Security, Identity &amp; Compliance category</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#users-groups-and-roles","title":"Users, Groups, and Roles","text":""},{"location":"environments/cloud/aws/aws-dev-cert/iam/#users","title":"Users","text":"<p>Objects used to represent an identity - For real people or for automation</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#creating-users","title":"Creating Users","text":"<ul> <li>Can create users via AWS Management Console (web interface) or programmatically with the aws cli, tools for windows powershell, or IAM HTTP API</li> </ul> <p>7 steps 1. Create username 2. Select AWS Access Type      - Programmatic -&gt; Enables an access key ID and secret access key for the AWS API, CLI, SDK, and other dev tools and/or      - AWS Management Console -&gt; Enables a password that allows uers to sign into the web interface 3. Define password  4. Permission assignment     - Attached to Users directly or inherited from a Group that the user can be assigned to. Using groups to assign permissions is considered best practice. 5. Review and confirm info 6. Create the user 7. Download security credentials within a .csv file</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#access-keys","title":"Access Keys","text":"<p>Required for programmatic access for authentication - Access Key ID is made up of 20 random uppercase alphanumeric characters - Secret Access Key ID is made up of 40 random upper/lowercase characters</p> <p>The Secret Access Key will only be visible upon creation and cannot be recoverred since AWS doesn't store a copy. If it's lost a new set of keys will need to be created</p> <p>Keys must be applied and associated with application.  - AWS CLI needs to be instructed to use Access Keys</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#user-summary-page","title":"User Summary Page","text":"<ul> <li>Summary shows User ARN (Amazon Resource Number), Permissions, Groups, Security Credentials management, and Access Advisor</li> </ul> <p>MFA (Multi-factor Authentication) should be a minimum for admins</p> <p>AWS CodeCommit: Self-hosted private git repositories</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#groups","title":"Groups","text":"<p>Objects similar to User objects. - Not used in authentication process - Used to authorize access through AWS Policies</p> <ul> <li>AWS managed polices and customer managed polices are assigned to groups. </li> <li>Groups are usually for specific requirements or job roles. Individual users inherit policies from groups.</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#creating-groups","title":"Creating Groups","text":"<ol> <li>Group Name</li> <li>Attach Policies</li> <li> <p>Review</p> </li> <li> <p>AWS default is max 100 groups. Can be increased by contacting AWS.</p> </li> <li> <p>User can only be associated to 10 groups.</p> </li> </ol>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#roles","title":"Roles","text":"<p>Allow you to adopt a set of temporary IAM permissions</p> <p>Beneficial in that you can assign a predefined Role to a process (like EC2) instead of a fresh set of credentials. This is considered best practice.</p> <p>There are many advantages to IAM Roles: - Don't have access to any keys or credentials associated with them. Credentials are dynamically assigned by AWS.</p> <p>If you had a fleet of EC2 instances you could modify the Role/s associated with them to update all their permissions quickly and easily.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#roles-and-users","title":"Roles and Users","text":"<p>There are circumstances where you need to grant temporary access to a resource for a particular user. In this case you can allow the user to assume a Role temporarily.</p> <p>There are 4 different types of Roles 1. Service Role     - Used by other services to assume the Role needed to perform their tasks     - Examples: EC2, Lambda, etc 2. Service-Linked Role     - Predefined by AWS for services with permissions that cannot be altered      - Examples: Lex - Bots, Lex - Channels 3. Role for Cross-Account Access     - Provide access for account you own and a 3rd party     - \"Trusting\" account has the resources that need to be accessed     - \"Trusted\" account contains users that need to access the resouces in the \"trusting\" account     1. A role is created in the Trusting account     2. A Trust is established with the Role by the AWS Account number of the Trusted account     3. Permissions are applied to the Role via policies     4. The users in the Trusted account have a policy account 4. Role for Identity Provider Access Offers 3 options:     1. Grant access to web identity providers - creates a trust for users using Amazon Cognito, Amazon, Facebook, Google, or some other provider     2. Grant Web Single Sign On to SAML providers - allows access to users coming from a Security Assertion Markup Language (SAML) provider     3. Grant API access to SAML providers - allows access from SAML provider via the AWS CLI, SDKs, or API calls</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#demo","title":"Demo","text":"<p>Create a new Group -&gt; Attach permissions to the Group -&gt; Create a new User -&gt; Assign the user to the Group -&gt; Set up a new Service Role -&gt; Apply Service Role to a new EC2 instance</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#iam-policies","title":"IAM Policies","text":"<p>Formatted as JSON with at least one structure with this structure: <pre><code>{\n    \"Version\": 2012-10-17\"\n    \"Statement\": [\n        \"Sid\": \"Stmt1494509737040\"\n        \"Action\": \"cloudtrail:*\",\n        \"Effect\": \"Allow\",\n        \"Resource\": \"*\",\n        \"Condition\": {\n            \"IpAddress\": {\n                \"aws:SourceIp\": \"10.10.0.0/16\"\n            }\n        }\n    ]\n}\n</code></pre> Version: The policy language version Statement: The main element of the policy which includes:      Sid: The Statement ID is an optional unique identifier within the Statement array     Action: What action will be allowed or denied depending on the value entered for the Effect element - effectively api calls     Effect: Can be set to \"Allow\" or \"Deny\" for previously defined Action. All access to your resources are denied by default.      Resource: This element specifies the actual resource you wish the \"Action\" and \"Effect\" to be applied to. AWS uses ARNs to specify resources following the sytnax - arn:partition:service:region:account-id:resource     Condition: Optional element that allows control over when the permission will be effective</p> <p>There can be multiple Sids per statement, each granding different levels of access</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#iam-policy-types","title":"IAM Policy Types","text":"<p>Two kinds available: - Managed Policies - In-line Policies</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#managed-policies","title":"Managed Policies","text":"<p>These policies can be associated with Groups, Roles, or Users - AWS Managed Polices -- Preconfigured by AWS -- Covers most common permissions - Customer Managed Polices -- Configured by you</p> <p>Creating a Customer Managed Policy - Copy an AWS Managed Policy: Copy any existing AWS Managed Policy and edit it to create a new policy - Policy Generator: Create a poicy by selecting options from dropdown boxes - Create Your Own Policy: Write your own poicies from scratch or paste in a JSON policy from another source</p> <p>It's a good idea to click \"Validate Policy\" when editing/creating policies to make sure syntax is correct</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#inline-policies","title":"Inline Policies","text":"<p>Polices which are directly embedded into a specific User, Group, or Role. Cannot be used by multiple identities. - Created and attached directly to IAM object - Do not show up under Policies list as they are not publicly available for other identities - Typically used when you dont want to run the risk of the permission being used by any other identity</p> <p>What if there are conflicting permissions assigned to the same User? - By default, all access is denied - Access will only be allowed if an explicit \"Allow\" has been specified - A single \"Deny\" will overrule any \"Allow</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#multi-factor-authentication-mfa","title":"Multi-Factor Authentication (MFA)","text":"<p>Utilizes a random 6 digit number generated by an MFA device - No additional charge for MFA - You need your own MVA device (Google Authenticator on phone) - It can be a virtual token</p> <p>Sometimes you can use MFA to increase security when making API calls to other resources.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#identity-federation","title":"Identity Federation","text":"<p>Allows you to access and manage AWS resources even if you don't have a user account within IAM - Identity providers allow users to access AWS resources securely - Can be any OpenID Connect web providers (facebook, Google, Amazon, etc)</p> <p>Benefits:  - Minimizes the amount of administration required within IAM - Allows for a SSO solution</p> <p>There must be a trust relationship between IdP and your AWS account. AWS supports two types of IdP: - OpenID: Allows auth between AWS resources and any public OpenID Connect web providers (facebook, Google, Amazon, etc). IdP credentials are used to exchange an authentication token for temporary authentication credentials - Security Assertion Markup Language (SAML) 2.0 Allows your existing Microsoft Active Directory (MS-AD) users to authenticate to your QWS resources on a SSO approach. Lets the exchange of security data,k including authentication and authorization tokens to take place between a IdP and a service provider</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#active-directory-authentication","title":"Active Directory Authentication","text":"<p>A user within an organization requires API access to S3, EC2, and RDS</p> <p>Security Token Service (STS) allows you to gain temporary security credentials for federated users via IAM</p> <p>Example: 1. The user initiates a request to authenticate against the ADFS Server via a web browser using a SSO URL 2. If their authentication is successful by the AD credentials, SAML issues an assertion back to the users client requesting federated access 3. The SAML assertion is sent to the AWS STS to assume a role within IAM using the AssumeRoleWithSAML API 4. STS response to the user requesting federated access with temporary security credentials with an assumed role and associated permissions 5. The user has federated access to the necessary AWS services</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#creating-an-identity-provider","title":"Creating an Identity Provider","text":"<p>OpenID: - A client ID (audience) that you receive once you register your app with your IdP - A Thumbprint to verify the certificate of your IdP SAML: - A SAML Metadata doc that you get by using the identity management software from your IdP - This document includes information such as the issuer's name, expiration data, and security keys</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#features-of-iam_1","title":"Features of IAM","text":"<ul> <li>Can set specific guidelines for password rules (min length, what chars are allowed/required, prevent reuse, etc) </li> <li>Can deactivate Security Token Service Regions (like Asia) for added security</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#credential-report","title":"Credential Report","text":"<p>Can generate downloadable CSV with all IAM users and credentials.  - Will only be generated once in 4 hours</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#key-management-service-kms","title":"Key Management Service (KMS)","text":"<p>Enables you to easily manage encryption keys to secure data - You can control how the keys can be used to encrypt your data - If you lose or delete your keys they cannot be recovered - You can manage your KMS Custom Master Keys (CMK) from within the IAM console</p>"},{"location":"environments/cloud/aws/aws-dev-cert/iam/#implementing-cross-account-access-using-iam","title":"Implementing Cross-Account Access Using IAM","text":"<p>If a team only has access to Dev environment but needs to ocassionally access Prod environment they can temporarily assume a role. As this access is not always required, the team must consciously switch to and asusme the role to acces the other resources. - It's possible to add MFA to the role</p> <ol> <li>Create a new role from within the trusting account (Prod)</li> <li>Specify the permissions attached to this newly created role which the users in the dev account would assume to carry out their required tasks</li> <li>You must switch to the trusted account (Dev) to grant permissions to your team to allow them to assume the newly created role in the trusting account (Prod)</li> <li>Test the configs by switching to the role</li> </ol>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/","title":"Compute Fundamentals For AWS","text":"<p>Topics:</p> <ul> <li>Amazon Elastic Compute Cloud (EC2)</li> <li>Amazon EC2 Container Service</li> <li>Amazon Elastic Container Registry</li> <li>Amazon Elastic Container Service for Kubernetes (EKS)</li> <li>AWS Elastic Beanstalk</li> <li>AWS Lambda</li> <li>AWS Batch</li> <li>Amazon Lightsail</li> </ul> <p>\"Compute resources can be considered the brains and processing power required by applications and systems to carry out computational tasks via a series of instructions\"</p> <p>Closely related to common server components such as CPUs and RAM. A physical server within a data center is a Compute resource.</p> <p>Compute services can:</p> <ul> <li>comprise of utilizing hundreds of EC2 instances (virtual servers) which may be used continuously for  months or years processing millions of instructions.</li> <li>utilize a few hundred milliseconds of compute resource to execute just a few lines of code within AWS Lambda before relinquishing that compute power.</li> </ul> <p>Can be comsumed in different quantities for diffrerent liengs of time actross a range of categories offering a wide scope of performance and benefit options</p> <p>https://aws.amazon.com/products/compute for examples on how some resources could be used</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#elastic-compute-cloud-ec2","title":"Elastic Compute Cloud (EC2)","text":"<p>Allows you to deploy virtual servers within your AWS environment. Most people will require an EC2 instance wthin their environment as part of at least one of their solutions</p> <p>Can be broken down into the following components:</p> <ul> <li>Amazon Machine Images (AMIs)</li> <li>Instance Types</li> <li>Instance Purchasing Options</li> <li>Tenancy</li> <li>User Data</li> <li>Storage options</li> <li>Security</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#amazon-machine-images-ami","title":"Amazon Machine Images (AMI)","text":"<p>Templates of pre-configured EC2 instances which allow you to quickly launch a new EC2 instance based on the configuration within the AMI</p> <p>An AMI is an image baseline with an operating system and applications along with any custom configs.</p> <p>You can also create your own AMIs to help you speed up your own deployments</p> <p>AWS Marketplace: You can also purchase AMIs from AWS Marketplace which is an online store of AMIs from trusted vendors. May be good since they could be configured to be optimized for certain security setups or for data migration.</p> <p>Community AMIs: An independantly developed AMI made public (shared).</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#instance-types","title":"Instance Types","text":"<p>Once you have selected AMI from a source an instance type must be selected.</p> <ul> <li>Defines the instance size based on a number of different parameters, ECUs - EC2 Compute Units for the instnace, vCPUs: virtual CPUs</li> </ul> <p>Generally we just pay attention to:</p> <ul> <li>vCPUs</li> <li>Memory</li> <li>Instance Storage</li> <li>Network Performance</li> </ul> <p>There are preconfigured types for specific purposes:</p> <ul> <li>Micro instances</li> <li>General purpose</li> <li>Compute optimized</li> <li>Memory optimized (ideal for apps that manage real-time unstructured data processing or distributed web cache stores)</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#instance-purchasing-options","title":"Instance Purchasing Options","text":"<ul> <li>On-Demand Instances</li> <li>Reserved Instances</li> <li>Scheduled Instances</li> <li>Spot Instances</li> <li>On-Demand Capacity Reservations</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#on-demand-instances","title":"On-Demand Instances","text":"<ul> <li>Can be launched at any time</li> <li>Can be used for as long as needed</li> <li>Flat rate determined on the instance type - paid by the second</li> <li>Typically used for short-term uses</li> <li>Best fit for testing and dev environments</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#reserved-instances","title":"Reserved Instances","text":"<p>Purchases for a set period of time for reduced cost</p> <ul> <li>All Upfront: Complete payment for 1 or 3 year time frame</li> <li>Partial Upfront: Smaller upfront payment for smaller discount</li> <li>No Upfront: smallest discount is applied</li> </ul> <p>Best for long term, predictable workloads</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#scheduled-instances","title":"Scheduled Instances","text":"<ul> <li>You pay for the reservations on a recurring schedule, either daily, weekly, or monthly</li> <li>You could set up a scheduled instance to run durning a set time frame once a week</li> <li>Not that even if you didn't use the instance, you will be charged</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#spot-instances","title":"Spot Instances","text":"<ul> <li>Bid for a unused EC2 Compute resource</li> <li>No guarantees for a fixed period of time</li> <li>Fluctuation of prices based on supply and demand</li> <li>Can purchase large EC2 instances at a very low price</li> <li>Useful for processing data that can be suddenly interrupted (batch jobs, background processing of data)</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#on-demand-capacity-reservations","title":"On-Demand Capacity Reservations","text":"<ul> <li>Reserve capacity based on different attributes such as instance type, platform and tenancy, within a particular availability zone for any period of time</li> <li>It could be used in conjunction with your reserved instances discount</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#ec2-tenancy","title":"EC2 Tenancy","text":"<p>Relates to what underlying host your EC2 instance will reside on, so essentially the physical server within an AWS Data Center</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#shared-tenancy","title":"Shared Tenancy","text":"<ul> <li>EC2 instance is launched on any available host with the required resources</li> <li>The same host may be used by multiple customers</li> <li>AWS Security mechanisms prevent one EC2 instance accessing another in the same host</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#dedicated-instances","title":"Dedicated Instances","text":"<ul> <li>Hosted on hardware that no other customer can access</li> <li>May be required to meet compliance</li> <li>Dedicated instances incur additional chrages</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#dedicated-hosts","title":"Dedicated Hosts","text":"<ul> <li>Additional visibility and control on the physical host</li> <li>Allows you to use existing licenses (per-VM, Windows server, etc)</li> <li>Allows you to use the same host for a number of instances</li> <li>May be required to meet compliance</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#user-data","title":"User Data","text":"<p>Allows you to enter commands that will run during the first boot cycle of that instance</p> <ul> <li>Perform functions upon boot such as to pull down any additional software you want</li> <li>Download latest OS updates (yum update -y)</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#storage-options","title":"Storage Options","text":"<p>Selecting storage for your EC2 instance will depend on the instance selected, what you intend to use the instance for, and how critical the data is.</p> <p>Two types of storage:</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#persistent-storage","title":"Persistent Storage","text":"<ul> <li>Available by attaching Elastic Beanstalk Storage (EBS) volumes</li> <li>EBS volumes are separated from the EC2 instances (not physically attached)</li> <li>These volumes are logically attached via AWS network. This is similar to attaching an external drive to a personal computer.</li> <li>You can disconnect the volume from the EC2 instance and the data remains on the volume</li> <li>You can implement encryption on these volumes and take backup snapshots of all data on the volumes</li> </ul> <p>The data on EBS volumes are automatically replicated to other EBS volumes within the same availibilty zone for resiliency which is managed by AWS</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#ephemeral-storage","title":"Ephemeral Storage","text":"<ul> <li>Created by EC2 instances using local storage</li> <li>Physically attached to the underlying host (like built in storage on a laptop)</li> <li>When the instance is stopped or terminated, all saved data on disk is lost</li> <li>However, if you reboot your data will remain intact</li> <li>Unlike EBS volumes, you're unable to detach instance store volumes from the instance</li> </ul> <p>AWS Instance Store Volumes are hosted by EC2 instances themselves</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#security","title":"Security","text":"<p>During the creation of your EC2 instance you will be asked to select a Security Group for your instance</p> <ul> <li>A Security Group is essentially an instance-level firewall</li> <li>Allows you to control traffic</li> <li>At the end of your EC2 instance creation you will need to select an existing Key Pair or create and download a new one</li> </ul> <p>A Key pair is made up of a Public Key and a Private Key.</p> <p>The function is to encrypt the login information for Linux and Windows EC2 instances, and then decrypt the same information allowing you to authenticate onto an instance</p> <ul> <li>Public Key is held and kept by AWS</li> <li>Private key is the users responsibility</li> <li>It is possible to use the same Key Pair on multiple instances (be wary of this)</li> <li>You can set up additional, less privileged access controls (e.g. local Windows accounts)</li> </ul> <p>It is your responsibility to maintain and install the latest OS and security patches released by the OS vendor as dictated within the AWS shared responsibility model</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#ec2-container-service-ecs","title":"EC2 Container Service (ECS)","text":"<p>Allows you to run Docker-enabled applications packaged as containers across a cluster of EC2 instances without requiring you to manage a complex and administratively heavy cluster management system.</p> <p>The burden of managing your own cluster management system is abstracted with the ECS service by passing that responsibility over to AWS, specifically through the use of AWS Fargate.</p> <p>AWS Fargate is an engine used to enable ECS to run Containers without having to manage and provision instances and clusters for Containers</p> <p>Docker is a piece of softare that allows you to automate the installation and distribution of applications inside Linux Containers</p> <p>A Container holds everything an application needs to run from within its container package. They're decoupled from the OS, making Container apps very portable</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#launching-an-ecs-cluster","title":"Launching an ECS Cluster","text":"<p>Two options:</p> <ul> <li> <p>Fargate Launch Far less configuration - requires you to specify the CPU and memory required, define networking and IAM policies, in addition to you having to package your application into containers</p> </li> <li> <p>EC2 Launch Responsible for patching and scaling your instances and you can specify instance type and how many containers should be in a cluster</p> </li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#monitoring-containers","title":"Monitoring containers","text":"<p>Can be taken care of through Amazon CloudWatch. You can easily create alarms based off of these metrics, providing you notifications when specific events occur, such as your cluster size scaling up or down</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#amazon-ecs-cluster","title":"Amazon ECS Cluster","text":"<p>Comprised of a collection of EC2 instances</p> <p>Features such as Security Groups, Elastic Load Balancing, and Auto Scaling can be used with these instances</p> <p>These instances still operate in much the same way as a single EC2 instance</p> <ul> <li>Clusters act as a resource pool, aggregating resources such as CPU and memory</li> <li>Clusters are dynamically scalable and multiple instances can be used</li> <li>Clusters can only scale in a single region</li> <li>Containers can be scheduled to be deployed across your cluster</li> <li>Instances within the cluster also have a Docker daemon and an ECS agent</li> </ul> <p>ECS agents communicate with each other allowing ECS commands to be translated into Docker commands</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#elastic-container-registry-ecr","title":"Elastic Container Registry (ECR)","text":"<p>Provides a secure location to store and manage your docker images.</p> <p>This is a fully managed service so you dont need to provision any infra to allow you to create this registry of docker images.</p> <p>This service allows developers to push, pull, and manage their library of docker images in a central and secure location.</p> <p>Components used in ECR:</p> <ul> <li>Registry</li> <li>Auth Token</li> <li>Repo</li> <li>Repo Policy</li> <li>Image</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#registry","title":"Registry","text":"<p>Allows you to host and store docker images as well as create image repos.</p> <p>Your account will have both read and write access by default to any images you create within the registry and any repos.</p> <p>Access for your registry and images can be controlled via IAM policies in addition to repository policies.</p> <p>Before your docker client can access your registry it needs to be authenticated as an AWS user via an Authorization token.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#authorization-token","title":"Authorization Token","text":"<p>To begin the authorization process to communicate your docker client with your default registry you can run the get-login command using the AWS CLI</p> <p><code>aws ecr get-login --region {region} --no-include-email</code></p> <p>This will produce an output response which will be a docker login command</p> <pre><code>docker login -u AWS -p {password}\nhttps://{aws_account_id}.dkr.ecr.{region}.amazonaws.com\n</code></pre> <p>This process produces an authorization token that can be used within the registry for 12 hours</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#repository","title":"Repository","text":"<p>These are objects within your registry that allow you to group together and secure different docker images.</p> <p>You can create multiple repositories with the registry allowing you to organize and manage your docker images into different categories.</p> <p>Using policies from both IAM and repository policies you can assign set permissions to each repository.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#repository-policy","title":"Repository Policy","text":"<p>You can control access to your Repositories through both IAM policies and Repository Policies.</p> <p>There a number of different IAM managed policies to help you control access to ECR: <code>AmazonEC2ContainerRegistryFullAccess</code> <code>AmazonEC2ContainerRegistryPowerUser</code> <code>AmazonEC2ContainerRegistryReadOnly</code></p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#repository-policies-are-resource-based-policies","title":"Repository policies are resource-based policies","text":"<ul> <li>You need to ensure you add a principal to the policy to determine who has access and what permissions they have</li> <li>For an AWS user to gain access to the registry they will require access to the ecr:GetAuthorizationToken API call</li> <li>Once they have this access repository policies can control what actions those users can perform on each of the repositories</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#image","title":"Image","text":"<p>Once you have configured your registry, repositories and security controls, and authenticated your docker client with ECR, you can then begin storing your docker images in the required repositories</p> <p>To push and publish image into ECR, you can use the <code>docker push</code> command, and to retrieve an image you can use the docker pull command</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#elastic-container-service-for-kubernetes-eks","title":"Elastic Container Service for Kubernetes (EKS)","text":"<p>Kubernetes is an open-source container orchestration tool designed to automate, deploy, scale, and operate containerized applications</p> <p>It can grow from tens, thousands, or even millions of containers</p> <p>It is container-runtime agnostic which means you can use K8s to run Rocket and Docker images</p> <p>AWS provides a managed service allowing you to run Kubernetes across your AWS Infrastructure without having to take care of provisioning and running the Kubernetes management infrastructure in what's referred to as the control plane</p> <p>You only need to provision and maintain the worker nodes</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#k8s-control-plane","title":"K8s Control Plane","text":"<p>There are a number of different components that make up the control plane and these include a number of differnet APIs, the kubelet processes and the K8s Master</p> <p>The control plane schedules containers onto nodes</p> <p>The control plane also tracks the state of all K8s objects by continually monitoring the objects</p> <p>In EKS, AWS is responsible for provisioning, scaling and managing the control plane, and they do this by utilizing multiple availability zones for additional resilience</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#worker-nodes","title":"Worker Nodes","text":""},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#k8s-clusters-are-composed-of-nodes","title":"K8s clusters are composed of nodes","text":"<p>A node is a worker machine in K8s. It runs as an on-demand EC2 instance and includes software to run containers</p> <p>For each node created, a specific AMI is used, which also ensures Docker and kubelet in addition to the AWS IAM authenticator</p> <p>Once the worker nodes are provisioned they can then connect to EKS using an endpoint</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#working-with-eks","title":"Working with EKS","text":"<ol> <li>Create an EKS service role Before you begin working with EKS you need to configure and create an IAM service role that allows EKS to provision and configure specific resources</li> </ol> <p>The role needs to have the following permissions policies attached to the role:</p> <ul> <li>AmazonEKSServicePolicy</li> <li> <p>AmazonEKSClusterPolicy</p> </li> <li> <p>Create an EKS Cluster VPC Create and run a CLoudFormation stack based on teh template below which will configure a new VPC for you to use with EKS</p> </li> <li> <p>Install kubectl and the AWS-IAM-Authenticator Kubectl is a command line utility for K8s and can be installed following the details supplied here.</p> </li> </ul> <p>The IAM-Authenticator is required to authenticate with the EKS cluster</p> <ol> <li> <p>Create your EKS Cluster You can now create your EKS cluster using the details and info from the VPC created in step 1 and 2</p> </li> <li> <p>Configure kubectl for EKS Using the <code>update-kubeconfig</code> command via the AWS CLI you need to create a kubeconfig file for your EKS cluster</p> </li> <li> <p>Provision and configure Worker Nodes Once your EKS cluster shows an 'Active' status you can launch your worker nodes using CloudFormation based on the following template</p> </li> <li> <p>Configure the Worker Node to join the EKS Cluster Using a configuration map downloaded here: <code>curl -O https://amazon-eks.s3-us-west-2.amazonaws.com/cloudformation/2019-02-11/aws-auth-cm.yaml</code></p> </li> </ol> <p>You must edit it wand replace the  with the NodeInstanceRole value from step 6"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#aws-elastic-beanstalk","title":"AWS Elastic Beanstalk","text":"<p>Is a managed service that takes your uploaded code of your web app code and automatically provisions and deploys the required resources within AWS to make the web application operational</p> <p>These resources include EC2, Auto Scaling, application health-monitoring and Elastic Load Balancing, in addition to capacity provisioning</p> <p>An ideal service for engineers who may not have the familiarity or the necessary skills within AWS to deploy, provision, monitor, and scale the correct environment to run the developed applications</p> <p>The responsibility is passed on to AWS Elastic Beanstalk to deploy the correct infrastructure to run the uploaded code</p> <p>You can continue to support and maintain the environment as you would with a custom built environment</p> <p>You can perform some maintenance tasks from the Elastic Beanstalk dashboard itself</p> <p>It's able to operate with a variety of platforms and programming languages:</p> <ul> <li>Packer Builder</li> <li>Single Container Docker</li> <li>Multicontainer Docker</li> <li>Preconfigured Docker</li> <li>Go</li> <li>Java SE</li> <li>Java with Tomcat.NET on Windows Server with IIS</li> <li>Node.js</li> <li>PHP</li> <li>Python</li> <li>Ruby</li> </ul> <p>The service itself is free to use</p> <p>There is no cost associated with Elastic Beanstalk. However, for any resources that are created on your application's behalf (such as EC2 instances), you will be charged for as per the standard pricing policies at the time of deployment.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#elastic-beanstalk-core-components","title":"Elastic Beanstalk Core Components","text":""},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#application-version","title":"Application Version","text":"<p>An application version is a very specific reference to a section of deployable code</p> <p>The application version will point typically to S3, simple storage service to where the deployable code may reside</p> <p>With Elastic Beanstalk, applications can have many versions and each application version is unique</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#environment","title":"Environment","text":"<p>An environment refers to an application version that has been deployed on AWS resources which are configured and provisioned by AWS Elastic Beanstalk</p> <p>At this stage, the application is deployed as a solution and becomes operational within your environment</p> <p>The environment is comprised of ALL the resources created by Elastic Beanstalk and not just an EC2 instance with your uploaded code</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#environment-configurations","title":"Environment Configurations","text":"<p>This is a collection of parameters and settings that dictate how an environment will have its resources provisioned by Elastic Beanstalk and how these resources will behave</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#environment-tier","title":"Environment Tier","text":"<p>Reflects on how Elastic Beanstalk provisions resources based on what the application is designed to do</p> <p>If the application manages and handles HTTP requests then the app will be run in a web server environment</p> <p>If the application does not process HTTP requests and instead perhaps pulls data from an Simple Queue Service (SQS) queue then it would run in a worker environment</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#configuration-template","title":"Configuration Template","text":"<p>The template that provides the baseline for creating a new, unique, environment configuration</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#platform","title":"Platform","text":"<p>A columination of components in which you can build your application upon using Elastic Beanstalk</p> <p>These comprise of the OS of the instance, the programming language, the server type (web or application) and components of Elastic Beanstalk itself, and as a whole can be defined as a platform</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#applications","title":"Applications","text":"<p>An application is a collection of different elements, such as environments, environment configuration, and application versions</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#web-server-environment","title":"Web Server Environment","text":"<p>This is typically used for standard web apps that operate and serve requests over HTTP port 80</p> <ul> <li>Route 53</li> <li>Elastic Load Balancer</li> <li>Auto Scaling</li> <li>EC2</li> <li>Security Groups</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#worker-environment","title":"Worker Environment","text":"<p>It is used by applications that will have a back-end proccessing task, that will interact with AWS SQS</p> <ul> <li>SQS Queue</li> <li>IAM Service Role</li> <li>Auto Scaling</li> <li>EC2</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#elastic-beanstalk-workflow","title":"Elastic Beanstalk Workflow","text":"<p>AWS Elastic Beanstalk operates a very simple workflow process for your application deployment and ongoing management</p> <p>Create Application -&gt; Upload Version -&gt; Launch Environment -&gt; Manage Environment</p> <p>If the management of your applications have altered the environment configuration then your environemnt will automatically be updated to reflect the new code should additional resoruces be required</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#aws-lambda","title":"AWS Lambda","text":"<p>A serverless compute service that allows you to run your app code without having to manage EC2 instances.</p> <p>Serverless means that you do not need to worry about provisioning and managing your own computer resource to run your own code, instead this is managed and provisioned by AWS.</p> <p>The service does require compute power to carry out your code requests, but because the AWS user does not need to be concerned with managing this compute power or where its provisioned from, it's considered 'serverless' from the user perspective.</p> <p>You only ever have to pay for compute power when Lambda is in use via Lambda Functions.</p> <p>AWS Lambda charges computer power per 100ms of use only when your code is running, in addition to the number of tiers your code runs.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#working-with-aws-lambda","title":"Working with AWS Lambda","text":"<p>There are essentially 4 steps to its operations:</p> <ol> <li>You can either upload your code to Lambda, or write it within the code editors that Lambda provides. Supported languages: Node.js, Java, C#, Python, Go, PowerShell, and Ruby.</li> <li>Configure your Lambda functions to execute upon specific triggers from supported event sources.</li> <li>Once the specific trigger is initiated, Lambda will run your code (as per your Lambda function) using only the required compute power as defined.</li> <li>AWS recoreds the compute time in Milliseconds and the quantity of Lambda functions run to ascertain the cost of the service</li> </ol>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#components-of-aws-lambda","title":"Components of AWS Lambda","text":"<p>Lambda Function is compiled of your own code that you want Lambda to invoke as per defined triggers.</p> <p>Event sources are AWS services that Lambda monitors for specific operations that can trigger your Lambda functions.</p> <p>Trigger is essentially an operation from an event source that causes the function to invoke (like a PUT request).</p> <p>Downstream Resources are resources that are required during the execution of your Lambda Function.</p> <p>Log streams help to identify issues and troubleshoot issues with your Lambda function. These log streams would essentially be a sequence of events that all come from the same function and recorded in CloudWatch.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#creating-lambda-functions","title":"Creating Lambda Functions","text":"<p>At a high level the configuration steps for creating a Lambda function via the AWS Management Console could consist of:</p> <ol> <li> <p>Selecting a Blueprint Select a blueprint template provided by AWS Lambda Ex: S3-get-object - an S3 trigger that retrieves metadata</p> </li> <li> <p>Configure Triggers Define the trigger for your Labmda function Ex: specifying the S3 bucket for your function</p> </li> <li> <p>Configure Functions Upload code or edit it in-line Define the required resources, maximum execution timeout, IAM Role and Handler Name</p> </li> </ol> <p>AWS Lambda is a highly scalable serverless service, coupled with fantastic cost optimization compared to EC2 as you are only charged for Computer power while the code is running and for the number of functions called.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#aws-batch","title":"AWS Batch","text":"<p>Used to manage and run batch computing workloads within AWS</p> <p>Batch Computing is primarily used in specialist use cases which require a vast amount of compute power across a cluster of compute resources to complete batch processing, executing a series of tasks. This can be very costly and difficult to manage.</p> <p>With AWS Batch many constraints, administration activities, and maintenance tasks are removed. The process is a lot more streamlined by taking advantage of the elasticity of AWS coping with any level of batch processing while optimizing the distribution of the workloads. Everything's taken care of by AWS.</p> <p>Perfect if you have a requirement to run multiple jobs in parallel using Batch computing such as to analyze financial risk modles, perform media transcoding, or engineering simulations.</p> <p>There are 5 components that make up AWS Batch Service:</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#jobs","title":"Jobs","text":"<p>A Job is classed as a unit of work that is to be run by AWS Batch.</p> <ul> <li>Can be an executable file, an app within an ECS Cluster, or a shell script.</li> <li>Run on EC2 instances as a containerized application</li> <li>Can have different states such as Submitted, Pending, Running, Failed, etc</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#job-definitions","title":"Job Definitions","text":"<p>These define specific parameters for the Jobs themselves and dictate how the Job will run and with what configuration.</p> <p>Some examples may be:</p> <ul> <li>How many vCPUs to use for the container</li> <li>Which data volumes hsould be used</li> <li>Which IAM role should be used to allow access for AWS Batch to communicate with other AWS services</li> <li>Mount points</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#job-queues","title":"Job Queues","text":"<p>Jobs that are scheduled are placed into a Job Queue until they run.</p> <p>You can have multiple queues with different priorities.</p> <p>On Demand and Spot instances are supported.</p> <p>AWS Batch can bid on your behalf for Spot instances.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#job-scheduling","title":"Job Scheduling","text":"<p>Takes care of when a Job should be run and from which Compute Environment</p> <ul> <li>Typically will operate on a FIFO basis</li> <li>Ensures that higher priority queues are run first</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#compute-environments","title":"Compute Environments","text":"<p>Environments containing the compute resources to carry out the Job.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#managed-environments","title":"Managed Environments","text":"<ul> <li>The service will handle provisioning, scaling, and termination of Compute instances</li> <li>This environment is created as an Amazon ECS cluster</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#unmanaged-environments","title":"Unmanaged Environments","text":"<ul> <li>These environments are povisioned, managed, and maintained by you</li> <li>Gives greater customization but requires greater administration and maintenance</li> <li>Requires you to create the necessary ECS Cluster</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#amazon-lightsail","title":"Amazon Lightsail","text":"<p>Is essetially a Virtual Private Server (VPS) backed by AWS infrastructure, much like an EC2 instance but without as many configurable steps throughout its creation.</p> <p>It has been designed to be simple, quick, and very easy to use at a low cost point for small scale use cases by small businesses or for single users</p> <ul> <li>It's commmonly used to host simple websites, small applications, and blogs</li> <li>You can run multiple Lightsail instances together allowing them to communicate</li> <li>It's possible to connect it to other AWS resources and to your existing VPC running within AWS via a peering connection</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/compute/compute_fundamentals/#deploying-a-lightsail-instance","title":"Deploying a Lightsail instance","text":"<p>Can be deployed from a single page with just a few configuration options.</p> <p>Can be accessed either via the AWS console under the Compute category or directly from the homepage of AWS Lightsail which sits outside of the AWS Management Console</p> <p>Instance is charged on-demand only so you're only charged for when you're actually using the resource</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/","title":"Database Fundamentals for AWS","text":"<p>There's good info in here but it's slightly dated. See notes in this directory pertaining to the specific services</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#what-is-a-cloud-database","title":"What is a Cloud Database?","text":"<p>RDS - Availability and Durability</p> <ul> <li>Automated backups</li> <li>Database snapshots</li> <li>Multi-AZ deployments</li> <li>Automatic host replacemnt</li> <li>No set up fee</li> <li>no hardware required</li> <li>pay only for what we use</li> <li>easy to scale up and down</li> </ul> <p>Each region includes distinct facilities located in different areas with the region called Availability Zones (AZ).</p> <p>AZ's are distinct geographical locations that are engineered to b insulated from failures in any other AZ.</p> <p>Loctated on separate electrical grids, flood plains, risk profiles.</p> <p>By having Amazon RDS instances in more than one AZ a database can be protected from failure at a single region.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#aws-shared-responsibility-model","title":"AWS Shared Responsibility Model","text":""},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#customer-responsibility","title":"Customer Responsibility","text":"<p>Customer manage security in the cloud</p> <ul> <li>Customer Data</li> <li>Platform, Applications, Identity &amp; Access Management</li> <li>Operating System, Network &amp; Firewall Configuration</li> <li>Client Side Data Encryption &amp; Data Integrity Authentication</li> <li>Server Side Encryption (File System and/or Data)</li> <li>Network Traffic Protection (Encryption/Integrity/Identity)</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#aws-responsibility","title":"AWS Responsibility","text":"<p>AWS Manages security of the cloud</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#aws-foundation-services","title":"AWS Foundation Services","text":"<ul> <li>Compute</li> <li>Storage</li> <li>Database</li> <li>Network</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#aws-global-infrastructure","title":"AWS Global Infrastructure","text":"<ul> <li>Region</li> <li>Availability Zones</li> <li>Edge Locations</li> </ul> <p>Security has two aspects: Data at rest and data in transit. AWS provides encryption for data in transit. Data at rest can be encrypted through a number of AWS services</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#overview-of-the-aws-database-services","title":"Overview of the AWS Database Services","text":"<p>Split up into Relational and Non-relational Databases</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#relational-databases","title":"Relational Databases","text":""},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#amazon-relational-database-service-rds","title":"Amazon relational database service (RDS)","text":"<p>Takes care of backups and patches</p> <p>Uses storage autoscaling by utilizing Elastic Block Storage (EBS) with the exception of Amazon Aurora which uses Shared Cluster Storage architecture.</p> <p>Structured tables &lt;- generally support -&gt; Structured Query Language (SQL) operations</p> <p>A database view is basically a saved select statement. Tables are linked together via joining on common fields and they generate a temporary view. This processing is done by the RDBMS script engine which means a relational database is generally quite complex and requires a lot of computing resources. This means generally the software footprint for relational databases are bigger than for non-relational.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#storage-types","title":"Storage types","text":""},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#general-purpose-ssd-storage","title":"General Purpose SSD Storage","text":"<ul> <li>Good option for broad range of use cases</li> <li>Provides single-digit millisecond latencies</li> <li>Cost effective</li> <li>SSD storage for primary data<ul> <li>Minimum 20GiB</li> <li>Maximum 64TiB (SQL Server 16 TiB)</li> </ul> </li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#provisioned-iops-ssd-storage","title":"Provisioned IOPS (SSD) Storage","text":"<ul> <li>Good for workloads that operate at a very high I/O</li> <li>IOPS: Minimum 8,800, max 80,000 (SQL Server 40,000)</li> <li>Storage for primary data: Minimum 100GiB, max 64TiB (SQL Server 16TiB)</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#magnetic-storage","title":"Magnetic Storage","text":"<p>Provided for backwards compatibility. AWS recommends General Purpose instead.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#shared-cluster-storage","title":"Shared Cluster Storage","text":"<ul> <li>Aurora uses this</li> <li>The option to configure and select storage options does not exist</li> <li>Your storage will scale automatically as your database grows</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#database-engines","title":"Database engines","text":"<ul> <li>MySQL</li> <li>MariaDB (Community developed sql</li> <li>PostreSQL</li> <li>Amazon Aurora (AWS fork of mysql)</li> <li>Oracle (commonly used in corporate because they enjoy pain)</li> <li>SQL Server (Microsoft)</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#non-relational-databases","title":"Non Relational Databases","text":"<ul> <li>No table schema required</li> <li>Can support non structured data</li> <li>Focus on providing a fast, secure data store</li> <li>Generally lack a processing engine so are lighter in design</li> <li>Do not require a schema</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#communication","title":"Communication","text":""},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#relational","title":"Relational","text":"<p>Persisted Network Connection: Computer -&gt; SQL statement -&gt; Relational Database Relational Database -&gt; Response from Database -&gt; Computer</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#non-relational","title":"Non Relational","text":"<p>Computer -&gt; Request via RESTful API call -&gt; Amazon DynamoDB Amazon DynamoDB -&gt; HTTP response -&gt; Computer</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#summary-of-differencesuse-cases","title":"Summary of differences/use cases","text":""},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#relational_1","title":"Relational","text":"<ul> <li>RDBMS/ACID Engine</li> <li>Supports complex relationships between tables</li> <li>Uses the Structured Query Language</li> <li>Generally accessed vai a persistent network connection (ODBC)</li> <li>Uses a schema to define tables</li> <li>Generally provides a processing engine within the database to manage procssing of select, create, replace, update, delete statements</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#non-relational_1","title":"Non Relational","text":"<ul> <li>Simple document or key store</li> <li>Can store many different data types</li> <li>Generally accessed via RESTful HTTP</li> <li>No schema required</li> <li>Every table must have a primary key</li> <li>Scales fast</li> <li>Lighter by design</li> </ul> <p>AWS Non Relational Databases are DynamoDB and Amazon Elasticache.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#amazon-non-relational-databases-in-depth","title":"Amazon Non Relational Databases In Depth","text":""},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#dynamodb","title":"DynamoDB","text":"<ul> <li>A cloud native database for managing high volumes of records and transactions without the need for provisioning capacity up front.</li> <li>A fully managed service</li> <li>Supports both document and key store object</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#amazon-elasticache","title":"Amazon Elasticache","text":"<p>Runs on 2 different database engines -</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#amazon-elasticache-for-redis","title":"Amazon Elasticache for Redis","text":"<p>Use if</p> <ul> <li>You need complex data types, such as strings, hashes, lists, sets, sorted sets, and bitmaps</li> <li>You need persistence of your key store</li> <li>You need to encryptyour cache data (e.g. to maintain compliance)</li> <li>You need to replicate your cached data</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#amazon-elasticache-for-memcached","title":"Amazon Elasticache for Memcached","text":"<p>Use if</p> <ul> <li>You need the simplest model possible</li> <li>You need to run large nodes with multiple cores or threads</li> <li>You need the ability to scale out/in, adding and removing nodes as demand on your system increases and decreases</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#aws-relational-databases-in-depth","title":"AWS Relational Databases in Depth","text":"<p>What are the benefits of using Amazon RDS?</p> <ul> <li>The ability to scale components</li> <li>Automatic backups and patching</li> <li>High availability - can run in parallel over several availability zones</li> <li>Automatic failure detection and recovery</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/database_fundamentals/#amazon-rds-for-mysql","title":"Amazon RDS for MySQL","text":"<ul> <li>Micro instances - often assocated with free tier so useful for non-prod or pilot projects</li> <li>General purpose - latest gen and current gen like the M series</li> <li>Memory optimized - the latest and current gen R series instances</li> <li>Burst support - latest and current T series instances</li> <li>General purpose storage provides cost-effective storage for small or medium sized workloads</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/documentdb/","title":"Amazon DocumentDB (With MongoDB Compatibility)","text":""},{"location":"environments/cloud/aws/aws-dev-cert/database/dynamodb/","title":"Amazon DynamoDB","text":"<p>NoSQL Database Key-value store</p> <p>You can look up data: - Using a primary key for each item - Through the use of indexes</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/dynamodb/#use-cases","title":"Use cases","text":"<ul> <li>For ultra high performance</li> <li>Used for gaming web mobile and IoT </li> <li>Fully managed service - aws handles everything. Very easy to set up</li> </ul> <p>Set tables up, config provisioned throughput and that's it</p> <p>DynamoDB tables are considered schemaless - As long as the table has the appropriate primary key the item can contain varying sets of attributes - The record doesn't need to have the same attributes or the same number of attributes - It doesn't care about type for individual attributes either</p> <p>As a general rule, you should maintain as few tables as possible in a DynamoDB application. Most well designed applications require only one table, unless there is a specific reason for using multiple tables.</p> <p>Exceptions are cases where high-volume time series data are involved, or datasets that have very different access patterns\u2014but these are exceptions. A single table with inverted indexes can usually enable simple queries to create and retrieve the complex hierarchical data structures required by your application.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/dynamodb/#indexes","title":"Indexes","text":"<p>1 query 1 index</p> <p>You can add secondary indexes to search by different attributes</p> <p>If you want to query and match on two different columns you need to create an index that can do that properly When you write your qureies you need to specify exactly which index should be used for each query</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/dynamodb/#secondary-indexes","title":"Secondary indexes","text":""},{"location":"environments/cloud/aws/aws-dev-cert/database/dynamodb/#global","title":"global","text":"<p>lets you query across the entire table to find any record that matches a particular vlue</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/dynamodb/#local","title":"local","text":"<p>can help find data within a single partition key</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/dynamodb/#readwrite-capacity-mode","title":"Read/write capacity mode","text":"<p>This is for memory not disk space. AWS will allocate more disk space as needed but you do need to specify a capacity for input/output - amazon charges by this metric. By default you're given 5 read capacity units and 5 write capacity units. Don't go crazy with it because it will get very expensive.</p> <p>2 modes for this: Provisioned and On-Demand</p> <p>Provisioned - Allows you to provision set read and writes allowed against your db per second by your app. - Measured in capcaity units RCU, WCU. Depending on the transaction, each action will use 1 or more RCUs or WCUs. - Generally used when you have a predicted and forecasted workload of traffic</p> <p>On-Demand - Does not provision any RCUs or WCUs, instead they are scaled on demand. Not as cost effective. - Generally used if you do not know how much workload you are expected to experience - With more understanding of load y9ou can then change your mode across to Provisioned</p> <p>Advantages of DynamoDB - Fully-managed - Schemaless - Highly Available. Can be distributed across multiple AZs. - Fast. Does not slow down no matter how big a table gets (unlike RDBs)</p> <p>Disadvantages - Eventual consistency - Data is automatically copied across 3 AZs within a geographic region which can lead ot inconsistencies across zones for a short time. Usually this only takes a few miliseconds but can sometimes take a bit longer. - Queries are less flexible than SQL. You can't write advanced queries with joins, groupings, and summaries. More computation will have to happen in the app code. - Workflow limitations. Maximum record size is 400kb. Max indexes per table is 20 global, 5 secondary. This can be adjusted by contacting AWS support. - Provisioned throughput. You need to manually scale if there's an uptick in traffic otherwise queries will fail with a ProvisionedThroughputExceededException.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/elasticache/","title":"Amazon Elasticache","text":"<p>A service that makes it easy to deploy, operate, and scale open-source, in-memory data stores in the cloud.  This service improves the performance through caching, where web applications allow you to retrieve information from fast managed in-memory data stores instead of relying entirely on slower disk-based solutions.</p> <p>Caching - Additional memory enables our device to store frequently accessed info in memory instead of having to request the information from the harddrive. - Memory accessed from RAM is way faster than hard drive. - Typically good for read-only data since the majority of users are reading and not writing most of the time</p> <p>Elasticache can be used for apps using RDBs or NoSQL databases. It can be used for any application that could use performance improvements from in-memory caching - not just web apps.</p> <p>If an application gets growing traffic you can add caching instead of having to scale your web servers way up quickly. Instead of scaling vertically you scale horizontally which is less resource intensive and ccheaper.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/elasticache/#engines","title":"Engines","text":""},{"location":"environments/cloud/aws/aws-dev-cert/database/elasticache/#memcached","title":"Memcached","text":"<p>A high performance, submillisecond latency in-memory key-value store service that can either be used as a cache, in addition to a data store.</p> <p>Less features than Redis but is recognized for simplicity and speed/performance.</p> <ul> <li>Caching</li> <li>Session Store</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/elasticache/#redis","title":"Redis","text":"<p>A purely in-memory data store designed for high performance and again providing sub-millisecond latency on a huge scale to real-time applications.</p> <p>More robust than Memcached but not as performant.</p> <ul> <li>Caching</li> <li>Session Store</li> <li>Chat and Messaging</li> <li>Gaming Leaderboards</li> <li>Geospacial</li> <li>Machine Learning</li> <li>Media Streaming</li> <li>Qiueues</li> <li>Real-Time Analytics</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/elasticache/#components-of-elasticache-nodes-shards-and-clusters","title":"Components of Elasticache - Nodes, Shards, and Clusters","text":""},{"location":"environments/cloud/aws/aws-dev-cert/database/elasticache/#node","title":"Node","text":"<p>A fixed sized chunk of secure, network-attached RAM. Essentially the buildling blocks of the Elasticache service and supports a cluster configuration.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/elasticache/#shard","title":"Shard","text":"<p>Redis Shard (node group) a group of up to 6 nodes</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/elasticache/#cluster","title":"Cluster","text":"<p>Redis Cluster - a group of 1-90 shards. Data is distrubted across all the shards in the cluster. Memcached Cluster - a collection of one or more cache nodes.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/elasticache/#common-use-cases-of-elasticache","title":"Common Use Cases of Elasticache","text":"<ul> <li>Used a lot in online gaming where scores and other data need to be presented ASAP</li> <li>Social media for storing temp session data and management</li> <li>Real time analytics</li> </ul> <p>These uses are very read-only heavy.</p> <p>When to not use this? - Not good for data persistence - working with primary data records - need write performance rather than read performance</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/knowledge_check/","title":"Knowledge Check","text":"<p>Which of the following statements about Amazon DynamoDB is not true? Reads and writes can slow down as the table gets large.</p> <p>Explanation:  DynamoDB tables are schemaless so you don't have to define the exact data model in advance. The data model can change automatically to fit your application's needs. DynamoDB is designed to be fast; reads and writes take just a few milliseconds to complete and DynamoDB will be fast no matter how large your table grows, unlike a relational database, which can slow down as the table gets large. DynamoDB is designed to be highly available and your data is automatically replicated across three different availability zones within a geographic region. Finally, although DynamoDB performance can scale up as your needs grow, your performance is limited to the amount of read and write throughput that you've provisioned for each table.</p> <p>When using Provisioned Capacity mode, how are you charged for Amazon DynamoDB? by the total amount of throughput that you configure for your tables plus the total amount of storage space used by your data</p> <p>Explanation:  You are charged for the total amount of throughput that you configure for your tables plus the total amount of storage space used by your data.</p> <p>In Amazon RDS, what is the purpose of Multi-AZ deployment? to create high availability and data redundancy</p> <p>Explanation:  If high availability and resiliency are of importance when it comes to your database, then you might want to consider a feature known as Multi-AZ, which stands for multi-availability zones. When Multi-AZ is configured, a secondary RDS instance is deployed within a different availability zone within the same region as the primary instance. The primary purpose of the second instance is to provide a failover option for your primary RDS instance. When we have a Multi-AZ deployment, it will create another standby instance in a different availability zone to create high availability and data redundancy.</p> <p>Which of the following database engines is supported by Amazon RDS? Amazon Aurora MySQL PostgreSQL</p> <p>Explanation:  Amazon RDS allows you to select from a range of different database engines. These currently include MySQL, MariaDB, PostgreSQL, Amazon Aurora, Oracle, and SQL Server. MySQL is considered the number-one open-source relational database management system. MariaDB is the community-developed fork of MySQL. The PostgreSQL database engine comes in a close second behind MySQL as the preferred open source database. Amazon Aurora is AWS's own fork of MySQL, which provides ultrafast processing and availability, as it has its own cloud-native database engine. The Oracle database is a common platform in corporate environments. SQL Server is a Microsoft database with a number of different licensing options.</p> <p>Which of the following statements about Amazon Neptune is not true? It supports SQL queries.</p> <p>Explanation:  Amazon Neptune uses its own graph database engine and supports two graph query frameworks. One of these is Apache Tinkerpop Gremlin, and this allows you to query your graph running on your Neptune database, using the Gremlin traversal language. And we have the Worldwide Web Consortium Sparql.</p> <p>With regard to database storage, what does IOPS stand for? input and output operations per second</p> <p>Explanation:  If we look at the Provisioned IOPS, we can define the allocated storage and then also the number of IOPS as well, which is the input and output operations per second.</p> <p>An in-memory cache is generally used to improve read-only performance</p> <p>Explanation:  A common scenario is to have a web application that reads and writes data to persistent storage--for example, to a relational database such as RDS or a NoSQL database such as DynamoDB. This is where an in-memory cache is useful. It's generally used to improve read-only performance.</p> <p>Which of the following is not a common use case for ElastiCache? customer relationship management systems</p> <p>Explanation:  Before I finish this lecture covering ElastiCache, I want to point out some of the common use cases where you might use Amazon ElastiCache. Due to its incredibly fast performance and scaling abilities, this is commonly used in the online gaming industry, where it's vital that statistical information like a scoreboard is presented as quickly and as consistently as possible to all the players in the game. Another common use is for social networking sites, where they need a way to store temporary session information in session management. Real-time analytics is also a great use for ElastiCache, as it can be used in conjunction with other services such as Amazon Kinesis to ingest, process, and analyze data at scale.</p> <p>Each Neptune database cluster maintains a separate copy of the virtual database cluster volume in at least how many different availability zones? 3</p> <p>Explanation:  To ensure high availability is factored into Neptune, each cluster maintains a separate copy of the shared volume in at least three different availability zones.</p> <p>What is caching? storing frequently accessed information in memory to allow faster access than when requesting the information from a persistent data store</p> <p>Explanation:  Additional memory enables our device to store frequently accessed information in memory instead of having to request the information from the hard drive, which is much slower than RAM. This process is known as caching.</p> <p>Amazon Neptune is a fast, reliable, secure, and fully managed (blank) service. graph database</p> <p>Explanation:  Amazon Neptune is a fast, reliable, secure, and fully managed graph database service.</p> <p>Which Amazon RDS template uses defaults for high availability and fast and consistent performance? Production</p> <p>Explanation:  Now, depending on what template we select, Amazon RDS will predefine a list of other configurable components. The Production template uses defaults for high availability and fast and consistent performance. The Dev/Test template is intended for development use outside of a production environment. And the Free Tier is simply to allow you to get hands-on experience with RDS and doesn't really use many of the features.</p> <p>Which of the following is a type of Amazon Neptune endpoint? A reader A cluster An instance All of these are Neptune endpoints.</p> <p>Explanation:  There are three different types of Amazon Neptune endpoints: cluster endpoint, reader endpoint, and instance endpoint.</p> <p>What is an endpoint on Amazon Neptune? a URL address and a port that points to your components</p> <p>Explanation:  An endpoint is simply a URL address and a port that points to your components.</p> <p>Amazon ElastiCache allows you to retrieve information from where? in-memory data stores</p> <p>Explanation:  Amazon ElastiCache is a service that makes it easy to deploy, operate, and scale open-source, in-memory data stores in the cloud. This service improves performance through caching, where web applications allow you to retrieve information from fast, managed, in-memory data stores instead of relying entirely on slower disk-based solutions.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/neptune/","title":"Amazon Neptune","text":"<p>May not be as widely utilized as perhaps Amazon RDS or Amazon DynamoDB, simply due to what it is designed for. Amazon Neptune is a fast, reliable, secure, and fully managed graph database service.</p> <p>Graph databases are for storing and navigating relationships between highly connected data which could contain billions of separate relationships. This can be very difficult with traditional RDBs.</p> <p>Use case examples:</p> <ul> <li>Social networking</li> <li>Fraud detection (security should always be priority #1)</li> <li>Recommendation engines (targeted ads)</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/neptune/#query-languages","title":"Query Languages","text":"<p>Apache Tinkerpop Gremlin: Allows you to query your graph running on your Neptune DB using the Gremlin traversal language</p> <p>World Wide Web Consortium SparQL: Designed to work with the internet and can be used to run queries against your Neptune DB graph</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/neptune/#database-clusters-instances-and-storage","title":"Database Clusters, Instances, and Storage","text":"<p>A database cluster is comprised of one or more database instances across different AZs, in addition to a virtual database cluster volume which contains the data across all instances within the cluster. The single cluster volume consists of a number of Solid State Discs, SSDs. As your graph database grows, your shared volume will automatically scale and increase in size as required to a maximum of 64 terabytes.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/neptune/#high-availability-and-reliability","title":"High Availability and Reliability","text":"<ul> <li>Each cluster maintains a separate copy of the shared volume in at least 3 different AZs. </li> <li>Also has Storage auto-repair to ensure data loss is minimized and need to restore data is minimized.</li> <li>Can also run replica instances that are read only while primary is r+w. 15 clusters per replica can exist which span multiple AZs</li> <li>If primary fails, a replica in a different AZ will be delegated as the primary and adopt r+w operations. This process takes usually about 30 seconds.</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/neptune/#connecting-to-your-neptune-database","title":"Connecting to your Neptune Database","text":"<p>3 types of endpoints: Cluster, Reader, Instance</p> <p>Cluster Endpoint - Points directly to current primary DB instance of cluster - Should be used by apps that require r+w access to db - When primary DB instance fails, it will point to the new primary instance without any changes required by your apps accessing the DB</p> <p>Reader Endpoint - Used to connect to read replicas - Allows apps to access db on read only bases for queries - A single reader endpoint exists even if you have multiple read replicas - Connections served by the read replicas will be performed on a round-robin basis - The endpoint does not load balance your traffic in any way across the available replicas in your cluster</p> <p>Instance Endpoint - Every instance within your cluster will have a unique instance endpoint - Allows you to direct certain traffic to specific instances within the cluster - You might want to do this for load balancing reasons across your appications reading from yoru replicas</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/qldb/","title":"Quantum Ledger Database (QLDB)","text":"<p>Just came out in September 2019. Is another fully managed and severless database service designed to be a ledger database.</p> <p>Highly secure</p> <p>Is great for maintaining an immutable ledger with cryptographic abilities to enable the verifiable tracking of changes over time.</p> <p>Use case example: Recording financial data over a period of time.</p> <p>Plain text -&gt; hash function -&gt; hashed text</p> <p>Maintains a complete hisstory of accounting and transactional data between parties in an immutable, transparent, and cryptographic way.</p> <p>Is append only and uses a database journal. This is to ensure that no existing data is changed. Essentially, the immutable transaction log that records all entries in a sequenced manner over time. This service therefore negates the need for an organization to develop and implement their own ledger applications.</p> <p>Similar to blockchain, but blockchain is distributed across multiple hosts in a decentralized environment. QLDB is owned and managed by a central and trusted authority.</p> <p>QLDB is serverless: Infrastructure administration and maintenance is managed by AWS. All scaling is managed by AWS.</p> <p>This would be a good service for the insurance industry (especially Market Regulation). By its nature it can be a long winded and extensive process involving many different parties and operations over a long time period. QLDB's immutable nature would help prevent fraud.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/qldb/#concepts-and-components","title":"Concepts and Components","text":"<p>Is based on a ledger.</p> <p>Data is placed in Amazon Ion documents which is internally created by Amazon. Ion documents are an open-source self-describing data serialization format and is a superset of JSON, meaning any JSON doc is also considered a valid Ion document.</p> <ul> <li>Each time a change is committed to the journal, a sequence number is added to identify its place in the change history. </li> <li>Can verify the integrity of the changes because of the SHA-256 bit hash used which creates a cryptographic digest file of the journal.</li> <li>This helps to ensure that the data within your document  has not been altered</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/qldb/#storage","title":"Storage","text":"<p>Split into two categories: journal and index</p> <p>Journal storage: used to hold the history of changes made within the ledger database. Holds immutable changes and history for the Ion documents within your table.</p> <p>Index storage: Used to provision the tables and indexes within your ledger. Optimized for querying.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/qldb/#amazon-kinesis-integration","title":"Amazon Kinesis Integration","text":"<p>Integrates really well with Amazon Kinesis, which is a service that makes it easy to collect, process, and analyze real-time streaming data so you can get timely insights and react quickly to new information. Ingest real-time data such as application logs, website clickstreams, IoT telemetry data, and more into your databases, your data lakes and data warehouses, or build your own real-time applications using this data. It allows you to analyze the data in real time rather than having to wait for all your data to be collected before processing can begin.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/rds/","title":"Amazon Relational Database Service","text":"<p>Is a managed service which means amazon handles backups, OS patches, and DB software patches.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/rds/#database-engines","title":"Database engines","text":"<ul> <li>MySQL (The most popular open source relational database managment system)</li> <li>MariaDB (Community developed sql</li> <li>PostreSQL (Close 2nd behind MySQL for open source RDMS)</li> <li>Amazon Aurora (AWS fork of mysql)</li> <li>Oracle (commonly used in corporate because they enjoy pain)</li> <li>SQL Server (Microsoft)</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/rds/#multi-az","title":"Multi AZ","text":"<p>You can deploy your RDS instance in a single availability zone. However, if high availability and resiliency is of importance then consider Multi AZ (multi availability zones) </p> <p>When configured a secondary RDS instance is deployed within a different availability zone within the same region as the primary instance. </p> <p>The primary purpose of the second instance is to provide a failover option for your primary RDS instance. The replication of data between the primary RDS database and the secondary replica instance happens synchronously. This happens automatically and doesn't require any intervention by you. RDS will update the DNS record to point to the secondary instance. Takes 1-2 minutes depending on size, number of transactions, and activity at the time of failover.</p> <p>Failover happens if:</p> <ul> <li>patching maintenance has been performed in the primary instance </li> <li>the instance of the primary database has a host failure </li> <li>the availability zone of the primary database fails</li> <li>the primary instance was rebooted with failover</li> <li>the primary database instance class on the primary database is modified</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/rds/#scaling-your-database","title":"Scaling Your Database","text":"<p>MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server all use Elastic Block Store, EBS volumes, for both data and log storage. However, Amazon Aurora uses a shared cluster storage architecture and does not use EBS.</p> <p>The engines that use EBS support General Purpose SSD Storage, Provisioned IOPS (SSD) Storage, and Magnetic Storage.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/rds/#general-purpose-ssd-storage","title":"General Purpose SSD Storage","text":"<ul> <li>A good option for a broad range of use cases </li> <li>Provides single-digit millisecond latencies </li> <li>Offers a cost-effective storage solution</li> <li>The minimum SSD storage volume for your primary dataset is 20 gibibytes with a maximum of 64 tebibytes for MySQL, </li> <li>PostgreSQL, MariaDB, and Oracle. However, the maximum for SQL Server is 16 tebibytes. </li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/rds/#provisioned-iops-ssd-storage","title":"Provisioned IOPS (SSD) Storage","text":"<ul> <li>Good for workloads that operate at a very high I/O</li> <li>IOPS: Minimum 8,800, max 80,000 (SQL Server 40,000)</li> <li>Storage for primary data: Minimum 100GiB, max 64TiB (SQL Server 16TiB)</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/rds/#magnetic-storage","title":"Magnetic Storage","text":"<p>Provided for backwards compatibility. AWS recommends General Purpose instead.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/rds/#shared-cluster-storage","title":"Shared Cluster Storage","text":"<ul> <li>Aurora uses this</li> <li>Automatically managed by the service itself</li> <li>The option to configure and select storage options does not exist</li> <li>Your storage will scale automatically as your database grows</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/rds/#compute-scaling","title":"Compute Scaling","text":"<ul> <li>Vertical scaling will enhance the performance of your database instance. For example, scaling up from an m4.large to an m4.2xlarge. <ul> <li>Can make the change immediately or wait for a scheduled maintenance window.</li> </ul> </li> <li>Horizontal scaling will see an increase in the quantity of your current instance. For example, moving from a single m4.large to three m4.large instances in your environment through the means of read replicas.<ul> <li>Can make a read replica to help share the load of heavy read traffic</li> </ul> </li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/rds/#automated-services","title":"Automated Services","text":"<p>Backups and software patches are handled automatically.</p> <p>As Amazon RDS is a managed service, and from a shared responsibility model is considered a container service where you have no access to the underlying operating system on which your database runs on.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/rds/#consumer-responsibility","title":"Consumer Responsibility","text":"<ul> <li>Customer data</li> <li>Client side data encryption &amp; data integrity authentication</li> <li>Network traffic protection (encryption/ integrity/identity)</li> <li>Customer IAM</li> <li>Firewall configuration</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/rds/#aws-responsibility","title":"AWS Responsibility","text":"<ul> <li>Platform &amp; Application management</li> <li>OS &amp; Network configuration</li> <li>AWS Endpoints</li> <li>AWS Foundation Services (Compute, Storate, Database, Network)</li> <li>AWS Global Infrastructure (Regions, Availability Zones, Edge Locations)</li> <li>AWS IAM</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/redshift/","title":"Amazon Redshift","text":"<p>Relational Database Management System based on Postgres. Designed for high performance and analysis of information capable of storing and processing petabytes of data and provide access to this data. It's considered a \"data warehouse\"</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/redshift/#data-warehouse","title":"Data Warehouse","text":"<p>Used to consolidate data from multiple sources to allow for data driven business decisions</p> <ul> <li>By its nature needs to be able to store huge amounts of data</li> <li>Commonly has data cleansing and ETL operations</li> </ul>"},{"location":"environments/cloud/aws/aws-dev-cert/database/redshift/#etl-extract-transform-and-load","title":"ETL (Extract, Transform, and Load)","text":"<p>Common process by which data from multiple systems is combined into a single data store for legacy storage or analytics.</p> <p>Extraction is the process of retrieving data from one or more sources. Either online, brick &amp; mortar, legacy data, Salesforce data and many others. After retrieving the data, ETL is to compute work that loads it into a staging area and prepares it for the next phase.</p> <p>Transformation is the process of mapping, reformatting, conforming, adding meaning and more to prepare the data in a way that is more easily consumed. One example of this is the transformation and computation where currency amounts are converted from US dollars to euros.</p> <p>Loading involves successfully inserting the transform data into the target database data store, or in this case, a data warehouse. All of this work is processed in what the business intelligent developers call an ETL job.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/redshift/#redshift-architecture","title":"Redshift Architecture","text":"<p>Clusters contain Redshift engine which contains at least one database. Also contains a compute node. If more than one node is present a Leader node is designated. This acts as a gatekeeper to the cluster for external apps. Takes information from outside and then executes plans to the cluster. Then takes response and passes it back to external app.</p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/redshift/#dense-compute-node-types","title":"Dense Compute Node Types","text":"<p>The nodes are split into slices. The slices are partitions of memory and disk space. It's used to process operations given by the leader node where parallel operations can be performaed across all slices and all nodes at o nce for the same query. When a table is created it is possible to distribute rows of that table across different nodes slices based upon how the distribution case is defined for the table. </p>"},{"location":"environments/cloud/aws/aws-dev-cert/database/redshift/#performance","title":"Performance","text":"<ul> <li>3x faster than other data warehouses</li> <li>Massively Parallel Processing (MPP)</li> <li>Columnar Data Storage - similar to caching where it reduces the number of disk I/O operations and allows for in-memory operations. </li> <li>Result Caching - stores common results in the leader node memory to reduce query time</li> </ul> <p>Redshift can be integrated with Amazon CloudWatch to monitor CPU utilization, throughput, and query/load performance.</p> <p>You can assign up to 10 IAM roles to associate with your cluster during creation.</p>"},{"location":"environments/linux/","title":"Linux Overview","text":"<p>For the stuff I may not use daily and can't keep inside my head</p>"},{"location":"environments/linux/automation/","title":"Automation","text":"","tags":["Linux","Automation","Cron","Systemd"]},{"location":"environments/linux/automation/#cron-logs","title":"Cron Logs","text":"<p>See the last 20 lines of the cron logs with</p> <pre><code>sudo journalctl -u cron.service | tail -20\n</code></pre>","tags":["Linux","Automation","Cron","Systemd"]},{"location":"environments/linux/automation/#systemd-is-it-present","title":"Systemd: is it present?","text":"<p>It's not always apparent that systemd is being used by a system and it shouldn't be assumed that it is (even though it seems most distros are utilizing it now). To confirm you can check what <code>/sbin/init</code> is; <code>file /sbin/init</code> will tell you if it's a real executable or if it's a symbolic link to some other package's executable. If systemd is present you'll see the following output:</p> <pre><code>user@hostname ~ $ file /sbin/init\n/sbin/init: symbolic link to ../lib/systemd/systemd\n</code></pre> <p>For more information, see: https://en.wikipedia.org/wiki/Linux_startup_process</p> <p>Another way of seeing exactly what you have on your system is typing <code>man init</code> and seeing which program's man page you end up on.</p> <p>Credit: https://superuser.com/a/1018046/514856</p>","tags":["Linux","Automation","Cron","Systemd"]},{"location":"environments/linux/desktop/","title":"Desktop","text":"","tags":["Linux"]},{"location":"environments/linux/desktop/#fefoda","title":"Fefoda","text":"<p>For Fedora-specific good-to-knows.</p>","tags":["Linux"]},{"location":"environments/linux/desktop/#dnf","title":"DNF","text":"","tags":["Linux"]},{"location":"environments/linux/desktop/#package-install-date","title":"Package install date","text":"<p>The <code>dnf info &lt;package-name&gt;</code> command gives useful metadata about a package but doesn't include when it was installed on your system. Use <code>rpm -qi &lt;package-name&gt;</code> to see an installation date.</p>","tags":["Linux"]},{"location":"environments/linux/desktop/#safe-dnf-update","title":"Safe dnf update","text":"<p>There's a risk with <code>dnf update</code> from a graphical terminal session.. This almost always works fine, but in extremely rare cases it can destroy your operating system. This can happen when a process in the desktop environment crashes while the update is running. To safely run dnf update without worry, switch to a virtual terminal <code>Ctrl+Alt+F3</code> and run it there, where it doesn't stand any risk of being killed mid-operation. Use <code>Ctrl+Alt+F2</code> to return to your desktop. Other online updaters, like <code>apt-get</code>, are no different; this advice applies to any program that you can use to update your system.</p>","tags":["Linux"]},{"location":"environments/linux/desktop/#when-to-reboot-after-update","title":"When to reboot after update","text":"<p>Install <code>dnf-plugin-tracer</code> to have dnf show what needs restarting after every transaction. It's much faster and friendlier than <code>needs-restarting</code>.</p>","tags":["Linux"]},{"location":"environments/linux/desktop/#installing-older-packages","title":"Installing older packages","text":"<p>Downgrade to the last package release by running <code>dnf downgrade &lt;package&gt;</code>.</p> <p>To find a specific older version (assuming it's still available in the repository) run <code>dnf --showduplicates list &lt;package&gt;</code>.</p> <p>If a package needs to be ignored during upgrade time, edit <code>/etc/dnf/dnf.conf</code> and add <code>exclude=package_names package_name_pattern</code>.</p>","tags":["Linux"]},{"location":"environments/linux/desktop/#updating-everything","title":"Updating everything","text":"<p>Gnome-software has gotten better but is still really slow in most cases and unreliable sometimes. Best bet is to do the following:</p> <ol> <li>Launch virtual terminal</li> <li>Run <code>dnf update</code>. <ol> <li>Note the incoming changes and type <code>y</code> followed by <code>&lt;enter&gt;</code></li> <li>Note whether a reboot is needed</li> </ol> </li> <li>Run <code>flatpak update</code></li> <li>Run <code>fwupdmgr refresh</code> followed by <code>fwupdmgr update</code></li> </ol>","tags":["Linux"]},{"location":"environments/linux/desktop/#misc","title":"Misc","text":"","tags":["Linux"]},{"location":"environments/linux/desktop/#installing-chromedriver","title":"Installing ChromeDriver","text":"<p>For running headless browser instances.</p> <p><code>chromedriver.x86_64</code> is now in <code>dnf</code> but if for some reason this needs to be installed manually do the following:</p> <ol> <li><code>sudo dnf install chromium</code></li> <li>Double check the installed version. If for some reason it's not obvious from the install output you can run <code>dnf list installed chromium</code>.</li> <li>Navigate to the ChromeDriver downloads page and find the version that matches the version of Chromium that was just installed. Only major version really matters.</li> <li>Download the zip file for Linux.</li> <li><code>unzip chromedriver_linux64.zip</code></li> <li>Move the executable to <code>/usr/bin</code>, change ownership to root, and give it execute permission:</li> </ol> <pre><code>sudo mv chromedriver /usr/bin/chromedriver \nsudo chown root:root /usr/bin/chromedriver \nsudo chmod +x /usr/bin/chromedriver\n</code></pre>","tags":["Linux"]},{"location":"environments/linux/desktop/#turn-off-machine-beeps-fedora","title":"Turn off machine beeps (Fedora)","text":"<p>su - modprobe -r pcspkr echo \"install pcspkr :\" &gt;&gt; /etc/modprobe.conf</p>","tags":["Linux"]},{"location":"environments/linux/desktop/#gnome","title":"Gnome","text":"","tags":["Linux"]},{"location":"environments/linux/desktop/#how-to-alt-tab-windows-on-current-workspace-only","title":"How to Alt Tab windows on current workspace only","text":"<p>gsettings set org.gnome.shell.app-switcher current-workspace-only true</p>","tags":["Linux"]},{"location":"environments/linux/desktop/#wayland-or-xorg","title":"Wayland or Xorg","text":"<p>Run <code>echo $XDG_SESSION_TYPE</code> to find out if the Gnome session you're using is Wayland or Xorg</p>","tags":["Linux"]},{"location":"environments/linux/desktop/#managing-des","title":"Managing DEs","text":"","tags":["Linux"]},{"location":"environments/linux/desktop/#kde-specific","title":"kde specific","text":"<pre><code>kquitapp5 plasmashell # Kill plasma\nkstart5 plasmashell # Start plasma\n</code></pre>","tags":["Linux"]},{"location":"environments/linux/desktop/#xfce-specific","title":"xfce specific","text":"<pre><code>xfce4-taskmanager    # Open up task manager\nxfce4-panel -r       # Reset xfce panel (for after styling/layout change)\n</code></pre>","tags":["Linux"]},{"location":"environments/linux/file-management/","title":"File Management","text":"","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#list-file-sizes","title":"List file sizes","text":"<p>To include hidden files and directories, as well was sort by size, use <code>du -hs $(ls -A) | sort -h</code>. </p>","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#delete-all-node_modules","title":"Delete all node_modules","text":"<pre><code>find . -name \"node_modules\" -exec rm -rvf '{}' +\n</code></pre>","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#trash","title":"Trash:","text":"<p>To trash some_file (or folder) use</p> <pre><code>gio trash some_file\n</code></pre> <p>To go dumpster diving use</p> <pre><code>gio list trash://\n</code></pre> <p>To empty trash</p> <pre><code>gio trash --empty\n</code></pre>","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#tar","title":"tar","text":"<p><pre><code>-z # filter the archive through gzip\n-v # verbose: what's in the archive\n-t # list files in archive\n-x # extract contents of archive\n-c # tells tar to create archive file\n-f # tells tar the name of the file that's being operated on\n</code></pre> So to compress a file, use</p> <pre><code>tar -czvf projects.tar.gz ./project*\n</code></pre> <p>\"tar: create an archive, run it through gzip, tell me what you're compressing, and the file's name is projects.tar.gz. Do this operation on files in this folder with names starting with 'projects' \".</p> <p>To extract the contents of a file, use</p> <p><pre><code>tar -xzvf projects.tar.gz\n</code></pre> \"tar: extract, uncompress, and tell me what you're pulling out of projects.tar.gz\"</p>","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#inodes","title":"inodes","text":"<p>An inode is a data structure that stores various metadata about a file such as permissions, type, size, physical location, etc. A name of a file is a reference that points to the inode. </p> <pre><code>myfile.txt -&gt; (some inode ID)\n</code></pre>","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#copy-vs-hard-link-vs-soft-symbolic-link","title":"Copy vs Hard Link vs Soft (Symbolic) Link","text":"","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#copy","title":"Copy","text":"<p>Creates a new file that points to a new memory location with the contents of the original file.</p> <p></p> <p></p>","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#hard-link","title":"Hard Link","text":"<p>Creates a new file that points to the same memory location as the original file. This essentially \"syncs\" the two files</p> <p></p> <p></p>","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#soft-link","title":"Soft Link","text":"<p>a.k.a. symbolic link. Creates a file that points to another file. They are just pointers to the file reference (which in itself is pointing to the inode)</p> <p></p> <p></p>","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#file-timestamps","title":"File Timestamps","text":"","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#stat","title":"stat","text":"<p>This gives the history of a file. Output looks something like the following:</p> <pre><code>$ stat *.py\n  File: ebay_is_us_store_sorter.py\n  Size: 3488            Blocks: 8          IO Block: 4096   regular file\nDevice: fd02h/64770d    Inode: 2266        Links: 1\nAccess: (0664/-rw-rw-r--)  Uid: ( 1000/swhitmore)   Gid: ( 1000/swhitmore)\nContext: unconfined_u:object_r:user_home_t:s0\nAccess: 2021-01-22 16:05:58.452056363 -0600\nModify: 2021-01-08 17:00:05.181712679 -0600\nChange: 2021-01-08 17:00:05.181712679 -0600\n Birth: 2021-01-08 17:00:05.181712679 -0600\n  File: surplus_automated_import.py\n  Size: 3415            Blocks: 8          IO Block: 4096   regular file\nDevice: fd02h/64770d    Inode: 2348        Links: 1\nAccess: (0664/-rw-rw-r--)  Uid: ( 1000/swhitmore)   Gid: ( 1000/swhitmore)\nContext: unconfined_u:object_r:user_home_t:s0\nAccess: 2021-04-23 09:34:19.442759029 -0500\nModify: 2021-01-08 17:00:05.183712768 -0600\nChange: 2021-01-08 17:00:05.183712768 -0600\n Birth: 2021-01-08 17:00:05.183712768 -0600\n</code></pre> <p>If you want to adjust the format, refer to the man pages, since the output is OS-specific and varies under Linux/Unix.</p>","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#via-ls-command","title":"via <code>ls</code> command","text":"<p>Generally, you can get the times through a normal directory listing as well:</p> <ul> <li><code>ls -l</code> outputs last time the file content was modified, the <code>mtime</code></li> <li><code>ls -lc</code> outputs last time of file status modification, the <code>ctime</code></li> <li><code>ls -lu</code> outputs last access time, the <code>atime</code></li> </ul>","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#mtime-vs-ctime-vs-atime","title":"<code>mtime</code> vs <code>ctime</code> vs <code>atime</code>","text":"<p><code>mtime</code>, or modification time, is when the file was last modified. When you change the contents of a file, its <code>mtime</code> changes.</p> <p><code>ctime</code>, or change time, is when the file's property changes. It will always be changed when the mtime changes, but also when you change the file's permissions, name or location.</p> <p><code>atime</code>, or access time, is updated when the file's contents are read by an application or a command such as <code>grep</code> or <code>cat</code>.</p> <p>The easiest way to remember which is which is to read their alphabetical order:</p> <ul> <li><code>atime</code> can be updated alone</li> <li><code>ctime</code> will update <code>atime</code></li> <li><code>mtime</code> will update both <code>atime</code> and <code>ctime</code></li> </ul>","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#vim","title":"Vim","text":"","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#findreplace-multiple-occurrences-of-same-word","title":"Find/replace multiple occurrences of same word","text":"","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#in-editor","title":"In editor","text":"<ol> <li>Put cursor over word</li> <li>While in Read mode, press <code>*</code>. This selects the word.</li> <li>Press <code>c</code> for change</li> <li>Press <code>gn</code> for next match</li> <li>Make change</li> <li>Press <code>&lt;esc&gt;</code> to get out of Insert mode</li> <li>Press <code>.</code> to replace next match</li> </ol>","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#from-command-sed-style","title":"From command (sed style)","text":"<p><code>%substitute//replacement/gc</code></p>","tags":["Linux","Bash"]},{"location":"environments/linux/file-management/#from-command-regex-style","title":"From command (regex style)","text":"<pre><code>:%norm A*\n</code></pre> <p>This is what it means:</p> <p>```bash  %       = for every line</p> <p>norm    = type the following commands</p> <p>A      = append '' to the end of current line  ```</p>","tags":["Linux","Bash"]},{"location":"environments/linux/interfacing/","title":"Interfacing","text":"","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#git","title":"git","text":"","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#how-to-merge-feature-branch-to-master-branch-assuming-you-can-push-to-master","title":"How to merge feature branch to master branch (assuming you can push to master)","text":"<pre><code>git checkout master\ngit pull origin master\ngit merge feature-branch\ngit push origin master\n</code></pre>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#update-feature-branch-with-master-contents","title":"Update feature branch with master contents","text":"<pre><code>git checkout feature-branch\ngit merge origin/master\ngit push origin feature-branch\n</code></pre>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#how-to-replace-the-master-branch-with-another-branch","title":"How to replace the master branch with another branch","text":"<pre><code>git checkout master\ngit reset --hard fixed-master-branch\ngit push origin master -f\n</code></pre>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#setting-local-branch-to-exactly-match-the-remote-branch","title":"Setting local branch to exactly match the remote branch","text":"<pre><code>git fetch origin\ngit reset --hard origin/&lt;branch-name&gt;\n</code></pre> <p>If you want to save your current branch's state before doing this (just in case), you can do:</p> <pre><code>git commit -a -m \"Saving my work, just in case\"\ngit branch my-saved-work\n</code></pre> <p>Alternatively you can stash your changes</p> <pre><code>git stash\n</code></pre> <p>do the reset, then</p> <pre><code>git stash pop\n</code></pre> <p>and then they're back with the local branch now synced up with the remote master.</p>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#undo-last-commit-before-push","title":"Undo last commit (before push)","text":"<pre><code>git reset --soft HEAD~1\n</code></pre>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#revert-branch-back-to-certain-commit","title":"Revert branch back to certain commit","text":"<pre><code>git reset --hard &lt;sha1&gt;\ngit push --force origin &lt;branch-name&gt;\n</code></pre>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#remember-git-credentials","title":"Remember git credentials","text":"<p>Assuming you can't use an ssh key (which would be best)...</p> <p>Install Gnome Keyring devel</p> <pre><code>sudo apt-get install libgnome-keyring-dev\nsudo make --directory=/usr/share/doc/git/contrib/credential/gnome-keyring\n</code></pre> <p>Set up credential</p> <pre><code>git config --global credential.helper /usr/share/doc/git/contrib/credential/gnome-keyring/git-credential-gnome-keyring\n</code></pre> <p>If you're on a machine where you can't install any additional software then you can use the following for a temporary workaround</p> <pre><code> git config --global credential.helper 'cache --timeout={number in seconds}'\n</code></pre>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#local-cleanup","title":"Local cleanup","text":"<pre><code>git branch | grep -v \\\"master\\\" | xargs git branch -D\n</code></pre> <p>purge all local branches except master</p> <pre><code>git fetch -p\n</code></pre> <p>prunes remote branches which show locally that dont exist in remote anymore</p> <pre><code>git branch -d &lt;doomed-branch-name&gt;\n</code></pre> <p>To delete a branch locally. If there's something that needs to be committed on that a branch it should warn you and refuse to delete it. If you don't care about the changes for that branch then run it with -D instead of -d and it'll force the deletion.</p> <pre><code>git branch | grep -v \"master\" | xargs git branch -D\n</code></pre> <p>To delete all local branches except master. This comes in handy if something happens like special characters get into a branch name (like when I copy/pasted something from a word doc that one time).</p> <pre><code>git update-index --assume-unchanged path/to/file\n</code></pre> <p>To ignore future revisions to file and</p> <pre><code>git update-index --no-assume-unchanged path/to/file\n</code></pre> <p>to stop ignoring it</p>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#misc","title":"Misc","text":"<p>Find out what your upstream url is</p> <pre><code>git remote -v\n</code></pre> <p>Reset remote url</p> <pre><code>git remote set-url origin &lt;repo-url&gt;\n</code></pre>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#multiple-git-repos","title":"Multiple Git Repos","text":"<p>If a project has to have multiple git repos (e.g. Bitbucket and Github) then it's better that they remain in sync.</p> <p>Usually this would involve pushing each branch to each repo in turn, but actually Git allows pushing to multiple repos in one go.</p> <p>If in doubt about what git is doing when you run these commands, just edit <code>.git/config</code> (<code>git-config(1)</code>_) and see what it's put there.</p>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#remotes","title":"Remotes","text":"<p>Suppose your git remotes are set up like this::</p> <pre><code>git remote add github git@github.com:muccg/my-project.git\ngit remote add bb git@bitbucket.org:ccgmurdoch/my-project.git\n</code></pre> <p>The <code>origin</code> remote probably points to one of these URLs.</p>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#remote-push-urls","title":"Remote Push URLs","text":"<p>To set up the push URLs do this::</p> <pre><code>git remote set-url --add --push origin git@github.com:muccg/my-project.git\ngit remote set-url --add --push origin git@bitbucket.org:ccgmurdoch/my-project.git\n</code></pre> <p>It will change the <code>remote.origin.pushurl</code> config entry. Now pushes will send to both of these destinations, rather than the fetch URL.</p> <p>Check it out by running::</p> <pre><code>git remote show origin\n</code></pre>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#per-branch","title":"Per-branch","text":"<p>A branch can push and pull from separate remotes. This might be useful in rare circumstances such as maintaining a fork with customizations to the upstream repo. If your branch follows <code>github</code> by default::</p> <pre><code>git branch --set-upstream-to=github next_release\n</code></pre> <p>(That command changed <code>branch.next_release.remote</code>.)</p> <p>Then git allows branches to have multiple <code>branch.&lt;name&gt;.pushRemote</code> entries. You must edit the <code>.git/config</code> file to set them.</p>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#pull-multiple","title":"Pull Multiple","text":"<p>You can't pull from multiple remotes at once, but you can fetch from all of them::</p> <pre><code>git fetch --all\n</code></pre> <p>Note that fetching won't update your current branch (that's why <code>git-pull</code> exists), so you have to merge -- fast-forward or otherwise.</p> <p>For example, this will octopus merge the branches if the remotes got out of sync::</p> <pre><code>git merge github/next_release bb/next_release\n</code></pre>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#references","title":"References","text":"<ul> <li><code>git-config(1)</code>_</li> <li><code>git-remote(1)</code>_</li> <li><code>git-branch(1)</code>_</li> </ul> <p>.. <code>git-config(1)</code>: https://www.kernel.org/pub/software/scm/git/docs/git-config.html .. <code>git-remote(1)</code>: https://www.kernel.org/pub/software/scm/git/docs/git-remote.html .. _<code>git-branch(1)</code>: https://www.kernel.org/pub/software/scm/git/docs/git-branch.html</p> <p>credit https://gist.github.com/rvl/c3f156e117e22a25f242</p>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#gitlab-compare-commits","title":"GitLab compare commits","text":"<ol> <li>go to Repository &gt; Compare</li> <li>https://gitlab.com/$USER/$REPO/compare?from=$SHA1&amp;to=$SHA2</li> </ol>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#ssh","title":"SSH","text":"<pre><code>ssh-keygen -t ed25519 -C \"your_email@example.com\"\n</code></pre>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#slow-ssh-connection","title":"Slow SSH Connection","text":"<p>There are several factors that could contribute to a slow SSH login and activity once logged in.</p> <p>One fix would be to try setting <code>UseDNS</code> to <code>no</code> in <code>/etc/sshd_config</code> or <code>/etc/ssh/sshd_config</code>. This didn't fix it for me but it's good to keep in mind.</p> <p>With systemd (which Debian 11 uses), logins and post-login activity may hang on dbus communication with logind after some upgrades, then you need to restart logind.</p> <p><code>systemctl restart systemd-logind</code></p>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#kill-a-port","title":"Kill a port","text":"","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#linux","title":"Linux","text":"<p><code>fuser 8080/tcp</code> will print the PID of the process bound to <code>8080</code>.</p> <p><code>fuser -k 8080/tcp</code> will kill the process. </p>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#macos","title":"macOs","text":"<p><code>lsof -i :8080</code> print PID of process bound to <code>8080</code>.  </p> <p><code>kill -9 &lt;pid&gt;</code> kill the process.</p>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#send-email-via-gmail-smtp-server","title":"Send Email via Gmail SMTP Server","text":"","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#set-up-gmail-app-password","title":"Set up Gmail App Password","text":"<p>https://myaccount.google.com/apppasswords</p>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/interfacing/#configure-bash-smtp","title":"Configure Bash SMTP","text":"<p><code>sudo apt-get install mutt</code></p> <p><code>vim ~/.muttrc</code></p> <pre><code>set from = \"&lt;gmail-id&gt;@gmail.com\"\nset realname = \"&lt;Your Name&gt;\"\nset smtp_url = \"smtp://&lt;gmail-id&gt;@smtp.gmail.com:587/\"\nset smtp_pass = \"&lt;smtp password generated in the google link above&gt;\"\n</code></pre> <p><code>echo \"This is an email body.\" | mutt -s \"This is an email subject\" recipient@example.com</code></p>","tags":["Linux","Bash","Git","SSH","SMTP"]},{"location":"environments/linux/mounts/","title":"Mounts","text":"","tags":["Linux","Bash"]},{"location":"environments/linux/mounts/#mounting-external-drives","title":"Mounting External Drives","text":"<ol> <li>Identify the External Hard Drive:</li> <li>List all connected drives to identify the external hard drive.    <pre><code>lsblk\n</code></pre></li> <li> <p>The output will show a list of block devices. Look for a device that matches the size of your external hard drive, typically named something like <code>/dev/sdb</code> or <code>/dev/sdc</code>.</p> </li> <li> <p>Mount the External Hard Drive:</p> </li> <li>Create a mount point (directory) where you want to access the external hard drive.    <pre><code>sudo mkdir /mnt/external_drive\n</code></pre></li> <li> <p>Mount the external hard drive to the mount point.    <pre><code>sudo mount /dev/sdX1 /mnt/external_drive\n</code></pre>    Replace <code>/dev/sdX1</code> with the actual device name identified in step 3 (e.g., <code>/dev/sdb1</code>).</p> </li> <li> <p>Access the External Hard Drive:</p> </li> <li> <p>Navigate to the mount point to access the files on the external hard drive.    <pre><code>cd /mnt/external_drive\nls\n</code></pre></p> </li> <li> <p>Unmount the External Hard Drive (when done):</p> </li> <li>Unmount the drive when you are finished to safely remove it.    <pre><code>sudo umount /mnt/external_drive\n</code></pre></li> </ol>","tags":["Linux","Bash"]},{"location":"environments/linux/mounts/#example-commands","title":"Example Commands","text":"<pre><code>ssh user@whitmore_tower_ip\nlsblk\nsudo mkdir /mnt/external_drive\nsudo mount /dev/sdb1 /mnt/external_drive\ncd /mnt/external_drive\nls\nsudo umount /mnt/external_drive\n</code></pre>","tags":["Linux","Bash"]},{"location":"environments/linux/mounts/#summary","title":"Summary","text":"<ul> <li>Use <code>lsblk</code> to identify the external hard drive.</li> <li>Create a mount point using <code>mkdir</code>.</li> <li>Mount the drive with <code>mount</code>.</li> <li>Access the files and unmount when done with <code>umount</code>.</li> </ul>","tags":["Linux","Bash"]},{"location":"environments/linux/mounts/#reformat-a-usb","title":"Reformat a USB","text":"<ol> <li>Identify the volume </li> </ol> <pre><code>$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\nudev            3.4G     0  3.4G   0% /dev\ntmpfs           695M  1.7M  693M   1% /run\n/dev/nvme0n1p2  234G   19G  203G   9% /\ntmpfs           3.4G  131M  3.3G   4% /dev/shm\ntmpfs           5.0M  4.0K  5.0M   1% /run/lock\ntmpfs           3.4G     0  3.4G   0% /sys/fs/cgroup\n/dev/nvme0n1p1  511M  7.8M  504M   2% /boot/efi\ntmpfs           695M   32K  694M   1% /run/user/1000\n/dev/sda1       1.6G  1.6G     0 100% /media/swhitmore/some-usb-name\n</code></pre> <ol> <li>Make sure it's not mounted</li> </ol> <pre><code>sudo umount /dev/sda1\n</code></pre> <ol> <li>Format the filesystem (pick one of the following)</li> </ol> <pre><code>sudo mkfs.vfat /dev/sda1 # for vFAT filesystem\nsudo mkfs.ntfs /dev/sda1 # for NTFS filesystem\nsudo mkfs.ext4 /dev/sda1 # for EXT4 filesystem\n</code></pre>","tags":["Linux","Bash"]},{"location":"environments/linux/mounts/#make-bootable-usb","title":"Make bootable USB","text":"<ol> <li>Identify the volume </li> </ol> <pre><code>$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\nudev            3.4G     0  3.4G   0% /dev\ntmpfs           695M  1.7M  693M   1% /run\n/dev/nvme0n1p2  234G   19G  203G   9% /\ntmpfs           3.4G  131M  3.3G   4% /dev/shm\ntmpfs           5.0M  4.0K  5.0M   1% /run/lock\ntmpfs           3.4G     0  3.4G   0% /sys/fs/cgroup\n/dev/nvme0n1p1  511M  7.8M  504M   2% /boot/efi\ntmpfs           695M   32K  694M   1% /run/user/1000\n/dev/sda1       1.6G  1.6G     0 100% /media/swhitmore/some-usb-name\n</code></pre> <ol> <li>Run the following command, replacing /dev/sdx with your drive, e.g. /dev/sda. (Do not append a partition number, so do not use something like /dev/sda1) </li> </ol> <pre><code>$ dd bs=4M if=path/to/archlinux.iso of=/dev/sdx status=progress oflag=sync\n</code></pre>","tags":["Linux","Bash"]},{"location":"environments/linux/mounts/#bluetooth-battery","title":"Bluetooth Battery","text":"<p>There's no simple way to check the battery life of connected Bluetooth headphones in Fedora 34 Workstation for some reason. This snippet combines the useful Python script with a few commands into a single command to check the battery level of my Cowin E7 headphones. The MAC address will be exposed but I don't care much about that. What are those black hats going to do, hijack my headphones? Probably not?</p> <p>--</p> <p>Bluetooth Headset Battery Level by TheWeirdDev. Other parts added by me.</p> <pre><code># Assumes `bluetooth_battery.py` is chmod'd with 755 (+x) permissions\n# Change \"mac_address\" and file path as needed\n\nfunction bluetoothbattery {\n    local mac_address=\"FC:58:FA:AC:5B:DF\"\n\n    cd ~/workspace/Bluetooth_Headset_Battery_Level || exit\n    bluetoothctl disconnect \"$mac_address\"\n    ./bluetooth_battery.py \"$mac_address\"\n    cd -\n}\n</code></pre>","tags":["Linux","Bash"]},{"location":"environments/linux/navigation/","title":"Navigation","text":"","tags":["Linux","Bash"]},{"location":"environments/linux/navigation/#find","title":"Find","text":"<pre><code>find &lt;directory&gt; -iname &lt;filename&gt;                # find files\nfind &lt;directory&gt; -type d -iname &lt;directory-name&gt;  # find directory\nfind . -name \"filename\" -delete                   # delete all files with given name\n</code></pre>","tags":["Linux","Bash"]},{"location":"environments/linux/navigation/#grep","title":"Grep","text":"<pre><code>-i # ignore case\n-r # recursive search\n-l # only list files (instead of showing exactly in the file where match is found)\n--include\\*.fileextension # only show matches for files of specified extension\n</code></pre> <p>To find all java files with \"some-pattern\", use</p> <p><pre><code>grep -irl \"some-pattern\" --include=\\*.java .\n</code></pre> \"grep: for every file ending in '.java' in this directory list those that include 'some-pattern', but only show the name and path of the file it's in and ignore case.\"</p>","tags":["Linux","Bash"]},{"location":"environments/linux/navigation/#tmux-shortcuts","title":"TMUX Shortcuts","text":"","tags":["Linux","Bash"]},{"location":"environments/linux/navigation/#attachingdetaching-from-sessions","title":"Attaching/detaching from sessions","text":"<p><code>tmux ls</code></p> <p><code>tmux a -t &lt;session-id&gt;</code></p>","tags":["Linux","Bash"]},{"location":"environments/linux/navigation/#resizing-panes","title":"Resizing Panes","text":"<pre><code># This assumes that you've hit ctrl + b and : to get to the command prompt\n:resize-pane -D (Resizes the current pane down)\n:resize-pane -U (Resizes the current pane upward)\n:resize-pane -L (Resizes the current pane left)\n:resize-pane -R (Resizes the current pane right)\n:resize-pane -D 10 (Resizes the current pane down by 10 cells)\n:resize-pane -U 10 (Resizes the current pane upward by 10 cells)\n:resize-pane -L 10 (Resizes the current pane left by 10 cells)\n:resize-pane -R 10 (Resizes the current pane right by 10 cells)\n</code></pre>","tags":["Linux","Bash"]},{"location":"environments/linux/system-management/","title":"System Management","text":"","tags":["Linux"]},{"location":"environments/linux/system-management/#package-updates","title":"Package updates","text":"<pre><code>sudo apt update        # Fetches the list of available updates\nsudo apt upgrade       # Installs some updates; does not remove packages\nsudo apt full-upgrade  # Installs updates; may also remove some packages, if needed\nsudo apt autoremove    # Removes any old packages that are no longer needed\n</code></pre>","tags":["Linux"]},{"location":"environments/linux/system-management/#os-info","title":"OS Info","text":"<p>Any of the following will display info about the current OS</p> <pre><code>cat /etc/os-release\n# or\nlsb_release -a\n# or\nhostnamectl\n</code></pre> <p>To see kernel version</p> <pre><code>uname -r\n</code></pre>","tags":["Linux"]},{"location":"environments/linux/system-management/#manual-package-installation","title":"Manual Package Installation","text":"<p>Sometimes applications are not available in a distro's package manager and only come as tar balls. In that case, follow the directions below for proper installation:</p>","tags":["Linux"]},{"location":"environments/linux/system-management/#example-postman","title":"Example: Postman","text":"<ol> <li>Download Postman from https://www.postman.com/downloads/</li> <li>Extract contents of file with <code>tar -xzvf Postman-whatever-version.tar.gz</code></li> <li>Move the extracted directory to somewhere public like <code>/opt</code></li> <li>Check which directories are in the path with <code>echo $PATH</code></li> <li>Create a symbolic link in one of the path directories (I used <code>/usr/local/bin</code>) with <code>sudo ln -s /opt/Postman/Postman /usr/local/bin/postman</code></li> <li>Create a desktop file with <code>touch ~/.local/share/applications/postman.desktop</code></li> <li>Enter the below contents into postman.desktop</li> </ol> <pre><code>[Desktop Entry]\nName=Postman\nGenericName=API Client\nX-GNOME-FullName=Postman API Client\nComment=Make and view REST API calls and responses\nKeywords=api;\nExec=/usr/local/bin/postman\nTerminal=false\nType=Application\nIcon=/opt/Postman/app/resources/app/assets/icon.png\nCategories=Development;Utilities;\n</code></pre>","tags":["Linux"]},{"location":"security/","title":"Security Overview","text":"<p>Notes on all things security</p>"},{"location":"security/vault/","title":"HashiCorp Vault Quick Reference","text":"<p>Source: https://learn.hashicorp.com/collections/vault/getting-started</p>","tags":["Security","Vault"]},{"location":"security/vault/#dev-server","title":"Dev Server","text":"<p><code>vault server -dev</code></p> <p><code>export VAULT_ADDR='http://127.0.0.1:8200'</code></p> <p>Token will be different. See output on server setup.</p> <p><code>export VAULT_TOKEN=\"s.XmpNPoi9sRhYtdKHaQhkHP6x\"</code></p> <p>Check status</p> <p><code>vault status</code></p>","tags":["Security","Vault"]},{"location":"security/vault/#preventing-exposure-during-interaction","title":"Preventing exposure during interaction","text":"<p>Q: How do I enter my secrets without exposing the secret in my shell's history?</p> <p>There are 3 options. </p> <p>The first option is good but will still expose the path and key. You can add a dash after the <code>key=</code>, hit enter, write the value on the next line, then press <code>Ctrl+d</code> to end the pipe which will write the secret to Vault.</p> <p>The second option involves storing the key-value pairs in a file which I'm never going to do.</p> <p>The simplest one is the 3rd one - Disable all vault command history</p> <p><code>export HISTIGNORE=\"&amp;:vault*\"</code></p> <p>https://learn.hashicorp.com/tutorials/vault/static-secrets#q-how-do-i-enter-my-secrets-without-exposing-the-secret-in-my-shell-s-history</p>","tags":["Security","Vault"]},{"location":"security/vault/#working-with-keyvalue-pairs","title":"Working with key/value pairs","text":"<p><code>vault kv -help</code></p> <p><code>vault kv put secret/hello foo=world</code></p> <p><code>vault kv get -field=excited secret/hello</code></p> <p><code>vault kv delete secret/hello</code> </p> <p><code>vault kv undelete -versions=2 secret/hello</code></p> <p>Use <code>destroy</code> to do a hard delete.</p>","tags":["Security","Vault"]},{"location":"security/vault/#enabling-a-secrets-engine","title":"Enabling a secrets engine","text":"<p><code>vault secrets enable -path=foo kv</code> starts a new secrets path at <code>foo/</code></p> <p><code>vault secrets list</code> to confirm this</p> <p><code>vault secrets disable foo</code> will remove it</p> <p><code>vault kv list foo</code> to show all paths following <code>foo</code></p>","tags":["Security","Vault"]},{"location":"about/tag_index/","title":"Tag Index","text":"<p>Herein is an index of tags used in this site's documentation.</p>"},{"location":"about/tag_index/#aws","title":"AWS","text":"<ul> <li>Kubernetes Crash Course</li> </ul>"},{"location":"about/tag_index/#angular","title":"Angular","text":"<ul> <li>RxJS</li> </ul>"},{"location":"about/tag_index/#apache","title":"Apache","text":"<ul> <li>LAMP stack</li> </ul>"},{"location":"about/tag_index/#automation","title":"Automation","text":"<ul> <li>Automation</li> </ul>"},{"location":"about/tag_index/#bash","title":"Bash","text":"<ul> <li>File Management</li> <li>Interfacing</li> <li>Mounts</li> <li>Navigation</li> </ul>"},{"location":"about/tag_index/#cloud","title":"Cloud","text":"<ul> <li>Docker Crash Course</li> <li>Kubernetes Crash Course</li> </ul>"},{"location":"about/tag_index/#cron","title":"Cron","text":"<ul> <li>Automation</li> </ul>"},{"location":"about/tag_index/#docker","title":"Docker","text":"<ul> <li>Docker Crash Course</li> </ul>"},{"location":"about/tag_index/#git","title":"Git","text":"<ul> <li>Interfacing</li> </ul>"},{"location":"about/tag_index/#k8s","title":"K8s","text":"<ul> <li>Kubernetes Crash Course</li> </ul>"},{"location":"about/tag_index/#lamp","title":"LAMP","text":"<ul> <li>LAMP stack</li> </ul>"},{"location":"about/tag_index/#linux","title":"Linux","text":"<ul> <li>Automation</li> <li>Desktop</li> <li>File Management</li> <li>Interfacing</li> <li>Mounts</li> <li>Navigation</li> <li>System Management</li> </ul>"},{"location":"about/tag_index/#mysql","title":"MySQL","text":"<ul> <li>LAMP stack</li> </ul>"},{"location":"about/tag_index/#php","title":"PHP","text":"<ul> <li>LAMP stack</li> </ul>"},{"location":"about/tag_index/#rxjs","title":"RxJS","text":"<ul> <li>RxJS</li> </ul>"},{"location":"about/tag_index/#smtp","title":"SMTP","text":"<ul> <li>Interfacing</li> </ul>"},{"location":"about/tag_index/#ssh","title":"SSH","text":"<ul> <li>Interfacing</li> </ul>"},{"location":"about/tag_index/#security","title":"Security","text":"<ul> <li>HashiCorp Vault Quick Reference</li> </ul>"},{"location":"about/tag_index/#systemd","title":"Systemd","text":"<ul> <li>Automation</li> </ul>"},{"location":"about/tag_index/#typescript","title":"TypeScript","text":"<ul> <li>RxJS</li> <li>Types vs Interfaces</li> </ul>"},{"location":"about/tag_index/#vault","title":"Vault","text":"<ul> <li>HashiCorp Vault Quick Reference</li> </ul>"},{"location":"about/tag_index/#wordpress","title":"WordPress","text":"<ul> <li>LAMP stack</li> </ul>"}]}